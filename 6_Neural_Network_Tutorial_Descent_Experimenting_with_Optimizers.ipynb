{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-requisites\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To clear print buffer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing functions from the previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "1\n",
      "(2, 3)\n",
      "[[-0.92606799  0.56882007  0.01466291]\n",
      " [-0.16292066 -0.48563007  1.35342642]]\n",
      "2\n",
      "(1, 3)\n",
      "[[ 0.47295854  1.37977747  0.44116873]]\n",
      "Cost: 0.222204091203\n",
      "Accuracy: \n",
      "[[ 0.74404282]\n",
      " [ 0.76974288]\n",
      " [ 0.76716486]\n",
      " [ 0.79266699]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing weight matrices from layer sizes\n",
    "def initializeWeights(layers):\n",
    "    weights = [np.random.randn(o, i+1) for i, o in zip(layers[:-1], layers[1:])]\n",
    "    return weights\n",
    "\n",
    "# Add a bias term to every data point in the input\n",
    "def addBiasTerms(X):\n",
    "        # Make the input an np.array()\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Forcing 1D vectors to be 2D matrices of 1xlength dimensions\n",
    "        if X.ndim==1:\n",
    "            X = np.reshape(X, (1, len(X)))\n",
    "        \n",
    "        # Inserting bias terms\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))\n",
    "\n",
    "# Forward Propagation of outputs\n",
    "def forwardProp(X, weights):\n",
    "    # Initializing an empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Assigning a name to reuse as inputs\n",
    "    inputs = X\n",
    "    \n",
    "    # For each layer\n",
    "    for w in weights:\n",
    "        # Add bias term to input\n",
    "        inputs = addBiasTerms(inputs)\n",
    "        \n",
    "        # Y = Sigmoid ( X .* W^T )\n",
    "        outputs.append(sigmoid(np.dot(inputs, w.T)))\n",
    "        \n",
    "        # Input of next layer is output of this layer\n",
    "        inputs = outputs[-1]\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "# Compute COST (J) of Neural Network\n",
    "def nnCost(weights, X, Y):\n",
    "    # Calculate yPred\n",
    "    yPred = forwardProp(X, weights)[-1]\n",
    "    \n",
    "    # Compute J\n",
    "    J = 0.5*np.sum((yPred-Y)**2)/len(Y)\n",
    "    \n",
    "    return J\n",
    "\n",
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE\n",
    "# Added eta, the learning rate, as an input\n",
    "def backProp(weights, X, Y, learningRate):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # Change weights of the current layer (W <- W + eta*deltaW)\n",
    "        w += -learningRate*grad\n",
    "\n",
    "# Evaluate the accuracy of weights for input X and desired outptut Y\n",
    "def evaluate(weights, X, Y):\n",
    "    yPreds = forwardProp(X, weights)[-1]\n",
    "    # Check if maximum probability is from that neuron corresponding to desired class,\n",
    "    # AND check if that maximum probability is greater than 0.5\n",
    "    yes = sum( int( ( np.argmax(yPreds[i]) == np.argmax(Y[i]) ) and \n",
    "                    ( (yPreds[i][np.argmax(yPreds[i])]>0.5) == (Y[i][np.argmax(Y[i])]>0.5) ) )\n",
    "              for i in range(len(Y)) )\n",
    "    return yes\n",
    "\n",
    "# TRAINING USING MINI-BATCH GRADIENT DESCENT\n",
    "# Default learning rate = 1.0\n",
    "def trainUsingMinibatchGD(weights, X, Y, minibatchSize, nEpochs, learningRate=1.0, testX=None, testY=None):\n",
    "    # If testing data is not provided, check accuracy on training data\n",
    "    if testX is None:\n",
    "        testX = X\n",
    "        testY = Y\n",
    "    \n",
    "    # For nEpochs number of epochs:\n",
    "    for i in range(nEpochs):\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Count number of mini-batches\n",
    "        nOfMinibatches = int(len(X)/minibatchSize)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for m in range(nOfMinibatches):\n",
    "            \n",
    "            # Compute the starting index of this mini-batch\n",
    "            startIdx = m*minibatchSize\n",
    "            \n",
    "            # Declare sampled inputs and outputs\n",
    "            xSample = X[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            ySample = Y[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            \n",
    "            # Run backprop\n",
    "            backProp(weights, xSample, ySample, learningRate)\n",
    "        \n",
    "        # Check cost and accuracy\n",
    "        cost = nnCost(weights, testX, testY)\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"Epoch \"+str(i+1)+\" of \"+str(nEpochs)+\" : \"+\n",
    "              str(yes)+\" of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "              \"; cost=\"+str(round(cost, 4)))\n",
    "\n",
    "# Initialize network\n",
    "layers = [2, 2, 1]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])\n",
    "\n",
    "# Declare input and desired output for AND gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Check current accuracy and cost\n",
    "print(\"Cost: \"+str(nnCost(weights, X, Y)))\n",
    "yes = evaluate(weights, X, Y)\n",
    "print(\"Accuracy: \"+str(yes)+\" of \"+str(len(Y))+\" = \"+str(round(float(yes/len(Y)), 4)))\n",
    "print(forwardProp(X, weights)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data, and format it for our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST DATA\n",
    "# Use numpy.load() to load the .npz file\n",
    "f = np.load('mnist.npz')\n",
    "# Saving the files\n",
    "x_train = f['x_train']\n",
    "y_train = f['y_train']\n",
    "x_test = f['x_test']\n",
    "y_test = f['y_test']\n",
    "f.close()\n",
    "\n",
    "# Reshaping x_train and x_test for our network with 784 inputs neurons\n",
    "x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "# Normalize x_train\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Make new y_train of nx10 elements\n",
    "new_y_train = np.zeros((len(y_train), 10))\n",
    "for i in range(len(y_train)):\n",
    "    new_y_train[i, y_train[i]] = 1\n",
    "\n",
    "del y_train\n",
    "y_train = new_y_train\n",
    "\n",
    "# Make new y_test of nx10 elements\n",
    "new_y_test = np.zeros((len(y_test), 10))\n",
    "for i in range(len(y_test)):\n",
    "    new_y_test[i, y_test[i]] = 1\n",
    "\n",
    "del y_test\n",
    "y_test = new_y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to train our network to give good accuracy on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let us try a deeper network, with 2 hidden layers of 10 neurons each. And let us test it on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 1021 out of 10000 = 0.1021; cost=2.3735\n",
      "Epoch 2 of 30 : 979 out of 10000 = 0.0979; cost=2.2225\n",
      "Epoch 3 of 30 : 971 out of 10000 = 0.0971; cost=2.1205\n",
      "Epoch 4 of 30 : 970 out of 10000 = 0.097; cost=2.0373\n",
      "Epoch 5 of 30 : 968 out of 10000 = 0.0968; cost=1.9495\n",
      "Epoch 6 of 30 : 964 out of 10000 = 0.0964; cost=1.8392\n",
      "Epoch 7 of 30 : 976 out of 10000 = 0.0976; cost=1.6981\n",
      "Epoch 8 of 30 : 968 out of 10000 = 0.0968; cost=1.5417\n",
      "Epoch 9 of 30 : 969 out of 10000 = 0.0969; cost=1.4061\n",
      "Epoch 10 of 30 : 979 out of 10000 = 0.0979; cost=1.309\n",
      "Epoch 11 of 30 : 974 out of 10000 = 0.0974; cost=1.2383\n",
      "Epoch 12 of 30 : 975 out of 10000 = 0.0975; cost=1.1794\n",
      "Epoch 13 of 30 : 988 out of 10000 = 0.0988; cost=1.1257\n",
      "Epoch 14 of 30 : 990 out of 10000 = 0.099; cost=1.0758\n",
      "Epoch 15 of 30 : 1001 out of 10000 = 0.1001; cost=1.03\n",
      "Epoch 16 of 30 : 997 out of 10000 = 0.0997; cost=0.9881\n",
      "Epoch 17 of 30 : 992 out of 10000 = 0.0992; cost=0.9481\n",
      "Epoch 18 of 30 : 977 out of 10000 = 0.0977; cost=0.9062\n",
      "Epoch 19 of 30 : 951 out of 10000 = 0.0951; cost=0.8582\n",
      "Epoch 20 of 30 : 877 out of 10000 = 0.0877; cost=0.8007\n",
      "Epoch 21 of 30 : 772 out of 10000 = 0.0772; cost=0.7356\n",
      "Epoch 22 of 30 : 594 out of 10000 = 0.0594; cost=0.6739\n",
      "Epoch 23 of 30 : 443 out of 10000 = 0.0443; cost=0.6263\n",
      "Epoch 24 of 30 : 316 out of 10000 = 0.0316; cost=0.5938\n",
      "Epoch 25 of 30 : 243 out of 10000 = 0.0243; cost=0.572\n",
      "Epoch 26 of 30 : 196 out of 10000 = 0.0196; cost=0.5568\n",
      "Epoch 27 of 30 : 163 out of 10000 = 0.0163; cost=0.5458\n",
      "Epoch 28 of 30 : 140 out of 10000 = 0.014; cost=0.5374\n",
      "Epoch 29 of 30 : 125 out of 10000 = 0.0125; cost=0.5308\n",
      "Epoch 30 of 30 : 113 out of 10000 = 0.0113; cost=0.5255\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "initialWeights = [np.array(w) for w in weights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = len(y_train)\n",
    "nEpochs = 30\n",
    "learningRate = 1.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 8779 out of 10000 = 0.8779; cost=0.0698\n",
      "Epoch 2 of 30 : 8828 out of 10000 = 0.8828; cost=0.0665\n",
      "Epoch 3 of 30 : 9095 out of 10000 = 0.9095; cost=0.0545\n",
      "Epoch 4 of 30 : 9098 out of 10000 = 0.9098; cost=0.0535\n",
      "Epoch 5 of 30 : 9160 out of 10000 = 0.916; cost=0.0503\n",
      "Epoch 6 of 30 : 9171 out of 10000 = 0.9171; cost=0.0496\n",
      "Epoch 7 of 30 : 9245 out of 10000 = 0.9245; cost=0.0472\n",
      "Epoch 8 of 30 : 9259 out of 10000 = 0.9259; cost=0.0484\n",
      "Epoch 9 of 30 : 9224 out of 10000 = 0.9224; cost=0.0472\n",
      "Epoch 10 of 30 : 9243 out of 10000 = 0.9243; cost=0.046\n",
      "Epoch 11 of 30 : 9288 out of 10000 = 0.9288; cost=0.0443\n",
      "Epoch 12 of 30 : 9259 out of 10000 = 0.9259; cost=0.0458\n",
      "Epoch 13 of 30 : 9281 out of 10000 = 0.9281; cost=0.045\n",
      "Epoch 14 of 30 : 9248 out of 10000 = 0.9248; cost=0.0474\n",
      "Epoch 15 of 30 : 9239 out of 10000 = 0.9239; cost=0.0457\n",
      "Epoch 16 of 30 : 9305 out of 10000 = 0.9305; cost=0.0449\n",
      "Epoch 17 of 30 : 9290 out of 10000 = 0.929; cost=0.0449\n",
      "Epoch 18 of 30 : 9296 out of 10000 = 0.9296; cost=0.0431\n",
      "Epoch 19 of 30 : 9286 out of 10000 = 0.9286; cost=0.0436\n",
      "Epoch 20 of 30 : 9261 out of 10000 = 0.9261; cost=0.0453\n",
      "Epoch 21 of 30 : 9267 out of 10000 = 0.9267; cost=0.0446\n",
      "Epoch 22 of 30 : 9307 out of 10000 = 0.9307; cost=0.046\n",
      "Epoch 23 of 30 : 9286 out of 10000 = 0.9286; cost=0.0441\n",
      "Epoch 24 of 30 : 9335 out of 10000 = 0.9335; cost=0.0446\n",
      "Epoch 25 of 30 : 9294 out of 10000 = 0.9294; cost=0.0449\n",
      "Epoch 26 of 30 : 9357 out of 10000 = 0.9357; cost=0.0422\n",
      "Epoch 27 of 30 : 9311 out of 10000 = 0.9311; cost=0.0434\n",
      "Epoch 28 of 30 : 9269 out of 10000 = 0.9269; cost=0.0448\n",
      "Epoch 29 of 30 : 9356 out of 10000 = 0.9356; cost=0.0438\n",
      "Epoch 30 of 30 : 9292 out of 10000 = 0.9292; cost=0.0452\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using mini-batch gradient descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "initialWeights = [np.array(w) for w in weights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that sometimes the accuracy decreases and cost increases. This means our learning rate is too big to move only so much as to decrease the cost. We need to find ways of adapting the learning rate to train our network with the optimal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decaying learning rate\n",
    "\n",
    "By decreasing the learning rate, we will be able to take smaller steps towards the optimum. There are different ways to decrease the learning rate.\n",
    "\n",
    "## Step decay\n",
    "\n",
    "Whenever the cost of the neural network increases (i.e. accuracy decreases), we shall revert the weights back to the values at the start of the iteration, and reduce the learning rate by half. We call this **Step decay**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training function, let us add the option of choosing the decay, and define the Step decay optimizer. If the cost does not decrease contiguously for too long (5 epochs), let us stop our training there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING USING MINI-BATCH GRADIENT DESCENT\n",
    "def trainUsingMinibatchGD(weights, X, Y, minibatchSize, nEpochs, learningRate=1.0, \n",
    "                          decay=None, testX=None, testY=None):\n",
    "    # If testing data is not provided, check accuracy on training data\n",
    "    if testX is None:\n",
    "        testX = X\n",
    "        testY = Y\n",
    "    \n",
    "    # Check cost and accuracy\n",
    "    # Initialize cost\n",
    "    prevCost = nnCost(weights, testX, testY)\n",
    "    yes = evaluate(weights, testX, testY)\n",
    "    print(\"Before training: \"+str(yes)+\" of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "          \"; cost=\"+str(prevCost))\n",
    "    \n",
    "    # Backup weights to revert back in case cost increases\n",
    "    oldWeights = [np.array(w) for w in weights]\n",
    "    \n",
    "    # To count the number of times learning rate had to be halved contiguously\n",
    "    countLRHalf = 0\n",
    "    \n",
    "    # Initialize index for iteration through epochs\n",
    "    epoch = 0\n",
    "    \n",
    "    # For nEpochs number of epochs:\n",
    "    while epoch < nEpochs:\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Count number of mini-batches\n",
    "        nOfMinibatches = int(len(X)/minibatchSize)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for m in range(nOfMinibatches):\n",
    "            \n",
    "            # Compute the starting index of this mini-batch\n",
    "            startIdx = m*minibatchSize\n",
    "            \n",
    "            # Declare sampled inputs and outputs\n",
    "            xSample = X[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            ySample = Y[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "\n",
    "            # Run backprop, with an optimizer\n",
    "            backProp(weights, xSample, ySample, learningRate)\n",
    "        \n",
    "        # Check cost and accuracy\n",
    "        cost = nnCost(weights, testX, testY)\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"Epoch \"+str(epoch+1)+\" of \"+str(nEpochs)+\" : \"+\n",
    "              str(yes)+\" of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "              \"; cost=\"+str(cost))\n",
    "        \n",
    "        # If decay type is 'step', when cost increases, revert back weights and halve learning rate \n",
    "        if decay is 'step':\n",
    "            # If cost does not decrease\n",
    "            if cost >= prevCost:\n",
    "                # Revert weights back to those at the start of this epoch\n",
    "                weights = [np.array(w) for w in oldWeights]\n",
    "                \n",
    "                # Recalculate prevCost\n",
    "                cost = nnCost(weights, testX, testY)\n",
    "                \n",
    "                # Halve the learning rate\n",
    "                learningRate = learningRate/2.0\n",
    "                \n",
    "                # Revert iteration number\n",
    "                epoch -= 1\n",
    "                \n",
    "                # Increment the count of halving learning rate by 1\n",
    "                countLRHalf += 1\n",
    "                \n",
    "                print(\"Halving learning rate to: \"+str(learningRate)+\", count=\"+str(countLRHalf))\n",
    "            # If cost decreases, reset the count to 0\n",
    "            else:\n",
    "                countLRHalf = 0\n",
    "        \n",
    "        # If learningRate has been halved contiguously for too long, break\n",
    "        if countLRHalf is 5:\n",
    "            break\n",
    "        \n",
    "        # Set prevCost for next epoch\n",
    "        prevCost = cost\n",
    "        \n",
    "        # Set oldWeights for next epoch\n",
    "        oldWeights = [np.array(w) for w in weights]\n",
    "        \n",
    "        # Increase iteration number for epochs\n",
    "        epoch += 1\n",
    "    \n",
    "    # If training was stopped because accuracy was not increasing\n",
    "    if epoch < nEpochs:\n",
    "        print(\"Training ended prematurely...\")\n",
    "    # If training ended in correct number of epochs\n",
    "    else:\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network with a decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.3706\n",
      "Epoch 1 of 30 : 8903 out of 10000 = 0.8903; cost=0.0675467370522\n",
      "Epoch 2 of 30 : 9009 out of 10000 = 0.9009; cost=0.0604533829713\n",
      "Epoch 3 of 30 : 9105 out of 10000 = 0.9105; cost=0.0558005001716\n",
      "Epoch 4 of 30 : 9185 out of 10000 = 0.9185; cost=0.050210922763\n",
      "Epoch 5 of 30 : 9203 out of 10000 = 0.9203; cost=0.0501670595612\n",
      "Epoch 6 of 30 : 9201 out of 10000 = 0.9201; cost=0.0469989425479\n",
      "Epoch 7 of 30 : 9269 out of 10000 = 0.9269; cost=0.0442005669555\n",
      "Epoch 8 of 30 : 9215 out of 10000 = 0.9215; cost=0.0476086394864\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 8 of 30 : 9288 out of 10000 = 0.9288; cost=0.0440865277318\n",
      "Epoch 9 of 30 : 9274 out of 10000 = 0.9274; cost=0.0433871957187\n",
      "Epoch 10 of 30 : 9301 out of 10000 = 0.9301; cost=0.0436701848444\n",
      "Halving learning rate to: 0.75, count=1\n",
      "Epoch 10 of 30 : 9283 out of 10000 = 0.9283; cost=0.0425153081087\n",
      "Epoch 11 of 30 : 9308 out of 10000 = 0.9308; cost=0.0423972816577\n",
      "Epoch 12 of 30 : 9303 out of 10000 = 0.9303; cost=0.042181669639\n",
      "Epoch 13 of 30 : 9297 out of 10000 = 0.9297; cost=0.0425309884927\n",
      "Halving learning rate to: 0.375, count=1\n",
      "Epoch 13 of 30 : 9308 out of 10000 = 0.9308; cost=0.0416875143518\n",
      "Epoch 14 of 30 : 9301 out of 10000 = 0.9301; cost=0.0421045140669\n",
      "Halving learning rate to: 0.1875, count=1\n",
      "Epoch 14 of 30 : 9309 out of 10000 = 0.9309; cost=0.0418304186028\n",
      "Halving learning rate to: 0.09375, count=2\n",
      "Epoch 14 of 30 : 9304 out of 10000 = 0.9304; cost=0.0417870434298\n",
      "Halving learning rate to: 0.046875, count=3\n",
      "Epoch 14 of 30 : 9298 out of 10000 = 0.9298; cost=0.0417780758227\n",
      "Halving learning rate to: 0.0234375, count=4\n",
      "Epoch 14 of 30 : 9305 out of 10000 = 0.9305; cost=0.0417283145432\n",
      "Halving learning rate to: 0.01171875, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN with decaying learning rate\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that by halving learning rate whenever cost increases, the final accuracy improves for the same number of epochs and same weight initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Momentum update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to improve accuracy faster is to push the change in weight more in the direction of the previous update. From [CS231n](http://cs231n.github.io/neural-networks-3/),\n",
    "```\n",
    "v = mu * v - learning_rate * dx # integrate velocity\n",
    "x += v # integrate position\n",
    "```\n",
    "\n",
    "So this is an improvement on the back-propagation algorithm itself.\n",
    "\n",
    "Let us add an option of choosing an optimization on back-propagation, and code Momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE, MOMENTUM\n",
    "# Added eta, the learning rate, as an input\n",
    "def backProp(weights, X, Y, learningRate, optimizer=None, mu=0.9):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Initialize velocity in the shape of weights for use with momentum\n",
    "    v = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # Change weights of the current layer (W <- W + eta*deltaW)\n",
    "        if optimizer is None:\n",
    "            w += -learningRate * grad\n",
    "        \n",
    "        # Momentum\n",
    "        if optimizer is 'momentum':\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad\n",
    "            w += v[-l-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the option of optimizer while calling backprop in trainUsingMiniGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING USING MINI-BATCH GRADIENT DESCENT\n",
    "def trainUsingMinibatchGD(weights, X, Y, minibatchSize, nEpochs, learningRate=1.0, \n",
    "                          decay=None, optimizer=None, mu=0.9, testX=None, testY=None):\n",
    "    # If testing data is not provided, check accuracy on training data\n",
    "    if testX is None:\n",
    "        testX = X\n",
    "        testY = Y\n",
    "    \n",
    "    # Check cost and accuracy\n",
    "    # Initialize cost\n",
    "    prevCost = nnCost(weights, testX, testY)\n",
    "    yes = evaluate(weights, testX, testY)\n",
    "    print(\"Before training: \"+str(yes)+\" of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "          \"; cost=\"+str(prevCost))\n",
    "    \n",
    "    # Backup weights to revert back in case cost increases\n",
    "    oldWeights = [np.array(w) for w in weights]\n",
    "    \n",
    "    # To count the number of times learning rate had to be halved contiguously\n",
    "    countLRHalf = 0\n",
    "    \n",
    "    # Initialize index for iteration through epochs\n",
    "    epoch = 0\n",
    "    \n",
    "    # For nEpochs number of epochs:\n",
    "    while epoch < nEpochs:\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Count number of mini-batches\n",
    "        nOfMinibatches = int(len(X)/minibatchSize)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for m in range(nOfMinibatches):\n",
    "            \n",
    "            # Compute the starting index of this mini-batch\n",
    "            startIdx = m*minibatchSize\n",
    "            \n",
    "            # Declare sampled inputs and outputs\n",
    "            xSample = X[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            ySample = Y[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "\n",
    "            # Run backprop, with an optimizer\n",
    "            backProp(weights, xSample, ySample, learningRate, optimizer, mu)\n",
    "        \n",
    "        # Check cost and accuracy\n",
    "        cost = nnCost(weights, testX, testY)\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"Epoch \"+str(epoch+1)+\" of \"+str(nEpochs)+\" : \"+\n",
    "              str(yes)+\" of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "              \"; cost=\"+str(cost))\n",
    "        \n",
    "        # If decay type is 'step', when cost increases, revert back weights and halve learning rate \n",
    "        if decay is 'step':\n",
    "            # If cost does not decrease\n",
    "            if cost >= prevCost:\n",
    "                # Revert weights back to those at the start of this epoch\n",
    "                weights = [np.array(w) for w in oldWeights]\n",
    "                \n",
    "                # Recalculate prevCost\n",
    "                cost = nnCost(weights, testX, testY)\n",
    "                \n",
    "                # Halve the learning rate\n",
    "                learningRate = learningRate/2.0\n",
    "                \n",
    "                # Revert iteration number\n",
    "                epoch -= 1\n",
    "                \n",
    "                # Increment the count of halving learning rate by 1\n",
    "                countLRHalf += 1\n",
    "                \n",
    "                print(\"Halving learning rate to: \"+str(learningRate)+\", count=\"+str(countLRHalf))\n",
    "            # If cost decreases, reset the count to 0\n",
    "            else:\n",
    "                countLRHalf = 0\n",
    "        \n",
    "        # If learningRate has been halved contiguously for too long, break\n",
    "        if countLRHalf is 10:\n",
    "            break\n",
    "        \n",
    "        # Set prevCost for next epoch\n",
    "        prevCost = cost\n",
    "        \n",
    "        # Set oldWeights for next epoch\n",
    "        oldWeights = [np.array(w) for w in weights]\n",
    "        \n",
    "        # Increase iteration number for epochs\n",
    "        epoch += 1\n",
    "    \n",
    "    # If training was stopped because accuracy was not increasing\n",
    "    if epoch < nEpochs:\n",
    "        print(\"Training ended prematurely...\")\n",
    "    # If training ended in correct number of epochs\n",
    "    else:\n",
    "        print(\"Training complete.\")\n",
    "    \n",
    "    # Printing training accuracy\n",
    "    yes = evaluate(weights, X, Y)\n",
    "    print(\"TRAINING ACCURACY, COST : \"+str(yes)+\" of \"+str(len(Y))+\n",
    "          \" = \"+str(round(float(yes/len(Y)),4)))\n",
    "    \n",
    "    # Printing test accuracy\n",
    "    if testY is not Y:\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"TEST ACCURACY, COST : \"+str(yes)+\" of \"+str(len(testY))+\n",
    "              \" = \"+str(round(float(yes/len(testY)),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network with the same initial weights, using step decay in learning rate, and Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n",
      "Epoch 1 of 30 : 8777 out of 10000 = 0.8777; cost=0.0760498120193\n",
      "Epoch 2 of 30 : 8928 out of 10000 = 0.8928; cost=0.0603590744736\n",
      "Epoch 3 of 30 : 9118 out of 10000 = 0.9118; cost=0.0527367063794\n",
      "Epoch 4 of 30 : 9098 out of 10000 = 0.9098; cost=0.0519137464361\n",
      "Epoch 5 of 30 : 9205 out of 10000 = 0.9205; cost=0.0500469055895\n",
      "Epoch 6 of 30 : 9154 out of 10000 = 0.9154; cost=0.049017530374\n",
      "Epoch 7 of 30 : 9197 out of 10000 = 0.9197; cost=0.0494850178497\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 7 of 30 : 9301 out of 10000 = 0.9301; cost=0.0442050439134\n",
      "Epoch 8 of 30 : 9298 out of 10000 = 0.9298; cost=0.0441667726679\n",
      "Epoch 9 of 30 : 9299 out of 10000 = 0.9299; cost=0.0437103775144\n",
      "Epoch 10 of 30 : 9285 out of 10000 = 0.9285; cost=0.0438829175281\n",
      "Halving learning rate to: 0.75, count=1\n",
      "Epoch 10 of 30 : 9331 out of 10000 = 0.9331; cost=0.0422593709095\n",
      "Epoch 11 of 30 : 9306 out of 10000 = 0.9306; cost=0.0422729755298\n",
      "Halving learning rate to: 0.375, count=1\n",
      "Epoch 11 of 30 : 9326 out of 10000 = 0.9326; cost=0.0419471385273\n",
      "Epoch 12 of 30 : 9333 out of 10000 = 0.9333; cost=0.0417391687278\n",
      "Epoch 13 of 30 : 9318 out of 10000 = 0.9318; cost=0.04171533867\n",
      "Epoch 14 of 30 : 9319 out of 10000 = 0.9319; cost=0.0419683619918\n",
      "Halving learning rate to: 0.1875, count=1\n",
      "Epoch 14 of 30 : 9330 out of 10000 = 0.933; cost=0.0417593630547\n",
      "Halving learning rate to: 0.09375, count=2\n",
      "Epoch 14 of 30 : 9333 out of 10000 = 0.9333; cost=0.0416114184656\n",
      "Epoch 15 of 30 : 9332 out of 10000 = 0.9332; cost=0.0415662772143\n",
      "Epoch 16 of 30 : 9334 out of 10000 = 0.9334; cost=0.0416654571536\n",
      "Halving learning rate to: 0.046875, count=1\n",
      "Epoch 16 of 30 : 9329 out of 10000 = 0.9329; cost=0.0415525851561\n",
      "Epoch 17 of 30 : 9330 out of 10000 = 0.933; cost=0.0415915185918\n",
      "Halving learning rate to: 0.0234375, count=1\n",
      "Epoch 17 of 30 : 9328 out of 10000 = 0.9328; cost=0.0415774069755\n",
      "Halving learning rate to: 0.01171875, count=2\n",
      "Epoch 17 of 30 : 9330 out of 10000 = 0.933; cost=0.0415714380325\n",
      "Halving learning rate to: 0.005859375, count=3\n",
      "Epoch 17 of 30 : 9328 out of 10000 = 0.9328; cost=0.0415618977884\n",
      "Halving learning rate to: 0.0029296875, count=4\n",
      "Epoch 17 of 30 : 9328 out of 10000 = 0.9328; cost=0.0415583792954\n",
      "Halving learning rate to: 0.00146484375, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Momentum\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='momentum', mu=mu, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better accuracy than vanilla gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [CS231n](http://cs231n.github.io/neural-networks-3/),\n",
    "```\n",
    "v_prev = v # back this up\n",
    "v = mu * v - learning_rate * dx # velocity update stays the same\n",
    "x += -mu * v_prev + (1 + mu) * v # position update changes form\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE, MOMENTUM, NAG\n",
    "def backProp(weights, X, Y, learningRate, optimizer=None, mu=0.9):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Initialize velocity in the shape of weights for use with momentum and NAG\n",
    "    v = [np.zeros(w.shape) for w in weights]\n",
    "    prevV = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # CHANGE WEIGHTS of the current layer (W <- W + eta*deltaW)\n",
    "        if optimizer is None:\n",
    "            w += -learningRate * grad\n",
    "        \n",
    "        # Momentum\n",
    "        if optimizer is 'momentum':\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad\n",
    "            w += v[-l-1]\n",
    "        \n",
    "        # Nesterov Momentum\n",
    "        if optimizer is 'nag':\n",
    "            prevV[-l-1] = np.array(v[-l-1]) # back this up\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad # velocity update stays the same\n",
    "            w += -mu * prevV[-l-1] + (1 + mu) * v[-l-1] # position update changes form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network with the same initial weights, using step decay in learning rate, and Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n",
      "Epoch 1 of 30 : 8849 out of 10000 = 0.8849; cost=0.0717524192533\n",
      "Epoch 2 of 30 : 9091 out of 10000 = 0.9091; cost=0.055670681835\n",
      "Epoch 3 of 30 : 9153 out of 10000 = 0.9153; cost=0.0525418892571\n",
      "Epoch 4 of 30 : 9117 out of 10000 = 0.9117; cost=0.0535916565845\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 4 of 30 : 9233 out of 10000 = 0.9233; cost=0.0481753882615\n",
      "Epoch 5 of 30 : 9237 out of 10000 = 0.9237; cost=0.0457395937391\n",
      "Epoch 6 of 30 : 9285 out of 10000 = 0.9285; cost=0.0452398507075\n",
      "Epoch 7 of 30 : 9251 out of 10000 = 0.9251; cost=0.0457638468201\n",
      "Halving learning rate to: 0.75, count=1\n",
      "Epoch 7 of 30 : 9295 out of 10000 = 0.9295; cost=0.0440445403005\n",
      "Epoch 8 of 30 : 9282 out of 10000 = 0.9282; cost=0.0428945881307\n",
      "Epoch 9 of 30 : 9302 out of 10000 = 0.9302; cost=0.0433700328082\n",
      "Halving learning rate to: 0.375, count=1\n",
      "Epoch 9 of 30 : 9307 out of 10000 = 0.9307; cost=0.0428650912142\n",
      "Epoch 10 of 30 : 9304 out of 10000 = 0.9304; cost=0.042643556965\n",
      "Epoch 11 of 30 : 9305 out of 10000 = 0.9305; cost=0.0428952189068\n",
      "Halving learning rate to: 0.1875, count=1\n",
      "Epoch 11 of 30 : 9312 out of 10000 = 0.9312; cost=0.0425517801805\n",
      "Epoch 12 of 30 : 9306 out of 10000 = 0.9306; cost=0.0423009801797\n",
      "Epoch 13 of 30 : 9314 out of 10000 = 0.9314; cost=0.0424361769251\n",
      "Halving learning rate to: 0.09375, count=1\n",
      "Epoch 13 of 30 : 9313 out of 10000 = 0.9313; cost=0.0422647509358\n",
      "Epoch 14 of 30 : 9307 out of 10000 = 0.9307; cost=0.0423255143017\n",
      "Halving learning rate to: 0.046875, count=1\n",
      "Epoch 14 of 30 : 9305 out of 10000 = 0.9305; cost=0.0422772878245\n",
      "Halving learning rate to: 0.0234375, count=2\n",
      "Epoch 14 of 30 : 9307 out of 10000 = 0.9307; cost=0.0422874367474\n",
      "Halving learning rate to: 0.01171875, count=3\n",
      "Epoch 14 of 30 : 9315 out of 10000 = 0.9315; cost=0.0422782299156\n",
      "Halving learning rate to: 0.005859375, count=4\n",
      "Epoch 14 of 30 : 9313 out of 10000 = 0.9313; cost=0.0422704386248\n",
      "Halving learning rate to: 0.0029296875, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Nesterov Momentum (NAG)\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='nag', mu=mu, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [CS231n](http://cs231n.github.io/neural-networks-3/),\n",
    "\n",
    "```\n",
    "# Assume the gradient dx and parameter vector x\n",
    "cache += dx**2\n",
    "x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE, MOMENTUM, NAG, ADAGRAD\n",
    "def backProp(weights, X, Y, learningRate, optimizer=None, mu=0.9):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Initialize velocity in the shape of weights for use with momentum and NAG\n",
    "    v = [np.zeros(w.shape) for w in weights]\n",
    "    prevV = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Initialize cache for use with Adagrad\n",
    "    cache = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # CHANGE WEIGHTS of the current layer (W <- W + eta*deltaW)\n",
    "        if optimizer is None:\n",
    "            w += -learningRate * grad\n",
    "        \n",
    "        # Momentum\n",
    "        if optimizer is 'momentum':\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad\n",
    "            w += v[-l-1]\n",
    "        \n",
    "        # Nesterov Momentum\n",
    "        if optimizer is 'nag':\n",
    "            prevV[-l-1] = np.array(v[-l-1]) # back this up\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad # velocity update stays the same\n",
    "            w += -mu * prevV[-l-1] + (1 + mu) * v[-l-1] # position update changes form\n",
    "        \n",
    "        # Adagrad\n",
    "        if optimizer is 'adagrad':\n",
    "            cache[-l-1] += grad**2\n",
    "            w += - learningRate * grad / (np.sqrt(cache[-l-1]) + np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to train the network using Adagrad, with the same options as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 of 10000 = 0.1004; cost=2.37056621443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.75, count=2\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.375, count=3\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.1875, count=4\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.09375, count=5\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.046875, count=6\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.0234375, count=7\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.01171875, count=8\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.005859375, count=9\n",
      "Epoch 2 of 30 : 958 out of 10000 = 0.0958; cost=0.9042\n",
      "Halving learning rate to: 0.0029296875, count=10\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Adagrad\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='adagrad', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the learningRate is too high at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 5806 out of 10000 = 0.5806; cost=0.260757491593\n",
      "Epoch 2 of 30 : 6782 out of 10000 = 0.6782; cost=0.204815630246\n",
      "Epoch 3 of 30 : 6842 out of 10000 = 0.6842; cost=0.192322763729\n",
      "Epoch 4 of 30 : 6874 out of 10000 = 0.6874; cost=0.187409859878\n",
      "Epoch 5 of 30 : 6444 out of 10000 = 0.6444; cost=0.206949182702\n",
      "Halving learning rate to: 0.5, count=1\n",
      "Epoch 5 of 30 : 6822 out of 10000 = 0.6822; cost=0.185802116025\n",
      "Epoch 6 of 30 : 6915 out of 10000 = 0.6915; cost=0.185329131066\n",
      "Epoch 7 of 30 : 7694 out of 10000 = 0.7694; cost=0.156780001919\n",
      "Epoch 8 of 30 : 7686 out of 10000 = 0.7686; cost=0.147885292916\n",
      "Epoch 9 of 30 : 7745 out of 10000 = 0.7745; cost=0.149509382291\n",
      "Halving learning rate to: 0.25, count=1\n",
      "Epoch 9 of 30 : 7776 out of 10000 = 0.7776; cost=0.142723029862\n",
      "Epoch 10 of 30 : 7794 out of 10000 = 0.7794; cost=0.140961464727\n",
      "Epoch 11 of 30 : 7650 out of 10000 = 0.765; cost=0.148943247373\n",
      "Halving learning rate to: 0.125, count=1\n",
      "Epoch 11 of 30 : 7715 out of 10000 = 0.7715; cost=0.146167760021\n",
      "Halving learning rate to: 0.0625, count=2\n",
      "Epoch 11 of 30 : 7904 out of 10000 = 0.7904; cost=0.140910028135\n",
      "Epoch 12 of 30 : 7899 out of 10000 = 0.7899; cost=0.139502353621\n",
      "Epoch 13 of 30 : 7924 out of 10000 = 0.7924; cost=0.139301605439\n",
      "Epoch 14 of 30 : 7928 out of 10000 = 0.7928; cost=0.139030056525\n",
      "Epoch 15 of 30 : 7843 out of 10000 = 0.7843; cost=0.139055102172\n",
      "Halving learning rate to: 0.03125, count=1\n",
      "Epoch 15 of 30 : 7899 out of 10000 = 0.7899; cost=0.138513794825\n",
      "Epoch 16 of 30 : 7888 out of 10000 = 0.7888; cost=0.140383794651\n",
      "Halving learning rate to: 0.015625, count=1\n",
      "Epoch 16 of 30 : 7915 out of 10000 = 0.7915; cost=0.139852236166\n",
      "Halving learning rate to: 0.0078125, count=2\n",
      "Epoch 16 of 30 : 7928 out of 10000 = 0.7928; cost=0.138930431629\n",
      "Halving learning rate to: 0.00390625, count=3\n",
      "Epoch 16 of 30 : 7920 out of 10000 = 0.792; cost=0.138591936215\n",
      "Halving learning rate to: 0.001953125, count=4\n",
      "Epoch 16 of 30 : 7924 out of 10000 = 0.7924; cost=0.138398817994\n",
      "Epoch 17 of 30 : 7922 out of 10000 = 0.7922; cost=0.138667644032\n",
      "Halving learning rate to: 0.0009765625, count=1\n",
      "Epoch 17 of 30 : 7922 out of 10000 = 0.7922; cost=0.138561992417\n",
      "Halving learning rate to: 0.00048828125, count=2\n",
      "Epoch 17 of 30 : 7922 out of 10000 = 0.7922; cost=0.138487825914\n",
      "Halving learning rate to: 0.000244140625, count=3\n",
      "Epoch 17 of 30 : 7924 out of 10000 = 0.7924; cost=0.138409643251\n",
      "Halving learning rate to: 0.0001220703125, count=4\n",
      "Epoch 17 of 30 : 7924 out of 10000 = 0.7924; cost=0.138398883401\n",
      "Halving learning rate to: 6.103515625e-05, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Adagrad\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 1.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='adagrad', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't use step decay with Adagrad, since Adagrad is changing the learning rate itself? And also decrease the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n",
      "Epoch 1 of 30 : 8598 out of 10000 = 0.8598; cost=0.0897437218552\n",
      "Epoch 2 of 30 : 8834 out of 10000 = 0.8834; cost=0.0800948018294\n",
      "Epoch 3 of 30 : 8890 out of 10000 = 0.889; cost=0.0766467365506\n",
      "Epoch 4 of 30 : 8959 out of 10000 = 0.8959; cost=0.0733428871671\n",
      "Epoch 5 of 30 : 9026 out of 10000 = 0.9026; cost=0.0702227264073\n",
      "Epoch 6 of 30 : 9036 out of 10000 = 0.9036; cost=0.0696045149575\n",
      "Epoch 7 of 30 : 9031 out of 10000 = 0.9031; cost=0.0689520775266\n",
      "Epoch 8 of 30 : 9036 out of 10000 = 0.9036; cost=0.0684540180848\n",
      "Epoch 9 of 30 : 9040 out of 10000 = 0.904; cost=0.0672808434164\n",
      "Epoch 10 of 30 : 9072 out of 10000 = 0.9072; cost=0.0645739458428\n",
      "Epoch 11 of 30 : 9079 out of 10000 = 0.9079; cost=0.0655200181255\n",
      "Epoch 12 of 30 : 9063 out of 10000 = 0.9063; cost=0.0661727976719\n",
      "Epoch 13 of 30 : 9109 out of 10000 = 0.9109; cost=0.0649883412977\n",
      "Epoch 14 of 30 : 9128 out of 10000 = 0.9128; cost=0.0633661602872\n",
      "Epoch 15 of 30 : 9106 out of 10000 = 0.9106; cost=0.0625576427636\n",
      "Epoch 16 of 30 : 9122 out of 10000 = 0.9122; cost=0.0620826698458\n",
      "Epoch 17 of 30 : 9132 out of 10000 = 0.9132; cost=0.0630176925986\n",
      "Epoch 18 of 30 : 9144 out of 10000 = 0.9144; cost=0.0617538368915\n",
      "Epoch 19 of 30 : 9126 out of 10000 = 0.9126; cost=0.0631208084599\n",
      "Epoch 20 of 30 : 9155 out of 10000 = 0.9155; cost=0.0623776196507\n",
      "Epoch 21 of 30 : 9150 out of 10000 = 0.915; cost=0.0608370782688\n",
      "Epoch 22 of 30 : 9115 out of 10000 = 0.9115; cost=0.0633837031458\n",
      "Epoch 23 of 30 : 9122 out of 10000 = 0.9122; cost=0.0631003614743\n",
      "Epoch 24 of 30 : 9155 out of 10000 = 0.9155; cost=0.0621716865532\n",
      "Epoch 25 of 30 : 9159 out of 10000 = 0.9159; cost=0.0627398957204\n",
      "Epoch 26 of 30 : 9132 out of 10000 = 0.9132; cost=0.0625497515645\n",
      "Epoch 27 of 30 : 9141 out of 10000 = 0.9141; cost=0.0628887240243\n",
      "Epoch 28 of 30 : 9155 out of 10000 = 0.9155; cost=0.0616397269062\n",
      "Epoch 29 of 30 : 9145 out of 10000 = 0.9145; cost=0.061533998062\n",
      "Epoch 30 of 30 : 9141 out of 10000 = 0.9141; cost=0.0621658119997\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Adagrad\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 0.01\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='adagrad', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we decrease the learning rate even more, and increase the number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 of 10000 = 0.1004; cost=2.37056621443\n",
      "Epoch 1 of 60 : 8196 out of 10000 = 0.8196; cost=0.112331510188\n",
      "Epoch 2 of 60 : 8600 out of 10000 = 0.86; cost=0.0941485638431\n",
      "Epoch 3 of 60 : 8727 out of 10000 = 0.8727; cost=0.0856475371118\n",
      "Epoch 4 of 60 : 8816 out of 10000 = 0.8816; cost=0.080994834278\n",
      "Epoch 5 of 60 : 8821 out of 10000 = 0.8821; cost=0.0781747558821\n",
      "Epoch 6 of 60 : 8895 out of 10000 = 0.8895; cost=0.075240924262\n",
      "Epoch 7 of 60 : 8929 out of 10000 = 0.8929; cost=0.0736012691173\n",
      "Epoch 8 of 60 : 8894 out of 10000 = 0.8894; cost=0.0735342850554\n",
      "Epoch 9 of 60 : 8968 out of 10000 = 0.8968; cost=0.0707477427605\n",
      "Epoch 10 of 60 : 8958 out of 10000 = 0.8958; cost=0.0703344996912\n",
      "Epoch 11 of 60 : 8990 out of 10000 = 0.899; cost=0.0713376111158\n",
      "Epoch 12 of 60 : 9024 out of 10000 = 0.9024; cost=0.0703251505404\n",
      "Epoch 13 of 60 : 8982 out of 10000 = 0.8982; cost=0.0698244936925\n",
      "Epoch 14 of 60 : 9011 out of 10000 = 0.9011; cost=0.0696684471508\n",
      "Epoch 15 of 60 : 9042 out of 10000 = 0.9042; cost=0.0674240807105\n",
      "Epoch 16 of 60 : 8997 out of 10000 = 0.8997; cost=0.0696902075927\n",
      "Epoch 17 of 60 : 9060 out of 10000 = 0.906; cost=0.0669010117488\n",
      "Epoch 18 of 60 : 9040 out of 10000 = 0.904; cost=0.0670870054335\n",
      "Epoch 19 of 60 : 9056 out of 10000 = 0.9056; cost=0.0651170597142\n",
      "Epoch 20 of 60 : 9058 out of 10000 = 0.9058; cost=0.0661782909189\n",
      "Epoch 21 of 60 : 9061 out of 10000 = 0.9061; cost=0.0655822310993\n",
      "Epoch 22 of 60 : 9053 out of 10000 = 0.9053; cost=0.065657812983\n",
      "Epoch 23 of 60 : 9052 out of 10000 = 0.9052; cost=0.065821155934\n",
      "Epoch 24 of 60 : 9047 out of 10000 = 0.9047; cost=0.0660398504622\n",
      "Epoch 25 of 60 : 9079 out of 10000 = 0.9079; cost=0.0652302828667\n",
      "Epoch 26 of 60 : 9037 out of 10000 = 0.9037; cost=0.0666850699557\n",
      "Epoch 27 of 60 : 9077 out of 10000 = 0.9077; cost=0.0656820306592\n",
      "Epoch 28 of 60 : 9072 out of 10000 = 0.9072; cost=0.0654896412926\n",
      "Epoch 29 of 60 : 9058 out of 10000 = 0.9058; cost=0.0659149638184\n",
      "Epoch 30 of 60 : 9063 out of 10000 = 0.9063; cost=0.065486210862\n",
      "Epoch 31 of 60 : 9073 out of 10000 = 0.9073; cost=0.065423898929\n",
      "Epoch 32 of 60 : 9099 out of 10000 = 0.9099; cost=0.063662814182\n",
      "Epoch 33 of 60 : 9085 out of 10000 = 0.9085; cost=0.0650665917124\n",
      "Epoch 34 of 60 : 9084 out of 10000 = 0.9084; cost=0.0635708923893\n",
      "Epoch 35 of 60 : 9109 out of 10000 = 0.9109; cost=0.0630462437262\n",
      "Epoch 36 of 60 : 9095 out of 10000 = 0.9095; cost=0.0627702088906\n",
      "Epoch 37 of 60 : 9090 out of 10000 = 0.909; cost=0.0637077027951\n",
      "Epoch 38 of 60 : 9115 out of 10000 = 0.9115; cost=0.0628731520457\n",
      "Epoch 39 of 60 : 9082 out of 10000 = 0.9082; cost=0.0646191339397\n",
      "Epoch 40 of 60 : 9097 out of 10000 = 0.9097; cost=0.0641377018469\n",
      "Epoch 41 of 60 : 9105 out of 10000 = 0.9105; cost=0.0641680400527\n",
      "Epoch 42 of 60 : 9124 out of 10000 = 0.9124; cost=0.0627939663809\n",
      "Epoch 43 of 60 : 9132 out of 10000 = 0.9132; cost=0.0620690317052\n",
      "Epoch 44 of 60 : 9098 out of 10000 = 0.9098; cost=0.0634264204207\n",
      "Epoch 45 of 60 : 9110 out of 10000 = 0.911; cost=0.0626586613093\n",
      "Epoch 46 of 60 : 9113 out of 10000 = 0.9113; cost=0.0629913383822\n",
      "Epoch 47 of 60 : 9142 out of 10000 = 0.9142; cost=0.0620151148525\n",
      "Epoch 48 of 60 : 9134 out of 10000 = 0.9134; cost=0.0618114751464\n",
      "Epoch 49 of 60 : 9128 out of 10000 = 0.9128; cost=0.0628622232506\n",
      "Epoch 50 of 60 : 9156 out of 10000 = 0.9156; cost=0.0612319678475\n",
      "Epoch 51 of 60 : 9128 out of 10000 = 0.9128; cost=0.0614515436686\n",
      "Epoch 52 of 60 : 9121 out of 10000 = 0.9121; cost=0.0621269087617\n",
      "Epoch 53 of 60 : 9118 out of 10000 = 0.9118; cost=0.0628420995814\n",
      "Epoch 54 of 60 : 9148 out of 10000 = 0.9148; cost=0.0623108872709\n",
      "Epoch 55 of 60 : 9106 out of 10000 = 0.9106; cost=0.0627892750909\n",
      "Epoch 56 of 60 : 9141 out of 10000 = 0.9141; cost=0.0617362643089\n",
      "Epoch 57 of 60 : 9130 out of 10000 = 0.913; cost=0.063330307684\n",
      "Epoch 58 of 60 : 9140 out of 10000 = 0.914; cost=0.0630860080099\n",
      "Epoch 59 of 60 : 9152 out of 10000 = 0.9152; cost=0.0619317536328\n",
      "Epoch 60 of 60 : 9151 out of 10000 = 0.9151; cost=0.0618662751169\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Adagrad\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 60\n",
    "learningRate = 0.005\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='adagrad', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
