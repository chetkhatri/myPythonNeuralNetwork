{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-requisites\n",
    "import numpy as np\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To clear print buffer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing functions from the previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "1\n",
      "(2, 3)\n",
      "[[-0.70293368 -0.85346313  1.24808122]\n",
      " [ 1.55773865  0.24196371  0.39007253]]\n",
      "2\n",
      "(1, 3)\n",
      "[[ 0.35572419  0.95834148  1.11996833]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing weight matrices from layer sizes\n",
    "def initializeWeights(layers):\n",
    "    weights = [np.random.randn(o, i+1) for i, o in zip(layers[:-1], layers[1:])]\n",
    "    return weights\n",
    "\n",
    "# Add a bias term to every data point in the input\n",
    "def addBiasTerms(X):\n",
    "        # Make the input an np.array()\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Forcing 1D vectors to be 2D matrices of 1xlength dimensions\n",
    "        if X.ndim==1:\n",
    "            X = np.reshape(X, (1, len(X)))\n",
    "        \n",
    "        # Inserting bias terms\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))\n",
    "\n",
    "# Forward Propagation of outputs\n",
    "def forwardProp(X, weights):\n",
    "    # Initializing an empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Assigning a name to reuse as inputs\n",
    "    inputs = X\n",
    "    \n",
    "    # For each layer\n",
    "    for w in weights:\n",
    "        # Add bias term to input\n",
    "        inputs = addBiasTerms(inputs)\n",
    "        \n",
    "        # Y = Sigmoid ( X .* W^T )\n",
    "        outputs.append(sigmoid(np.dot(inputs, w.T)))\n",
    "        \n",
    "        # Input of next layer is output of this layer\n",
    "        inputs = outputs[-1]\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "# Compute COST (J) of Neural Network\n",
    "def nnCost(weights, X, Y):\n",
    "    # Calculate yPred\n",
    "    yPred = forwardProp(X, weights)[-1]\n",
    "    \n",
    "    # Compute J\n",
    "    J = 0.5*np.sum((yPred-Y)**2)/len(Y)\n",
    "    \n",
    "    return J\n",
    "\n",
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE\n",
    "# Added eta, the learning rate, as an input\n",
    "def backProp(weights, X, Y, learningRate):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate deltaW for this layer\n",
    "        deltaW = -np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # Change weights of the current layer (W <- W + eta*deltaW)\n",
    "        w += learningRate*deltaW\n",
    "\n",
    "# A function to check accuracy\n",
    "def accuracy(weights, X, Y):\n",
    "    yPred = forwardProp(X, weights)[-1]\n",
    "    return np.sum([np.all((yPred[i]>0.5)!=Y[i]) for i in range(len(Y))])/float(len(Y))\n",
    "\n",
    "# Initialize network\n",
    "layers = [2, 2, 1]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])\n",
    "\n",
    "# Declare input and desired output for AND gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [0], [0], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent is how we have tried to train our network so far - give it ALL the data points, compute ${\\Delta}W$s by summing up quantities across ALL the data points, change all the weights once, Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to train our 3-neuron network to implement Logical XOR.\n",
    "\n",
    "Inputs are: $X=\\left[\\begin{array}{c}(0,0)\\\\(0,1)\\\\(1,0)\\\\(1,1)\\end{array}\\right]$, and the desired output is $Y=\\left[\\begin{array}{c}0\\\\1\\\\1\\\\0\\end{array}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that in order to train the network, we need to call backProp repeatedly. Let us use a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION, USING GD\n",
    "def train(weights, X, Y, nIterations, learningRate=1):\n",
    "    for i in range(nIterations):\n",
    "        # Clears screen output\n",
    "        clear_output()\n",
    "        \n",
    "        print(\"Iteration \"+str(i+1)+\" of \"+str(nIterations))\n",
    "        \n",
    "        # Run backprop\n",
    "        backProp(weights, X, Y, learningRate)\n",
    "        \n",
    "        # Prints Cost and Accuracy\n",
    "        print(\"Cost: \"+str(nnCost(weights, X, Y)))\n",
    "        print(\"Accuracy: \"+str(accuracy(weights, X, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "1\n",
      "(2, 3)\n",
      "[[ 0.72044055  1.07239632 -0.10197306]\n",
      " [-0.26584414 -0.15396786 -1.10604823]]\n",
      "2\n",
      "(1, 3)\n",
      "[[ 0.79691287  1.64620256  1.17547458]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "layers = [2, 2, 1]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "initialWeights = [np.array(w) for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare input and desired output for AND gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.125632295739\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Check current accuracy and cost\n",
    "print(\"Cost: \"+str(nnCost(weights, X, Y)))\n",
    "print(\"Accuracy: \"+str(accuracy(weights, X, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to train our model 600 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400 of 400\n",
      "Cost: 0.0631832731629\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "nIterations = 400\n",
    "train(weights, X, Y, nIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36906037],\n",
       "       [ 0.69797503],\n",
       "       [ 0.57753259],\n",
       "       [ 0.31553578]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardProp(X, weights)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took our function a long time to train.\n",
    "\n",
    "What if we speed up using adaptive learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION, USING GD\n",
    "# Default learning rate = 1.0\n",
    "def trainUsingGD(weights, X, Y, nIterations, learningRate=1.0):\n",
    "    # Setting initial cost to infinity\n",
    "    prevCost = np.inf\n",
    "    \n",
    "    # For nIterations number of iterations:\n",
    "    for i in range(nIterations):\n",
    "        # Run backprop\n",
    "        backProp(weights, X, Y, learningRate)\n",
    "        \n",
    "        clear_output()\n",
    "        print(\"Iteration \"+str(i+1)+\" of \"+str(nIterations))\n",
    "        cost = nnCost(weights, X, Y)\n",
    "        print(\"Cost: \"+str(cost))\n",
    "        \n",
    "        # ADAPT LEARNING RATE\n",
    "        # If cost increases\n",
    "        if (cost > prevCost):\n",
    "            # Halve the learning rate\n",
    "            learningRate /= 2.0\n",
    "        # If cost decreases\n",
    "        else:\n",
    "            # Increase learning rate by 5%\n",
    "            learningRate *= 1.05\n",
    "        \n",
    "        prevCost = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revert weights back to initial values\n",
    "weights = [np.array(w) for w in initialWeights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80 of 80\n",
      "Cost: 0.00331565148282\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train for nIterations\n",
    "# Don't expect same results for running with 20 iterations\n",
    "# as with running twice with 10 iterations - learning rates are different!\n",
    "nIterations = 100\n",
    "trainUsingGD(weights, X, Y, nIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with adaptive learning rate, we reach the desired output much faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "MNIST is a dataset of 60000 images of hand-written numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST DATA\n",
    "# Use numpy.load() to load the .npz file\n",
    "f = np.load('mnist.npz')\n",
    "# Saving the files\n",
    "x_train = f['x_train']\n",
    "y_train = f['y_train']\n",
    "x_test = f['x_test']\n",
    "y_test = f['y_test']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (60000, 28, 28)\n",
      "y_train.shape = (60000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACNCAYAAACT6v+eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noMGUiIiISAD/D2VmfeQeqcmwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11786e5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To check MNIST data\n",
    "print(\"x_train.shape = \"+str(x_train.shape))\n",
    "print(\"y_train.shape = \"+str(y_train.shape))\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "for i in range(20):\n",
    "    ax1 = fig.add_subplot(2, 10, i+1)\n",
    "    ax1.imshow(x_train[i], cmap='gray');\n",
    "    ax1.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In supervised learning) Every (good) dataset consists of a training set and a test set.\n",
    "\n",
    "The training data set consists of data points and their desired outputs.\n",
    "\n",
    "In this case, the data points are grayscale images of hand-written numbers, and their desired outputs are the numbers that have been drawn.\n",
    "\n",
    "The test data set consists of data points whose outputs need to be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the following neural network to classify MNIST data:\n",
    "<center>![MNIST NN](images/digitsNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inintialize network\n",
    "\n",
    "MNIST dataset has images of size 28x28. So the input layer to our network must have $28*28=784$ neurons.\n",
    "\n",
    "Since we are tring to classify whether the image is that of 0 or 1 or 2 ... or 9, we need to have 10 output neurons, each catering to the probability of one number among 0-9.\n",
    "\n",
    "Let our hidden layer (as shown in the diagram) have 15 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before initializing the network though, let's ensure our inputs and outputs are appropriate for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are our inputs in the right format and shape?\n",
    "\n",
    "Remember that we give inputs as np.arrays of $n{\\times}784$ dimensions, $n$ being the number of data points we want to input to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is ``x_train`` an np.array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type of x_train\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, ``x_train`` is an np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is ``x_train`` in the shape required by the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of x_train\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not.\n",
    "\n",
    "We need to reshape this matrix to $60000{\\times}784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping x_train for our network with 784 inputs neurons\n",
    "x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "# Check the dimensions\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our input is in the right format and shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are our inputs normalized?\n",
    "\n",
    "Remember that we had decided to limit the range of values for the input to 0-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are all the values of ``x_train`` between 0 and 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in x_train lie between 0 and 255\n"
     ]
    }
   ],
   "source": [
    "# Check range of values of x_train\n",
    "print(\"Values in x_train lie between \"+str(np.min(x_train))+\" and \"+str(np.max(np.max(x_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs are images, their values range from 0 to 255. We need to bring them down to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize x_train\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in x_train lie between 0.0 and 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check range of values of x_train\n",
    "print(\"Values in x_train lie between \"+str(np.min(x_train))+\" and \"+str(np.max(np.max(x_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are our outputs in the right format and shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is ``y_train`` an np.array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type of y_train\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, ``y_train`` is an np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have 10 neurons in the output layer. That means our output needs to be of ${n{\\times}10}$ dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the shape of ``y_train`` $n{\\times}10$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of y_train\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, ``y_train`` is of shape $60000{\\times}1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are its values like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "4\n",
      "1\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ``y_train`` carries the numbers of the digits the images represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a new binary array of $60000{\\times}10$ and insert a 1 in the column corresponding to the number of the digit its image shows.\n",
    "\n",
    "For example, the first row of our new y_train should look like $\\left[\\begin{array}{c}0&0&0&0&0&1&0&0&0&0\\end{array}\\right]$, since it represents 5. This is called one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make new y_train of nx10 elements\n",
    "new_y_train = np.zeros((len(y_train), 10))\n",
    "for i in range(len(y_train)):\n",
    "    new_y_train[i, y_train[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make new y_test of nx10 elements\n",
    "new_y_test = np.zeros((len(y_test), 10))\n",
    "for i in range(len(y_test)):\n",
    "    new_y_test[i, y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Check first row of y_train\n",
    "print(new_y_train[0])\n",
    "print(new_y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that new_y_train is correctly shaped and formatted, let us reassign the name y_train to the matrix new_y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reassign the name \"y_train\" to new_y_train\n",
    "y_train = new_y_train\n",
    "y_test = new_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "layers = [784, 15, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "initialWeights = [np.array(w) for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"weights:\")\\nfor i in range(len(weights)):\\n    print(i+1); print(weights[i].shape); print(weights[i])\\n'"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please don't print the weights\n",
    "# There are 15*784=11760 weights in the first layer,\n",
    "# + 10*10=150 weights in the second layer\n",
    "'''\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "\n",
    "Use the proper inputs ``x_train`` and ``y_train`` to train your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many iterations do you want to perform? How much should be the learning rate? Should it be adaptive? How many neurons per layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there are 60,000 images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 of 5\n",
      "Cost: 0.866387772823\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Train the network using Gradient Descent\n",
    "# Let's check how much time it takes for 1 iteration\n",
    "nIterations = 5\n",
    "learningRate = 1.0\n",
    "trainUsingGD(weights, x_train, y_train, nIterations, learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See how it takes SO LONG for just one iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem: Batch Gradient Descent computes error, delta, etc. over the entire input data set**\n",
    "\n",
    "Solution: Don't change weights over the entire data set, repeatedly use a randomly sampled subset of the data set.\n",
    "\n",
    "This is called the Monte Carlo method, and in this case it has been developed into Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We shall define a $minibatchSize$. Say $minibatchSize = 100$.\n",
    "\n",
    "SGD:\n",
    "- randomly choose ($minibatchSize$=)100 images out of the entire input data set\n",
    "- use gradient descent on those 100 images to update weights\n",
    "- Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at every iteration we are using gradient descent on only $minibatchSize$ number of images.\n",
    "\n",
    "Mathematical proofs exist on why this works better than gradient descent, under some assumptions (like stationarity, which hold true for our purposes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code Stochastic Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING USING STOCHASTIC GRADIENT DESCENT\n",
    "# Default learning rate = 1.0\n",
    "def trainUsingSGD(weights, X, Y, minibatchSize, nIterations, learningRate=1.0):\n",
    "    # Setting initial cost to infinity\n",
    "    prevCost = np.inf\n",
    "    \n",
    "    # For nIterations number of iterations:\n",
    "    for i in range(nIterations):\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        print(\"Iteration \"+str(i+1)+\" of \"+str(nIterations))\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Choose only the first #minibatchSize number of indices\n",
    "        idx = fullIdx[:minibatchSize]\n",
    "        \n",
    "        # Declare sampled inputs and outputs\n",
    "        xSample = X[idx]\n",
    "        ySample = Y[idx]\n",
    "        \n",
    "        # Run backprop\n",
    "        backProp(weights, xSample, ySample, learningRate)\n",
    "        \n",
    "        '''\n",
    "        # Calculate current cost of the network\n",
    "        cost = nnCost(weights, X, Y)\n",
    "        print(\"Cost: \"+str(cost))\n",
    "        # ADAPT LEARNING RATE\n",
    "        # If cost increases\n",
    "        if (cost > prevCost):\n",
    "            # Halve the learning rate\n",
    "            learningRate /= 2.0\n",
    "        # If cost decreases\n",
    "        else:\n",
    "            # Increase learning rate by 5%\n",
    "            learningRate *= 1.05\n",
    "        '''\n",
    "        \n",
    "        # Updating prevCost at end of iteration\n",
    "        prevCost = cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let us define an evaluation metric for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(weights, X, Y):\n",
    "    yPreds = forwardProp(X, weights)[-1]\n",
    "    test_results = [(np.argmax(yPreds[i]), np.argmax(Y[i]))\n",
    "                        for i in range(len(Y))]\n",
    "    yes = sum(int(x == y) for (x, y) in test_results)\n",
    "    print(str(yes)+\" out of \"+str(len(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SGD, training should take lesser time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "layers = [784, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "initialWeights = [np.array(w) for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908 out of 60000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate initial weights on training data\n",
    "evaluate(weights, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801 out of 10000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate initial weights on test data\n",
    "evaluate(weights, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 1000\n",
      "Iteration 2 of 1000\n",
      "Iteration 3 of 1000\n",
      "Iteration 4 of 1000\n",
      "Iteration 5 of 1000\n",
      "Iteration 6 of 1000\n",
      "Iteration 7 of 1000\n",
      "Iteration 8 of 1000\n",
      "Iteration 9 of 1000\n",
      "Iteration 10 of 1000\n",
      "Iteration 11 of 1000\n",
      "Iteration 12 of 1000\n",
      "Iteration 13 of 1000\n",
      "Iteration 14 of 1000\n",
      "Iteration 15 of 1000\n",
      "Iteration 16 of 1000\n",
      "Iteration 17 of 1000\n",
      "Iteration 18 of 1000\n",
      "Iteration 19 of 1000\n",
      "Iteration 20 of 1000\n",
      "Iteration 21 of 1000\n",
      "Iteration 22 of 1000\n",
      "Iteration 23 of 1000\n",
      "Iteration 24 of 1000\n",
      "Iteration 25 of 1000\n",
      "Iteration 26 of 1000\n",
      "Iteration 27 of 1000\n",
      "Iteration 28 of 1000\n",
      "Iteration 29 of 1000\n",
      "Iteration 30 of 1000\n",
      "Iteration 31 of 1000\n",
      "Iteration 32 of 1000\n",
      "Iteration 33 of 1000\n",
      "Iteration 34 of 1000\n",
      "Iteration 35 of 1000\n",
      "Iteration 36 of 1000\n",
      "Iteration 37 of 1000\n",
      "Iteration 38 of 1000\n",
      "Iteration 39 of 1000\n",
      "Iteration 40 of 1000\n",
      "Iteration 41 of 1000\n",
      "Iteration 42 of 1000\n",
      "Iteration 43 of 1000\n",
      "Iteration 44 of 1000\n",
      "Iteration 45 of 1000\n",
      "Iteration 46 of 1000\n",
      "Iteration 47 of 1000\n",
      "Iteration 48 of 1000\n",
      "Iteration 49 of 1000\n",
      "Iteration 50 of 1000\n",
      "Iteration 51 of 1000\n",
      "Iteration 52 of 1000\n",
      "Iteration 53 of 1000\n",
      "Iteration 54 of 1000\n",
      "Iteration 55 of 1000\n",
      "Iteration 56 of 1000\n",
      "Iteration 57 of 1000\n",
      "Iteration 58 of 1000\n",
      "Iteration 59 of 1000\n",
      "Iteration 60 of 1000\n",
      "Iteration 61 of 1000\n",
      "Iteration 62 of 1000\n",
      "Iteration 63 of 1000\n",
      "Iteration 64 of 1000\n",
      "Iteration 65 of 1000\n",
      "Iteration 66 of 1000\n",
      "Iteration 67 of 1000\n",
      "Iteration 68 of 1000\n",
      "Iteration 69 of 1000\n",
      "Iteration 70 of 1000\n",
      "Iteration 71 of 1000\n",
      "Iteration 72 of 1000\n",
      "Iteration 73 of 1000\n",
      "Iteration 74 of 1000\n",
      "Iteration 75 of 1000\n",
      "Iteration 76 of 1000\n",
      "Iteration 77 of 1000\n",
      "Iteration 78 of 1000\n",
      "Iteration 79 of 1000\n",
      "Iteration 80 of 1000\n",
      "Iteration 81 of 1000\n",
      "Iteration 82 of 1000\n",
      "Iteration 83 of 1000\n",
      "Iteration 84 of 1000\n",
      "Iteration 85 of 1000\n",
      "Iteration 86 of 1000\n",
      "Iteration 87 of 1000\n",
      "Iteration 88 of 1000\n",
      "Iteration 89 of 1000\n",
      "Iteration 90 of 1000\n",
      "Iteration 91 of 1000\n",
      "Iteration 92 of 1000\n",
      "Iteration 93 of 1000\n",
      "Iteration 94 of 1000\n",
      "Iteration 95 of 1000\n",
      "Iteration 96 of 1000\n",
      "Iteration 97 of 1000\n",
      "Iteration 98 of 1000\n",
      "Iteration 99 of 1000\n",
      "Iteration 100 of 1000\n",
      "Iteration 101 of 1000\n",
      "Iteration 102 of 1000\n",
      "Iteration 103 of 1000\n",
      "Iteration 104 of 1000\n",
      "Iteration 105 of 1000\n",
      "Iteration 106 of 1000\n",
      "Iteration 107 of 1000\n",
      "Iteration 108 of 1000\n",
      "Iteration 109 of 1000\n",
      "Iteration 110 of 1000\n",
      "Iteration 111 of 1000\n",
      "Iteration 112 of 1000\n",
      "Iteration 113 of 1000\n",
      "Iteration 114 of 1000\n",
      "Iteration 115 of 1000\n",
      "Iteration 116 of 1000\n",
      "Iteration 117 of 1000\n",
      "Iteration 118 of 1000\n",
      "Iteration 119 of 1000\n",
      "Iteration 120 of 1000\n",
      "Iteration 121 of 1000\n",
      "Iteration 122 of 1000\n",
      "Iteration 123 of 1000\n",
      "Iteration 124 of 1000\n",
      "Iteration 125 of 1000\n",
      "Iteration 126 of 1000\n",
      "Iteration 127 of 1000\n",
      "Iteration 128 of 1000\n",
      "Iteration 129 of 1000\n",
      "Iteration 130 of 1000\n",
      "Iteration 131 of 1000\n",
      "Iteration 132 of 1000\n",
      "Iteration 133 of 1000\n",
      "Iteration 134 of 1000\n",
      "Iteration 135 of 1000\n",
      "Iteration 136 of 1000\n",
      "Iteration 137 of 1000\n",
      "Iteration 138 of 1000\n",
      "Iteration 139 of 1000\n",
      "Iteration 140 of 1000\n",
      "Iteration 141 of 1000\n",
      "Iteration 142 of 1000\n",
      "Iteration 143 of 1000\n",
      "Iteration 144 of 1000\n",
      "Iteration 145 of 1000\n",
      "Iteration 146 of 1000\n",
      "Iteration 147 of 1000\n",
      "Iteration 148 of 1000\n",
      "Iteration 149 of 1000\n",
      "Iteration 150 of 1000\n",
      "Iteration 151 of 1000\n",
      "Iteration 152 of 1000\n",
      "Iteration 153 of 1000\n",
      "Iteration 154 of 1000\n",
      "Iteration 155 of 1000\n",
      "Iteration 156 of 1000\n",
      "Iteration 157 of 1000\n",
      "Iteration 158 of 1000\n",
      "Iteration 159 of 1000\n",
      "Iteration 160 of 1000\n",
      "Iteration 161 of 1000\n",
      "Iteration 162 of 1000\n",
      "Iteration 163 of 1000\n",
      "Iteration 164 of 1000\n",
      "Iteration 165 of 1000\n",
      "Iteration 166 of 1000\n",
      "Iteration 167 of 1000\n",
      "Iteration 168 of 1000\n",
      "Iteration 169 of 1000\n",
      "Iteration 170 of 1000\n",
      "Iteration 171 of 1000\n",
      "Iteration 172 of 1000\n",
      "Iteration 173 of 1000\n",
      "Iteration 174 of 1000\n",
      "Iteration 175 of 1000\n",
      "Iteration 176 of 1000\n",
      "Iteration 177 of 1000\n",
      "Iteration 178 of 1000\n",
      "Iteration 179 of 1000\n",
      "Iteration 180 of 1000\n",
      "Iteration 181 of 1000\n",
      "Iteration 182 of 1000\n",
      "Iteration 183 of 1000\n",
      "Iteration 184 of 1000\n",
      "Iteration 185 of 1000\n",
      "Iteration 186 of 1000\n",
      "Iteration 187 of 1000\n",
      "Iteration 188 of 1000\n",
      "Iteration 189 of 1000\n",
      "Iteration 190 of 1000\n",
      "Iteration 191 of 1000\n",
      "Iteration 192 of 1000\n",
      "Iteration 193 of 1000\n",
      "Iteration 194 of 1000\n",
      "Iteration 195 of 1000\n",
      "Iteration 196 of 1000\n",
      "Iteration 197 of 1000\n",
      "Iteration 198 of 1000\n",
      "Iteration 199 of 1000\n",
      "Iteration 200 of 1000\n",
      "Iteration 201 of 1000\n",
      "Iteration 202 of 1000\n",
      "Iteration 203 of 1000\n",
      "Iteration 204 of 1000\n",
      "Iteration 205 of 1000\n",
      "Iteration 206 of 1000\n",
      "Iteration 207 of 1000\n",
      "Iteration 208 of 1000\n",
      "Iteration 209 of 1000\n",
      "Iteration 210 of 1000\n",
      "Iteration 211 of 1000\n",
      "Iteration 212 of 1000\n",
      "Iteration 213 of 1000\n",
      "Iteration 214 of 1000\n",
      "Iteration 215 of 1000\n",
      "Iteration 216 of 1000\n",
      "Iteration 217 of 1000\n",
      "Iteration 218 of 1000\n",
      "Iteration 219 of 1000\n",
      "Iteration 220 of 1000\n",
      "Iteration 221 of 1000\n",
      "Iteration 222 of 1000\n",
      "Iteration 223 of 1000\n",
      "Iteration 224 of 1000\n",
      "Iteration 225 of 1000\n",
      "Iteration 226 of 1000\n",
      "Iteration 227 of 1000\n",
      "Iteration 228 of 1000\n",
      "Iteration 229 of 1000\n",
      "Iteration 230 of 1000\n",
      "Iteration 231 of 1000\n",
      "Iteration 232 of 1000\n",
      "Iteration 233 of 1000\n",
      "Iteration 234 of 1000\n",
      "Iteration 235 of 1000\n",
      "Iteration 236 of 1000\n",
      "Iteration 237 of 1000\n",
      "Iteration 238 of 1000\n",
      "Iteration 239 of 1000\n",
      "Iteration 240 of 1000\n",
      "Iteration 241 of 1000\n",
      "Iteration 242 of 1000\n",
      "Iteration 243 of 1000\n",
      "Iteration 244 of 1000\n",
      "Iteration 245 of 1000\n",
      "Iteration 246 of 1000\n",
      "Iteration 247 of 1000\n",
      "Iteration 248 of 1000\n",
      "Iteration 249 of 1000\n",
      "Iteration 250 of 1000\n",
      "Iteration 251 of 1000\n",
      "Iteration 252 of 1000\n",
      "Iteration 253 of 1000\n",
      "Iteration 254 of 1000\n",
      "Iteration 255 of 1000\n",
      "Iteration 256 of 1000\n",
      "Iteration 257 of 1000\n",
      "Iteration 258 of 1000\n",
      "Iteration 259 of 1000\n",
      "Iteration 260 of 1000\n",
      "Iteration 261 of 1000\n",
      "Iteration 262 of 1000\n",
      "Iteration 263 of 1000\n",
      "Iteration 264 of 1000\n",
      "Iteration 265 of 1000\n",
      "Iteration 266 of 1000\n",
      "Iteration 267 of 1000\n",
      "Iteration 268 of 1000\n",
      "Iteration 269 of 1000\n",
      "Iteration 270 of 1000\n",
      "Iteration 271 of 1000\n",
      "Iteration 272 of 1000\n",
      "Iteration 273 of 1000\n",
      "Iteration 274 of 1000\n",
      "Iteration 275 of 1000\n",
      "Iteration 276 of 1000\n",
      "Iteration 277 of 1000\n",
      "Iteration 278 of 1000\n",
      "Iteration 279 of 1000\n",
      "Iteration 280 of 1000\n",
      "Iteration 281 of 1000\n",
      "Iteration 282 of 1000\n",
      "Iteration 283 of 1000\n",
      "Iteration 284 of 1000\n",
      "Iteration 285 of 1000\n",
      "Iteration 286 of 1000\n",
      "Iteration 287 of 1000\n",
      "Iteration 288 of 1000\n",
      "Iteration 289 of 1000\n",
      "Iteration 290 of 1000\n",
      "Iteration 291 of 1000\n",
      "Iteration 292 of 1000\n",
      "Iteration 293 of 1000\n",
      "Iteration 294 of 1000\n",
      "Iteration 295 of 1000\n",
      "Iteration 296 of 1000\n",
      "Iteration 297 of 1000\n",
      "Iteration 298 of 1000\n",
      "Iteration 299 of 1000\n",
      "Iteration 300 of 1000\n",
      "Iteration 301 of 1000\n",
      "Iteration 302 of 1000\n",
      "Iteration 303 of 1000\n",
      "Iteration 304 of 1000\n",
      "Iteration 305 of 1000\n",
      "Iteration 306 of 1000\n",
      "Iteration 307 of 1000\n",
      "Iteration 308 of 1000\n",
      "Iteration 309 of 1000\n",
      "Iteration 310 of 1000\n",
      "Iteration 311 of 1000\n",
      "Iteration 312 of 1000\n",
      "Iteration 313 of 1000\n",
      "Iteration 314 of 1000\n",
      "Iteration 315 of 1000\n",
      "Iteration 316 of 1000\n",
      "Iteration 317 of 1000\n",
      "Iteration 318 of 1000\n",
      "Iteration 319 of 1000\n",
      "Iteration 320 of 1000\n",
      "Iteration 321 of 1000\n",
      "Iteration 322 of 1000\n",
      "Iteration 323 of 1000\n",
      "Iteration 324 of 1000\n",
      "Iteration 325 of 1000\n",
      "Iteration 326 of 1000\n",
      "Iteration 327 of 1000\n",
      "Iteration 328 of 1000\n",
      "Iteration 329 of 1000\n",
      "Iteration 330 of 1000\n",
      "Iteration 331 of 1000\n",
      "Iteration 332 of 1000\n",
      "Iteration 333 of 1000\n",
      "Iteration 334 of 1000\n",
      "Iteration 335 of 1000\n",
      "Iteration 336 of 1000\n",
      "Iteration 337 of 1000\n",
      "Iteration 338 of 1000\n",
      "Iteration 339 of 1000\n",
      "Iteration 340 of 1000\n",
      "Iteration 341 of 1000\n",
      "Iteration 342 of 1000\n",
      "Iteration 343 of 1000\n",
      "Iteration 344 of 1000\n",
      "Iteration 345 of 1000\n",
      "Iteration 346 of 1000\n",
      "Iteration 347 of 1000\n",
      "Iteration 348 of 1000\n",
      "Iteration 349 of 1000\n",
      "Iteration 350 of 1000\n",
      "Iteration 351 of 1000\n",
      "Iteration 352 of 1000\n",
      "Iteration 353 of 1000\n",
      "Iteration 354 of 1000\n",
      "Iteration 355 of 1000\n",
      "Iteration 356 of 1000\n",
      "Iteration 357 of 1000\n",
      "Iteration 358 of 1000\n",
      "Iteration 359 of 1000\n",
      "Iteration 360 of 1000\n",
      "Iteration 361 of 1000\n",
      "Iteration 362 of 1000\n",
      "Iteration 363 of 1000\n",
      "Iteration 364 of 1000\n",
      "Iteration 365 of 1000\n",
      "Iteration 366 of 1000\n",
      "Iteration 367 of 1000\n",
      "Iteration 368 of 1000\n",
      "Iteration 369 of 1000\n",
      "Iteration 370 of 1000\n",
      "Iteration 371 of 1000\n",
      "Iteration 372 of 1000\n",
      "Iteration 373 of 1000\n",
      "Iteration 374 of 1000\n",
      "Iteration 375 of 1000\n",
      "Iteration 376 of 1000\n",
      "Iteration 377 of 1000\n",
      "Iteration 378 of 1000\n",
      "Iteration 379 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 380 of 1000\n",
      "Iteration 381 of 1000\n",
      "Iteration 382 of 1000\n",
      "Iteration 383 of 1000\n",
      "Iteration 384 of 1000\n",
      "Iteration 385 of 1000\n",
      "Iteration 386 of 1000\n",
      "Iteration 387 of 1000\n",
      "Iteration 388 of 1000\n",
      "Iteration 389 of 1000\n",
      "Iteration 390 of 1000\n",
      "Iteration 391 of 1000\n",
      "Iteration 392 of 1000\n",
      "Iteration 393 of 1000\n",
      "Iteration 394 of 1000\n",
      "Iteration 395 of 1000\n",
      "Iteration 396 of 1000\n",
      "Iteration 397 of 1000\n",
      "Iteration 398 of 1000\n",
      "Iteration 399 of 1000\n",
      "Iteration 400 of 1000\n",
      "Iteration 401 of 1000\n",
      "Iteration 402 of 1000\n",
      "Iteration 403 of 1000\n",
      "Iteration 404 of 1000\n",
      "Iteration 405 of 1000\n",
      "Iteration 406 of 1000\n",
      "Iteration 407 of 1000\n",
      "Iteration 408 of 1000\n",
      "Iteration 409 of 1000\n",
      "Iteration 410 of 1000\n",
      "Iteration 411 of 1000\n",
      "Iteration 412 of 1000\n",
      "Iteration 413 of 1000\n",
      "Iteration 414 of 1000\n",
      "Iteration 415 of 1000\n",
      "Iteration 416 of 1000\n",
      "Iteration 417 of 1000\n",
      "Iteration 418 of 1000\n",
      "Iteration 419 of 1000\n",
      "Iteration 420 of 1000\n",
      "Iteration 421 of 1000\n",
      "Iteration 422 of 1000\n",
      "Iteration 423 of 1000\n",
      "Iteration 424 of 1000\n",
      "Iteration 425 of 1000\n",
      "Iteration 426 of 1000\n",
      "Iteration 427 of 1000\n",
      "Iteration 428 of 1000\n",
      "Iteration 429 of 1000\n",
      "Iteration 430 of 1000\n",
      "Iteration 431 of 1000\n",
      "Iteration 432 of 1000\n",
      "Iteration 433 of 1000\n",
      "Iteration 434 of 1000\n",
      "Iteration 435 of 1000\n",
      "Iteration 436 of 1000\n",
      "Iteration 437 of 1000\n",
      "Iteration 438 of 1000\n",
      "Iteration 439 of 1000\n",
      "Iteration 440 of 1000\n",
      "Iteration 441 of 1000\n",
      "Iteration 442 of 1000\n",
      "Iteration 443 of 1000\n",
      "Iteration 444 of 1000\n",
      "Iteration 445 of 1000\n",
      "Iteration 446 of 1000\n",
      "Iteration 447 of 1000\n",
      "Iteration 448 of 1000\n",
      "Iteration 449 of 1000\n",
      "Iteration 450 of 1000\n",
      "Iteration 451 of 1000\n",
      "Iteration 452 of 1000\n",
      "Iteration 453 of 1000\n",
      "Iteration 454 of 1000\n",
      "Iteration 455 of 1000\n",
      "Iteration 456 of 1000\n",
      "Iteration 457 of 1000\n",
      "Iteration 458 of 1000\n",
      "Iteration 459 of 1000\n",
      "Iteration 460 of 1000\n",
      "Iteration 461 of 1000\n",
      "Iteration 462 of 1000\n",
      "Iteration 463 of 1000\n",
      "Iteration 464 of 1000\n",
      "Iteration 465 of 1000\n",
      "Iteration 466 of 1000\n",
      "Iteration 467 of 1000\n",
      "Iteration 468 of 1000\n",
      "Iteration 469 of 1000\n",
      "Iteration 470 of 1000\n",
      "Iteration 471 of 1000\n",
      "Iteration 472 of 1000\n",
      "Iteration 473 of 1000\n",
      "Iteration 474 of 1000\n",
      "Iteration 475 of 1000\n",
      "Iteration 476 of 1000\n",
      "Iteration 477 of 1000\n",
      "Iteration 478 of 1000\n",
      "Iteration 479 of 1000\n",
      "Iteration 480 of 1000\n",
      "Iteration 481 of 1000\n",
      "Iteration 482 of 1000\n",
      "Iteration 483 of 1000\n",
      "Iteration 484 of 1000\n",
      "Iteration 485 of 1000\n",
      "Iteration 486 of 1000\n",
      "Iteration 487 of 1000\n",
      "Iteration 488 of 1000\n",
      "Iteration 489 of 1000\n",
      "Iteration 490 of 1000\n",
      "Iteration 491 of 1000\n",
      "Iteration 492 of 1000\n",
      "Iteration 493 of 1000\n",
      "Iteration 494 of 1000\n",
      "Iteration 495 of 1000\n",
      "Iteration 496 of 1000\n",
      "Iteration 497 of 1000\n",
      "Iteration 498 of 1000\n",
      "Iteration 499 of 1000\n",
      "Iteration 500 of 1000\n",
      "Iteration 501 of 1000\n",
      "Iteration 502 of 1000\n",
      "Iteration 503 of 1000\n",
      "Iteration 504 of 1000\n",
      "Iteration 505 of 1000\n",
      "Iteration 506 of 1000\n",
      "Iteration 507 of 1000\n",
      "Iteration 508 of 1000\n",
      "Iteration 509 of 1000\n",
      "Iteration 510 of 1000\n",
      "Iteration 511 of 1000\n",
      "Iteration 512 of 1000\n",
      "Iteration 513 of 1000\n",
      "Iteration 514 of 1000\n",
      "Iteration 515 of 1000\n",
      "Iteration 516 of 1000\n",
      "Iteration 517 of 1000\n",
      "Iteration 518 of 1000\n",
      "Iteration 519 of 1000\n",
      "Iteration 520 of 1000\n",
      "Iteration 521 of 1000\n",
      "Iteration 522 of 1000\n",
      "Iteration 523 of 1000\n",
      "Iteration 524 of 1000\n",
      "Iteration 525 of 1000\n",
      "Iteration 526 of 1000\n",
      "Iteration 527 of 1000\n",
      "Iteration 528 of 1000\n",
      "Iteration 529 of 1000\n",
      "Iteration 530 of 1000\n",
      "Iteration 531 of 1000\n",
      "Iteration 532 of 1000\n",
      "Iteration 533 of 1000\n",
      "Iteration 534 of 1000\n",
      "Iteration 535 of 1000\n",
      "Iteration 536 of 1000\n",
      "Iteration 537 of 1000\n",
      "Iteration 538 of 1000\n",
      "Iteration 539 of 1000\n",
      "Iteration 540 of 1000\n",
      "Iteration 541 of 1000\n",
      "Iteration 542 of 1000\n",
      "Iteration 543 of 1000\n",
      "Iteration 544 of 1000\n",
      "Iteration 545 of 1000\n",
      "Iteration 546 of 1000\n",
      "Iteration 547 of 1000\n",
      "Iteration 548 of 1000\n",
      "Iteration 549 of 1000\n",
      "Iteration 550 of 1000\n",
      "Iteration 551 of 1000\n",
      "Iteration 552 of 1000\n",
      "Iteration 553 of 1000\n",
      "Iteration 554 of 1000\n",
      "Iteration 555 of 1000\n",
      "Iteration 556 of 1000\n",
      "Iteration 557 of 1000\n",
      "Iteration 558 of 1000\n",
      "Iteration 559 of 1000\n",
      "Iteration 560 of 1000\n",
      "Iteration 561 of 1000\n",
      "Iteration 562 of 1000\n",
      "Iteration 563 of 1000\n",
      "Iteration 564 of 1000\n",
      "Iteration 565 of 1000\n",
      "Iteration 566 of 1000\n",
      "Iteration 567 of 1000\n",
      "Iteration 568 of 1000\n",
      "Iteration 569 of 1000\n",
      "Iteration 570 of 1000\n",
      "Iteration 571 of 1000\n",
      "Iteration 572 of 1000\n",
      "Iteration 573 of 1000\n",
      "Iteration 574 of 1000\n",
      "Iteration 575 of 1000\n",
      "Iteration 576 of 1000\n",
      "Iteration 577 of 1000\n",
      "Iteration 578 of 1000\n",
      "Iteration 579 of 1000\n",
      "Iteration 580 of 1000\n",
      "Iteration 581 of 1000\n",
      "Iteration 582 of 1000\n",
      "Iteration 583 of 1000\n",
      "Iteration 584 of 1000\n",
      "Iteration 585 of 1000\n",
      "Iteration 586 of 1000\n",
      "Iteration 587 of 1000\n",
      "Iteration 588 of 1000\n",
      "Iteration 589 of 1000\n",
      "Iteration 590 of 1000\n",
      "Iteration 591 of 1000\n",
      "Iteration 592 of 1000\n",
      "Iteration 593 of 1000\n",
      "Iteration 594 of 1000\n",
      "Iteration 595 of 1000\n",
      "Iteration 596 of 1000\n",
      "Iteration 597 of 1000\n",
      "Iteration 598 of 1000\n",
      "Iteration 599 of 1000\n",
      "Iteration 600 of 1000\n",
      "Iteration 601 of 1000\n",
      "Iteration 602 of 1000\n",
      "Iteration 603 of 1000\n",
      "Iteration 604 of 1000\n",
      "Iteration 605 of 1000\n",
      "Iteration 606 of 1000\n",
      "Iteration 607 of 1000\n",
      "Iteration 608 of 1000\n",
      "Iteration 609 of 1000\n",
      "Iteration 610 of 1000\n",
      "Iteration 611 of 1000\n",
      "Iteration 612 of 1000\n",
      "Iteration 613 of 1000\n",
      "Iteration 614 of 1000\n",
      "Iteration 615 of 1000\n",
      "Iteration 616 of 1000\n",
      "Iteration 617 of 1000\n",
      "Iteration 618 of 1000\n",
      "Iteration 619 of 1000\n",
      "Iteration 620 of 1000\n",
      "Iteration 621 of 1000\n",
      "Iteration 622 of 1000\n",
      "Iteration 623 of 1000\n",
      "Iteration 624 of 1000\n",
      "Iteration 625 of 1000\n",
      "Iteration 626 of 1000\n",
      "Iteration 627 of 1000\n",
      "Iteration 628 of 1000\n",
      "Iteration 629 of 1000\n",
      "Iteration 630 of 1000\n",
      "Iteration 631 of 1000\n",
      "Iteration 632 of 1000\n",
      "Iteration 633 of 1000\n",
      "Iteration 634 of 1000\n",
      "Iteration 635 of 1000\n",
      "Iteration 636 of 1000\n",
      "Iteration 637 of 1000\n",
      "Iteration 638 of 1000\n",
      "Iteration 639 of 1000\n",
      "Iteration 640 of 1000\n",
      "Iteration 641 of 1000\n",
      "Iteration 642 of 1000\n",
      "Iteration 643 of 1000\n",
      "Iteration 644 of 1000\n",
      "Iteration 645 of 1000\n",
      "Iteration 646 of 1000\n",
      "Iteration 647 of 1000\n",
      "Iteration 648 of 1000\n",
      "Iteration 649 of 1000\n",
      "Iteration 650 of 1000\n",
      "Iteration 651 of 1000\n",
      "Iteration 652 of 1000\n",
      "Iteration 653 of 1000\n",
      "Iteration 654 of 1000\n",
      "Iteration 655 of 1000\n",
      "Iteration 656 of 1000\n",
      "Iteration 657 of 1000\n",
      "Iteration 658 of 1000\n",
      "Iteration 659 of 1000\n",
      "Iteration 660 of 1000\n",
      "Iteration 661 of 1000\n",
      "Iteration 662 of 1000\n",
      "Iteration 663 of 1000\n",
      "Iteration 664 of 1000\n",
      "Iteration 665 of 1000\n",
      "Iteration 666 of 1000\n",
      "Iteration 667 of 1000\n",
      "Iteration 668 of 1000\n",
      "Iteration 669 of 1000\n",
      "Iteration 670 of 1000\n",
      "Iteration 671 of 1000\n",
      "Iteration 672 of 1000\n",
      "Iteration 673 of 1000\n",
      "Iteration 674 of 1000\n",
      "Iteration 675 of 1000\n",
      "Iteration 676 of 1000\n",
      "Iteration 677 of 1000\n",
      "Iteration 678 of 1000\n",
      "Iteration 679 of 1000\n",
      "Iteration 680 of 1000\n",
      "Iteration 681 of 1000\n",
      "Iteration 682 of 1000\n",
      "Iteration 683 of 1000\n",
      "Iteration 684 of 1000\n",
      "Iteration 685 of 1000\n",
      "Iteration 686 of 1000\n",
      "Iteration 687 of 1000\n",
      "Iteration 688 of 1000\n",
      "Iteration 689 of 1000\n",
      "Iteration 690 of 1000\n",
      "Iteration 691 of 1000\n",
      "Iteration 692 of 1000\n",
      "Iteration 693 of 1000\n",
      "Iteration 694 of 1000\n",
      "Iteration 695 of 1000\n",
      "Iteration 696 of 1000\n",
      "Iteration 697 of 1000\n",
      "Iteration 698 of 1000\n",
      "Iteration 699 of 1000\n",
      "Iteration 700 of 1000\n",
      "Iteration 701 of 1000\n",
      "Iteration 702 of 1000\n",
      "Iteration 703 of 1000\n",
      "Iteration 704 of 1000\n",
      "Iteration 705 of 1000\n",
      "Iteration 706 of 1000\n",
      "Iteration 707 of 1000\n",
      "Iteration 708 of 1000\n",
      "Iteration 709 of 1000\n",
      "Iteration 710 of 1000\n",
      "Iteration 711 of 1000\n",
      "Iteration 712 of 1000\n",
      "Iteration 713 of 1000\n",
      "Iteration 714 of 1000\n",
      "Iteration 715 of 1000\n",
      "Iteration 716 of 1000\n",
      "Iteration 717 of 1000\n",
      "Iteration 718 of 1000\n",
      "Iteration 719 of 1000\n",
      "Iteration 720 of 1000\n",
      "Iteration 721 of 1000\n",
      "Iteration 722 of 1000\n",
      "Iteration 723 of 1000\n",
      "Iteration 724 of 1000\n",
      "Iteration 725 of 1000\n",
      "Iteration 726 of 1000\n",
      "Iteration 727 of 1000\n",
      "Iteration 728 of 1000\n",
      "Iteration 729 of 1000\n",
      "Iteration 730 of 1000\n",
      "Iteration 731 of 1000\n",
      "Iteration 732 of 1000\n",
      "Iteration 733 of 1000\n",
      "Iteration 734 of 1000\n",
      "Iteration 735 of 1000\n",
      "Iteration 736 of 1000\n",
      "Iteration 737 of 1000\n",
      "Iteration 738 of 1000\n",
      "Iteration 739 of 1000\n",
      "Iteration 740 of 1000\n",
      "Iteration 741 of 1000\n",
      "Iteration 742 of 1000\n",
      "Iteration 743 of 1000\n",
      "Iteration 744 of 1000\n",
      "Iteration 745 of 1000\n",
      "Iteration 746 of 1000\n",
      "Iteration 747 of 1000\n",
      "Iteration 748 of 1000\n",
      "Iteration 749 of 1000\n",
      "Iteration 750 of 1000\n",
      "Iteration 751 of 1000\n",
      "Iteration 752 of 1000\n",
      "Iteration 753 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 754 of 1000\n",
      "Iteration 755 of 1000\n",
      "Iteration 756 of 1000\n",
      "Iteration 757 of 1000\n",
      "Iteration 758 of 1000\n",
      "Iteration 759 of 1000\n",
      "Iteration 760 of 1000\n",
      "Iteration 761 of 1000\n",
      "Iteration 762 of 1000\n",
      "Iteration 763 of 1000\n",
      "Iteration 764 of 1000\n",
      "Iteration 765 of 1000\n",
      "Iteration 766 of 1000\n",
      "Iteration 767 of 1000\n",
      "Iteration 768 of 1000\n",
      "Iteration 769 of 1000\n",
      "Iteration 770 of 1000\n",
      "Iteration 771 of 1000\n",
      "Iteration 772 of 1000\n",
      "Iteration 773 of 1000\n",
      "Iteration 774 of 1000\n",
      "Iteration 775 of 1000\n",
      "Iteration 776 of 1000\n",
      "Iteration 777 of 1000\n",
      "Iteration 778 of 1000\n",
      "Iteration 779 of 1000\n",
      "Iteration 780 of 1000\n",
      "Iteration 781 of 1000\n",
      "Iteration 782 of 1000\n",
      "Iteration 783 of 1000\n",
      "Iteration 784 of 1000\n",
      "Iteration 785 of 1000\n",
      "Iteration 786 of 1000\n",
      "Iteration 787 of 1000\n",
      "Iteration 788 of 1000\n",
      "Iteration 789 of 1000\n",
      "Iteration 790 of 1000\n",
      "Iteration 791 of 1000\n",
      "Iteration 792 of 1000\n",
      "Iteration 793 of 1000\n",
      "Iteration 794 of 1000\n",
      "Iteration 795 of 1000\n",
      "Iteration 796 of 1000\n",
      "Iteration 797 of 1000\n",
      "Iteration 798 of 1000\n",
      "Iteration 799 of 1000\n",
      "Iteration 800 of 1000\n",
      "Iteration 801 of 1000\n",
      "Iteration 802 of 1000\n",
      "Iteration 803 of 1000\n",
      "Iteration 804 of 1000\n",
      "Iteration 805 of 1000\n",
      "Iteration 806 of 1000\n",
      "Iteration 807 of 1000\n",
      "Iteration 808 of 1000\n",
      "Iteration 809 of 1000\n",
      "Iteration 810 of 1000\n",
      "Iteration 811 of 1000\n",
      "Iteration 812 of 1000\n",
      "Iteration 813 of 1000\n",
      "Iteration 814 of 1000\n",
      "Iteration 815 of 1000\n",
      "Iteration 816 of 1000\n",
      "Iteration 817 of 1000\n",
      "Iteration 818 of 1000\n",
      "Iteration 819 of 1000\n",
      "Iteration 820 of 1000\n",
      "Iteration 821 of 1000\n",
      "Iteration 822 of 1000\n",
      "Iteration 823 of 1000\n",
      "Iteration 824 of 1000\n",
      "Iteration 825 of 1000\n",
      "Iteration 826 of 1000\n",
      "Iteration 827 of 1000\n",
      "Iteration 828 of 1000\n",
      "Iteration 829 of 1000\n",
      "Iteration 830 of 1000\n",
      "Iteration 831 of 1000\n",
      "Iteration 832 of 1000\n",
      "Iteration 833 of 1000\n",
      "Iteration 834 of 1000\n",
      "Iteration 835 of 1000\n",
      "Iteration 836 of 1000\n",
      "Iteration 837 of 1000\n",
      "Iteration 838 of 1000\n",
      "Iteration 839 of 1000\n",
      "Iteration 840 of 1000\n",
      "Iteration 841 of 1000\n",
      "Iteration 842 of 1000\n",
      "Iteration 843 of 1000\n",
      "Iteration 844 of 1000\n",
      "Iteration 845 of 1000\n",
      "Iteration 846 of 1000\n",
      "Iteration 847 of 1000\n",
      "Iteration 848 of 1000\n",
      "Iteration 849 of 1000\n",
      "Iteration 850 of 1000\n",
      "Iteration 851 of 1000\n",
      "Iteration 852 of 1000\n",
      "Iteration 853 of 1000\n",
      "Iteration 854 of 1000\n",
      "Iteration 855 of 1000\n",
      "Iteration 856 of 1000\n",
      "Iteration 857 of 1000\n",
      "Iteration 858 of 1000\n",
      "Iteration 859 of 1000\n",
      "Iteration 860 of 1000\n",
      "Iteration 861 of 1000\n",
      "Iteration 862 of 1000\n",
      "Iteration 863 of 1000\n",
      "Iteration 864 of 1000\n",
      "Iteration 865 of 1000\n",
      "Iteration 866 of 1000\n",
      "Iteration 867 of 1000\n",
      "Iteration 868 of 1000\n",
      "Iteration 869 of 1000\n",
      "Iteration 870 of 1000\n",
      "Iteration 871 of 1000\n",
      "Iteration 872 of 1000\n",
      "Iteration 873 of 1000\n",
      "Iteration 874 of 1000\n",
      "Iteration 875 of 1000\n",
      "Iteration 876 of 1000\n",
      "Iteration 877 of 1000\n",
      "Iteration 878 of 1000\n",
      "Iteration 879 of 1000\n",
      "Iteration 880 of 1000\n",
      "Iteration 881 of 1000\n",
      "Iteration 882 of 1000\n",
      "Iteration 883 of 1000\n",
      "Iteration 884 of 1000\n",
      "Iteration 885 of 1000\n",
      "Iteration 886 of 1000\n",
      "Iteration 887 of 1000\n",
      "Iteration 888 of 1000\n",
      "Iteration 889 of 1000\n",
      "Iteration 890 of 1000\n",
      "Iteration 891 of 1000\n",
      "Iteration 892 of 1000\n",
      "Iteration 893 of 1000\n",
      "Iteration 894 of 1000\n",
      "Iteration 895 of 1000\n",
      "Iteration 896 of 1000\n",
      "Iteration 897 of 1000\n",
      "Iteration 898 of 1000\n",
      "Iteration 899 of 1000\n",
      "Iteration 900 of 1000\n",
      "Iteration 901 of 1000\n",
      "Iteration 902 of 1000\n",
      "Iteration 903 of 1000\n",
      "Iteration 904 of 1000\n",
      "Iteration 905 of 1000\n",
      "Iteration 906 of 1000\n",
      "Iteration 907 of 1000\n",
      "Iteration 908 of 1000\n",
      "Iteration 909 of 1000\n",
      "Iteration 910 of 1000\n",
      "Iteration 911 of 1000\n",
      "Iteration 912 of 1000\n",
      "Iteration 913 of 1000\n",
      "Iteration 914 of 1000\n",
      "Iteration 915 of 1000\n",
      "Iteration 916 of 1000\n",
      "Iteration 917 of 1000\n",
      "Iteration 918 of 1000\n",
      "Iteration 919 of 1000\n",
      "Iteration 920 of 1000\n",
      "Iteration 921 of 1000\n",
      "Iteration 922 of 1000\n",
      "Iteration 923 of 1000\n",
      "Iteration 924 of 1000\n",
      "Iteration 925 of 1000\n",
      "Iteration 926 of 1000\n",
      "Iteration 927 of 1000\n",
      "Iteration 928 of 1000\n",
      "Iteration 929 of 1000\n",
      "Iteration 930 of 1000\n",
      "Iteration 931 of 1000\n",
      "Iteration 932 of 1000\n",
      "Iteration 933 of 1000\n",
      "Iteration 934 of 1000\n",
      "Iteration 935 of 1000\n",
      "Iteration 936 of 1000\n",
      "Iteration 937 of 1000\n",
      "Iteration 938 of 1000\n",
      "Iteration 939 of 1000\n",
      "Iteration 940 of 1000\n",
      "Iteration 941 of 1000\n",
      "Iteration 942 of 1000\n",
      "Iteration 943 of 1000\n",
      "Iteration 944 of 1000\n",
      "Iteration 945 of 1000\n",
      "Iteration 946 of 1000\n",
      "Iteration 947 of 1000\n",
      "Iteration 948 of 1000\n",
      "Iteration 949 of 1000\n",
      "Iteration 950 of 1000\n",
      "Iteration 951 of 1000\n",
      "Iteration 952 of 1000\n",
      "Iteration 953 of 1000\n",
      "Iteration 954 of 1000\n",
      "Iteration 955 of 1000\n",
      "Iteration 956 of 1000\n",
      "Iteration 957 of 1000\n",
      "Iteration 958 of 1000\n",
      "Iteration 959 of 1000\n",
      "Iteration 960 of 1000\n",
      "Iteration 961 of 1000\n",
      "Iteration 962 of 1000\n",
      "Iteration 963 of 1000\n",
      "Iteration 964 of 1000\n",
      "Iteration 965 of 1000\n",
      "Iteration 966 of 1000\n",
      "Iteration 967 of 1000\n",
      "Iteration 968 of 1000\n",
      "Iteration 969 of 1000\n",
      "Iteration 970 of 1000\n",
      "Iteration 971 of 1000\n",
      "Iteration 972 of 1000\n",
      "Iteration 973 of 1000\n",
      "Iteration 974 of 1000\n",
      "Iteration 975 of 1000\n",
      "Iteration 976 of 1000\n",
      "Iteration 977 of 1000\n",
      "Iteration 978 of 1000\n",
      "Iteration 979 of 1000\n",
      "Iteration 980 of 1000\n",
      "Iteration 981 of 1000\n",
      "Iteration 982 of 1000\n",
      "Iteration 983 of 1000\n",
      "Iteration 984 of 1000\n",
      "Iteration 985 of 1000\n",
      "Iteration 986 of 1000\n",
      "Iteration 987 of 1000\n",
      "Iteration 988 of 1000\n",
      "Iteration 989 of 1000\n",
      "Iteration 990 of 1000\n",
      "Iteration 991 of 1000\n",
      "Iteration 992 of 1000\n",
      "Iteration 993 of 1000\n",
      "Iteration 994 of 1000\n",
      "Iteration 995 of 1000\n",
      "Iteration 996 of 1000\n",
      "Iteration 997 of 1000\n",
      "Iteration 998 of 1000\n",
      "Iteration 999 of 1000\n",
      "Iteration 1000 of 1000\n"
     ]
    }
   ],
   "source": [
    "# Train the network using Stochastic Gradient Descent\n",
    "minibatchSize = 2000\n",
    "nIterations = 1000\n",
    "learningRate = 1.0\n",
    "trainUsingSGD(weights, x_train, y_train, minibatchSize, nIterations, learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38233 out of 60000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate trained weights on training data\n",
    "evaluate(weights, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6494 out of 10000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate trained weights on test data\n",
    "evaluate(weights, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yPreds = forwardProp(x_train, weights)\n",
    "\n",
    "np.argmax(yPreds[-1][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
