{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-requisites\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To clear print buffer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing functions from the previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "1\n",
      "(2, 3)\n",
      "[[-0.29285599 -1.17220739 -0.85551856]\n",
      " [-0.84896999 -1.66102533  0.15182479]]\n",
      "2\n",
      "(1, 3)\n",
      "[[ 1.54946039 -0.34411433  0.87618229]]\n",
      "Cost: 0.267971323763\n",
      "Accuracy: \n",
      "[[ 0.84090348]\n",
      " [ 0.85294461]\n",
      " [ 0.82501862]\n",
      " [ 0.83123477]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing weight matrices from layer sizes\n",
    "def initializeWeights(layers):\n",
    "    weights = [np.random.randn(o, i+1) for i, o in zip(layers[:-1], layers[1:])]\n",
    "    return weights\n",
    "\n",
    "# Add a bias term to every data point in the input\n",
    "def addBiasTerms(X):\n",
    "        # Make the input an np.array()\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Forcing 1D vectors to be 2D matrices of 1xlength dimensions\n",
    "        if X.ndim==1:\n",
    "            X = np.reshape(X, (1, len(X)))\n",
    "        \n",
    "        # Inserting bias terms\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))\n",
    "\n",
    "# Forward Propagation of outputs\n",
    "def forwardProp(X, weights):\n",
    "    # Initializing an empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Assigning a name to reuse as inputs\n",
    "    inputs = X\n",
    "    \n",
    "    # For each layer\n",
    "    for w in weights:\n",
    "        # Add bias term to input\n",
    "        inputs = addBiasTerms(inputs)\n",
    "        \n",
    "        # Y = Sigmoid ( X .* W^T )\n",
    "        outputs.append(sigmoid(np.dot(inputs, w.T)))\n",
    "        \n",
    "        # Input of next layer is output of this layer\n",
    "        inputs = outputs[-1]\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "# Compute COST (J) of Neural Network\n",
    "def nnCost(weights, X, Y):\n",
    "    # Calculate yPred\n",
    "    yPred = forwardProp(X, weights)[-1]\n",
    "    \n",
    "    # Compute J\n",
    "    J = 0.5*np.sum((yPred-Y)**2)/len(Y)\n",
    "    \n",
    "    return J\n",
    "\n",
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE\n",
    "# Added eta, the learning rate, as an input\n",
    "def backProp(weights, X, Y, learningRate):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # Change weights of the current layer (W <- W + eta*deltaW)\n",
    "        w += -learningRate*grad\n",
    "\n",
    "# Evaluate the accuracy of weights for input X and desired outptut Y\n",
    "def evaluate(weights, X, Y):\n",
    "    yPreds = forwardProp(X, weights)[-1]\n",
    "    # Check if maximum probability is from that neuron corresponding to desired class,\n",
    "    # AND check if that maximum probability is greater than 0.5\n",
    "    yes = sum( int( ( np.argmax(yPreds[i]) == np.argmax(Y[i]) ) and \n",
    "                    ( (yPreds[i][np.argmax(yPreds[i])]>0.5) == (Y[i][np.argmax(Y[i])]>0.5) ) )\n",
    "              for i in range(len(Y)) )\n",
    "    return yes\n",
    "\n",
    "# TRAINING USING MINI-BATCH GRADIENT DESCENT\n",
    "# Default learning rate = 1.0\n",
    "def trainUsingMinibatchGD(weights, X, Y, minibatchSize, nEpochs, learningRate=1.0, testX=None, testY=None):\n",
    "    # If testing data is not provided, check accuracy on training data\n",
    "    if testX is None:\n",
    "        testX = X\n",
    "        testY = Y\n",
    "    \n",
    "    # For nEpochs number of epochs:\n",
    "    for i in range(nEpochs):\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Count number of mini-batches\n",
    "        nOfMinibatches = int(len(X)/minibatchSize)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for m in range(nOfMinibatches):\n",
    "            \n",
    "            # Compute the starting index of this mini-batch\n",
    "            startIdx = m*minibatchSize\n",
    "            \n",
    "            # Declare sampled inputs and outputs\n",
    "            xSample = X[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            ySample = Y[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            \n",
    "            # Run backprop\n",
    "            backProp(weights, xSample, ySample, learningRate)\n",
    "        \n",
    "        # Check cost and accuracy\n",
    "        cost = nnCost(weights, testX, testY)\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"Epoch \"+str(i+1)+\" of \"+str(nEpochs)+\" : \"+\n",
    "              str(yes)+\" out of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "              \"; cost=\"+str(round(cost, 4)))\n",
    "\n",
    "# Initialize network\n",
    "layers = [2, 2, 1]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])\n",
    "\n",
    "# Declare input and desired output for AND gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Check current accuracy and cost\n",
    "print(\"Cost: \"+str(nnCost(weights, X, Y)))\n",
    "print(\"Accuracy: \")\n",
    "evaluate(weights, X, Y)\n",
    "print(forwardProp(X, weights)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data, and format it for our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST DATA\n",
    "# Use numpy.load() to load the .npz file\n",
    "f = np.load('mnist.npz')\n",
    "# Saving the files\n",
    "x_train = f['x_train']\n",
    "y_train = f['y_train']\n",
    "x_test = f['x_test']\n",
    "y_test = f['y_test']\n",
    "f.close()\n",
    "\n",
    "# Reshaping x_train and x_test for our network with 784 inputs neurons\n",
    "x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "# Normalize x_train\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Make new y_train of nx10 elements\n",
    "new_y_train = np.zeros((len(y_train), 10))\n",
    "for i in range(len(y_train)):\n",
    "    new_y_train[i, y_train[i]] = 1\n",
    "\n",
    "del y_train\n",
    "y_train = new_y_train\n",
    "\n",
    "# Make new y_test of nx10 elements\n",
    "new_y_test = np.zeros((len(y_test), 10))\n",
    "for i in range(len(y_test)):\n",
    "    new_y_test[i, y_test[i]] = 1\n",
    "\n",
    "del y_test\n",
    "y_test = new_y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to train our network to give good accuracy on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let us try a deeper network, with 2 hidden layers of 10 neurons each. And let us test it on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 1021 out of 10000 = 0.1021; cost=2.3735\n",
      "Epoch 2 of 30 : 979 out of 10000 = 0.0979; cost=2.2225\n",
      "Epoch 3 of 30 : 971 out of 10000 = 0.0971; cost=2.1205\n",
      "Epoch 4 of 30 : 970 out of 10000 = 0.097; cost=2.0373\n",
      "Epoch 5 of 30 : 968 out of 10000 = 0.0968; cost=1.9495\n",
      "Epoch 6 of 30 : 964 out of 10000 = 0.0964; cost=1.8392\n",
      "Epoch 7 of 30 : 976 out of 10000 = 0.0976; cost=1.6981\n",
      "Epoch 8 of 30 : 968 out of 10000 = 0.0968; cost=1.5417\n",
      "Epoch 9 of 30 : 969 out of 10000 = 0.0969; cost=1.4061\n",
      "Epoch 10 of 30 : 979 out of 10000 = 0.0979; cost=1.309\n",
      "Epoch 11 of 30 : 974 out of 10000 = 0.0974; cost=1.2383\n",
      "Epoch 12 of 30 : 975 out of 10000 = 0.0975; cost=1.1794\n",
      "Epoch 13 of 30 : 988 out of 10000 = 0.0988; cost=1.1257\n",
      "Epoch 14 of 30 : 990 out of 10000 = 0.099; cost=1.0758\n",
      "Epoch 15 of 30 : 1001 out of 10000 = 0.1001; cost=1.03\n",
      "Epoch 16 of 30 : 997 out of 10000 = 0.0997; cost=0.9881\n",
      "Epoch 17 of 30 : 992 out of 10000 = 0.0992; cost=0.9481\n",
      "Epoch 18 of 30 : 977 out of 10000 = 0.0977; cost=0.9062\n",
      "Epoch 19 of 30 : 951 out of 10000 = 0.0951; cost=0.8582\n",
      "Epoch 20 of 30 : 877 out of 10000 = 0.0877; cost=0.8007\n",
      "Epoch 21 of 30 : 772 out of 10000 = 0.0772; cost=0.7356\n",
      "Epoch 22 of 30 : 594 out of 10000 = 0.0594; cost=0.6739\n",
      "Epoch 23 of 30 : 443 out of 10000 = 0.0443; cost=0.6263\n",
      "Epoch 24 of 30 : 316 out of 10000 = 0.0316; cost=0.5938\n",
      "Epoch 25 of 30 : 243 out of 10000 = 0.0243; cost=0.572\n",
      "Epoch 26 of 30 : 196 out of 10000 = 0.0196; cost=0.5568\n",
      "Epoch 27 of 30 : 163 out of 10000 = 0.0163; cost=0.5458\n",
      "Epoch 28 of 30 : 140 out of 10000 = 0.014; cost=0.5374\n",
      "Epoch 29 of 30 : 125 out of 10000 = 0.0125; cost=0.5308\n",
      "Epoch 30 of 30 : 113 out of 10000 = 0.0113; cost=0.5255\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "initialWeights = [np.array(w) for w in weights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = len(y_train)\n",
    "nEpochs = 30\n",
    "learningRate = 1.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 8779 out of 10000 = 0.8779; cost=0.0698\n",
      "Epoch 2 of 30 : 8828 out of 10000 = 0.8828; cost=0.0665\n",
      "Epoch 3 of 30 : 9095 out of 10000 = 0.9095; cost=0.0545\n",
      "Epoch 4 of 30 : 9098 out of 10000 = 0.9098; cost=0.0535\n",
      "Epoch 5 of 30 : 9160 out of 10000 = 0.916; cost=0.0503\n",
      "Epoch 6 of 30 : 9171 out of 10000 = 0.9171; cost=0.0496\n",
      "Epoch 7 of 30 : 9245 out of 10000 = 0.9245; cost=0.0472\n",
      "Epoch 8 of 30 : 9259 out of 10000 = 0.9259; cost=0.0484\n",
      "Epoch 9 of 30 : 9224 out of 10000 = 0.9224; cost=0.0472\n",
      "Epoch 10 of 30 : 9243 out of 10000 = 0.9243; cost=0.046\n",
      "Epoch 11 of 30 : 9288 out of 10000 = 0.9288; cost=0.0443\n",
      "Epoch 12 of 30 : 9259 out of 10000 = 0.9259; cost=0.0458\n",
      "Epoch 13 of 30 : 9281 out of 10000 = 0.9281; cost=0.045\n",
      "Epoch 14 of 30 : 9248 out of 10000 = 0.9248; cost=0.0474\n",
      "Epoch 15 of 30 : 9239 out of 10000 = 0.9239; cost=0.0457\n",
      "Epoch 16 of 30 : 9305 out of 10000 = 0.9305; cost=0.0449\n",
      "Epoch 17 of 30 : 9290 out of 10000 = 0.929; cost=0.0449\n",
      "Epoch 18 of 30 : 9296 out of 10000 = 0.9296; cost=0.0431\n",
      "Epoch 19 of 30 : 9286 out of 10000 = 0.9286; cost=0.0436\n",
      "Epoch 20 of 30 : 9261 out of 10000 = 0.9261; cost=0.0453\n",
      "Epoch 21 of 30 : 9267 out of 10000 = 0.9267; cost=0.0446\n",
      "Epoch 22 of 30 : 9307 out of 10000 = 0.9307; cost=0.046\n",
      "Epoch 23 of 30 : 9286 out of 10000 = 0.9286; cost=0.0441\n",
      "Epoch 24 of 30 : 9335 out of 10000 = 0.9335; cost=0.0446\n",
      "Epoch 25 of 30 : 9294 out of 10000 = 0.9294; cost=0.0449\n",
      "Epoch 26 of 30 : 9357 out of 10000 = 0.9357; cost=0.0422\n",
      "Epoch 27 of 30 : 9311 out of 10000 = 0.9311; cost=0.0434\n",
      "Epoch 28 of 30 : 9269 out of 10000 = 0.9269; cost=0.0448\n",
      "Epoch 29 of 30 : 9356 out of 10000 = 0.9356; cost=0.0438\n",
      "Epoch 30 of 30 : 9292 out of 10000 = 0.9292; cost=0.0452\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using mini-batch gradient descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "initialWeights = [np.array(w) for w in weights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that sometimes the accuracy decreases and cost increases. This means our learning rate is too big to move only so much as to decrease the cost. We need to find ways of adapting the learning rate to train our network with the optimal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decaying learning rate\n",
    "\n",
    "By decreasing the learning rate, we will be able to take smaller steps towards the optimum. There are different ways to decrease the learning rate.\n",
    "\n",
    "## Step decay\n",
    "\n",
    "Whenever the cost of the neural network increases (i.e. accuracy decreases), we shall revert the weights back to the values at the start of the iteration, and reduce the learning rate by half. We call this **Step decay**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training function, let us add the option of choosing the decay, and define the Step decay optimizer. If the cost does not decrease contiguously for too long (5 epochs), let us stop our training there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING USING MINI-BATCH GRADIENT DESCENT\n",
    "def trainUsingMinibatchGD(weights, X, Y, minibatchSize, nEpochs, learningRate=1.0, \n",
    "                          decay=None, testX=None, testY=None):\n",
    "    # If testing data is not provided, check accuracy on training data\n",
    "    if testX is None:\n",
    "        testX = X\n",
    "        testY = Y\n",
    "    \n",
    "    # Check cost and accuracy\n",
    "    # Initialize cost\n",
    "    prevCost = nnCost(weights, testX, testY)\n",
    "    yes = evaluate(weights, testX, testY)\n",
    "    print(\"Before training: \"+str(yes)+\" out of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "          \"; cost=\"+str(prevCost))\n",
    "    \n",
    "    # Backup weights to revert back in case cost increases\n",
    "    oldWeights = [np.array(w) for w in weights]\n",
    "    \n",
    "    # To count the number of times learning rate had to be halved contiguously\n",
    "    countLRHalf = 0\n",
    "    \n",
    "    # Initialize index for iteration through epochs\n",
    "    epoch = 0\n",
    "    \n",
    "    # For nEpochs number of epochs:\n",
    "    while epoch < nEpochs:\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Count number of mini-batches\n",
    "        nOfMinibatches = int(len(X)/minibatchSize)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for m in range(nOfMinibatches):\n",
    "            \n",
    "            # Compute the starting index of this mini-batch\n",
    "            startIdx = m*minibatchSize\n",
    "            \n",
    "            # Declare sampled inputs and outputs\n",
    "            xSample = X[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            ySample = Y[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "\n",
    "            # Run backprop, with an optimizer\n",
    "            backProp(weights, xSample, ySample, learningRate)\n",
    "        \n",
    "        # Check cost and accuracy\n",
    "        cost = nnCost(weights, testX, testY)\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"Epoch \"+str(epoch+1)+\" of \"+str(nEpochs)+\" : \"+\n",
    "              str(yes)+\" out of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "              \"; cost=\"+str(cost))\n",
    "        \n",
    "        # If decay type is 'step', when cost increases, revert back weights and halve learning rate \n",
    "        if decay is 'step':\n",
    "            # If cost does not decrease\n",
    "            if cost >= prevCost:\n",
    "                # Revert weights back to those at the start of this epoch\n",
    "                weights = [np.array(w) for w in oldWeights]\n",
    "                \n",
    "                # Recalculate prevCost\n",
    "                cost = nnCost(weights, testX, testY)\n",
    "                \n",
    "                # Halve the learning rate\n",
    "                learningRate = learningRate/2.0\n",
    "                \n",
    "                # Revert iteration number\n",
    "                epoch -= 1\n",
    "                \n",
    "                # Increment the count of halving learning rate by 1\n",
    "                countLRHalf += 1\n",
    "                \n",
    "                print(\"Halving learning rate to: \"+str(learningRate)+\", count=\"+str(countLRHalf))\n",
    "            # If cost decreases, reset the count to 0\n",
    "            else:\n",
    "                countLRHalf = 0\n",
    "        \n",
    "        # If learningRate has been halved contiguously for too long, break\n",
    "        if countLRHalf is 5:\n",
    "            break\n",
    "        \n",
    "        # Set prevCost for next epoch\n",
    "        prevCost = cost\n",
    "        \n",
    "        # Set oldWeights for next epoch\n",
    "        oldWeights = [np.array(w) for w in weights]\n",
    "        \n",
    "        # Increase iteration number for epochs\n",
    "        epoch += 1\n",
    "    \n",
    "    # If training was stopped because accuracy was not increasing\n",
    "    if epoch < nEpochs:\n",
    "        print(\"Training ended prematurely...\")\n",
    "    # If training ended in correct number of epochs\n",
    "    else:\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network with a decaying learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.3706\n",
      "Epoch 1 of 30 : 8903 out of 10000 = 0.8903; cost=0.0675467370522\n",
      "Epoch 2 of 30 : 9009 out of 10000 = 0.9009; cost=0.0604533829713\n",
      "Epoch 3 of 30 : 9105 out of 10000 = 0.9105; cost=0.0558005001716\n",
      "Epoch 4 of 30 : 9185 out of 10000 = 0.9185; cost=0.050210922763\n",
      "Epoch 5 of 30 : 9203 out of 10000 = 0.9203; cost=0.0501670595612\n",
      "Epoch 6 of 30 : 9201 out of 10000 = 0.9201; cost=0.0469989425479\n",
      "Epoch 7 of 30 : 9269 out of 10000 = 0.9269; cost=0.0442005669555\n",
      "Epoch 8 of 30 : 9215 out of 10000 = 0.9215; cost=0.0476086394864\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 8 of 30 : 9288 out of 10000 = 0.9288; cost=0.0440865277318\n",
      "Epoch 9 of 30 : 9274 out of 10000 = 0.9274; cost=0.0433871957187\n",
      "Epoch 10 of 30 : 9301 out of 10000 = 0.9301; cost=0.0436701848444\n",
      "Halving learning rate to: 0.75, count=1\n",
      "Epoch 10 of 30 : 9283 out of 10000 = 0.9283; cost=0.0425153081087\n",
      "Epoch 11 of 30 : 9308 out of 10000 = 0.9308; cost=0.0423972816577\n",
      "Epoch 12 of 30 : 9303 out of 10000 = 0.9303; cost=0.042181669639\n",
      "Epoch 13 of 30 : 9297 out of 10000 = 0.9297; cost=0.0425309884927\n",
      "Halving learning rate to: 0.375, count=1\n",
      "Epoch 13 of 30 : 9308 out of 10000 = 0.9308; cost=0.0416875143518\n",
      "Epoch 14 of 30 : 9301 out of 10000 = 0.9301; cost=0.0421045140669\n",
      "Halving learning rate to: 0.1875, count=1\n",
      "Epoch 14 of 30 : 9309 out of 10000 = 0.9309; cost=0.0418304186028\n",
      "Halving learning rate to: 0.09375, count=2\n",
      "Epoch 14 of 30 : 9304 out of 10000 = 0.9304; cost=0.0417870434298\n",
      "Halving learning rate to: 0.046875, count=3\n",
      "Epoch 14 of 30 : 9298 out of 10000 = 0.9298; cost=0.0417780758227\n",
      "Halving learning rate to: 0.0234375, count=4\n",
      "Epoch 14 of 30 : 9305 out of 10000 = 0.9305; cost=0.0417283145432\n",
      "Halving learning rate to: 0.01171875, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN with decaying learning rate\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that by halving learning rate whenever cost increases, the final accuracy improves for the same number of epochs and same weight initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Momentum update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to improve accuracy faster is to push the change in weight more in the direction of the previous update. From [CS231n](http://cs231n.github.io/neural-networks-3/),\n",
    "```\n",
    "v = mu * v - learning_rate * dx # integrate velocity\n",
    "x += v # integrate position\n",
    "```\n",
    "\n",
    "So this is an improvement on the back-propagation algorithm itself.\n",
    "\n",
    "Let us add an option of choosing an optimization on back-propagation, and code Momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE, MOMENTUM\n",
    "# Added eta, the learning rate, as an input\n",
    "def backProp(weights, X, Y, learningRate, optimizer=None, mu=0.9):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Initialize velocity in the shape of weights for use with momentum\n",
    "    v = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # Change weights of the current layer (W <- W + eta*deltaW)\n",
    "        if optimizer is None:\n",
    "            w += -learningRate * grad\n",
    "        \n",
    "        # Momentum\n",
    "        if optimizer is 'momentum':\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad\n",
    "            w += v[-l-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the option of optimizer while calling backprop in trainUsingMiniGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING USING MINI-BATCH GRADIENT DESCENT\n",
    "def trainUsingMinibatchGD(weights, X, Y, minibatchSize, nEpochs, learningRate=1.0, \n",
    "                          decay=None, optimizer=None, mu=0.9, testX=None, testY=None):\n",
    "    # If testing data is not provided, check accuracy on training data\n",
    "    if testX is None:\n",
    "        testX = X\n",
    "        testY = Y\n",
    "    \n",
    "    # Check cost and accuracy\n",
    "    # Initialize cost\n",
    "    prevCost = nnCost(weights, testX, testY)\n",
    "    yes = evaluate(weights, testX, testY)\n",
    "    print(\"Before training: \"+str(yes)+\" out of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "          \"; cost=\"+str(prevCost))\n",
    "    \n",
    "    # Backup weights to revert back in case cost increases\n",
    "    oldWeights = [np.array(w) for w in weights]\n",
    "    \n",
    "    # To count the number of times learning rate had to be halved contiguously\n",
    "    countLRHalf = 0\n",
    "    \n",
    "    # Initialize index for iteration through epochs\n",
    "    epoch = 0\n",
    "    \n",
    "    # For nEpochs number of epochs:\n",
    "    while epoch < nEpochs:\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Count number of mini-batches\n",
    "        nOfMinibatches = int(len(X)/minibatchSize)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for m in range(nOfMinibatches):\n",
    "            \n",
    "            # Compute the starting index of this mini-batch\n",
    "            startIdx = m*minibatchSize\n",
    "            \n",
    "            # Declare sampled inputs and outputs\n",
    "            xSample = X[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            ySample = Y[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "\n",
    "            # Run backprop, with an optimizer\n",
    "            backProp(weights, xSample, ySample, learningRate, optimizer, mu)\n",
    "        \n",
    "        # Check cost and accuracy\n",
    "        cost = nnCost(weights, testX, testY)\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"Epoch \"+str(epoch+1)+\" of \"+str(nEpochs)+\" : \"+\n",
    "              str(yes)+\" out of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "              \"; cost=\"+str(cost))\n",
    "        \n",
    "        # If decay type is 'step', when cost increases, revert back weights and halve learning rate \n",
    "        if decay is 'step':\n",
    "            # If cost does not decrease\n",
    "            if cost >= prevCost:\n",
    "                # Revert weights back to those at the start of this epoch\n",
    "                weights = [np.array(w) for w in oldWeights]\n",
    "                \n",
    "                # Recalculate prevCost\n",
    "                cost = nnCost(weights, testX, testY)\n",
    "                \n",
    "                # Halve the learning rate\n",
    "                learningRate = learningRate/2.0\n",
    "                \n",
    "                # Revert iteration number\n",
    "                epoch -= 1\n",
    "                \n",
    "                # Increment the count of halving learning rate by 1\n",
    "                countLRHalf += 1\n",
    "                \n",
    "                print(\"Halving learning rate to: \"+str(learningRate)+\", count=\"+str(countLRHalf))\n",
    "            # If cost decreases, reset the count to 0\n",
    "            else:\n",
    "                countLRHalf = 0\n",
    "        \n",
    "        # If learningRate has been halved contiguously for too long, break\n",
    "        if countLRHalf is 5:\n",
    "            break\n",
    "        \n",
    "        # Set prevCost for next epoch\n",
    "        prevCost = cost\n",
    "        \n",
    "        # Set oldWeights for next epoch\n",
    "        oldWeights = [np.array(w) for w in weights]\n",
    "        \n",
    "        # Increase iteration number for epochs\n",
    "        epoch += 1\n",
    "    \n",
    "    # If training was stopped because accuracy was not increasing\n",
    "    if epoch < nEpochs:\n",
    "        print(\"Training ended prematurely...\")\n",
    "    # If training ended in correct number of epochs\n",
    "    else:\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network with the same initial weights, using step decay in learning rate, and Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n",
      "Epoch 1 of 30 : 8777 out of 10000 = 0.8777; cost=0.0760498120193\n",
      "Epoch 2 of 30 : 8928 out of 10000 = 0.8928; cost=0.0603590744736\n",
      "Epoch 3 of 30 : 9118 out of 10000 = 0.9118; cost=0.0527367063794\n",
      "Epoch 4 of 30 : 9098 out of 10000 = 0.9098; cost=0.0519137464361\n",
      "Epoch 5 of 30 : 9205 out of 10000 = 0.9205; cost=0.0500469055895\n",
      "Epoch 6 of 30 : 9154 out of 10000 = 0.9154; cost=0.049017530374\n",
      "Epoch 7 of 30 : 9197 out of 10000 = 0.9197; cost=0.0494850178497\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 7 of 30 : 9301 out of 10000 = 0.9301; cost=0.0442050439134\n",
      "Epoch 8 of 30 : 9298 out of 10000 = 0.9298; cost=0.0441667726679\n",
      "Epoch 9 of 30 : 9299 out of 10000 = 0.9299; cost=0.0437103775144\n",
      "Epoch 10 of 30 : 9285 out of 10000 = 0.9285; cost=0.0438829175281\n",
      "Halving learning rate to: 0.75, count=1\n",
      "Epoch 10 of 30 : 9331 out of 10000 = 0.9331; cost=0.0422593709095\n",
      "Epoch 11 of 30 : 9306 out of 10000 = 0.9306; cost=0.0422729755298\n",
      "Halving learning rate to: 0.375, count=1\n",
      "Epoch 11 of 30 : 9326 out of 10000 = 0.9326; cost=0.0419471385273\n",
      "Epoch 12 of 30 : 9333 out of 10000 = 0.9333; cost=0.0417391687278\n",
      "Epoch 13 of 30 : 9318 out of 10000 = 0.9318; cost=0.04171533867\n",
      "Epoch 14 of 30 : 9319 out of 10000 = 0.9319; cost=0.0419683619918\n",
      "Halving learning rate to: 0.1875, count=1\n",
      "Epoch 14 of 30 : 9330 out of 10000 = 0.933; cost=0.0417593630547\n",
      "Halving learning rate to: 0.09375, count=2\n",
      "Epoch 14 of 30 : 9333 out of 10000 = 0.9333; cost=0.0416114184656\n",
      "Epoch 15 of 30 : 9332 out of 10000 = 0.9332; cost=0.0415662772143\n",
      "Epoch 16 of 30 : 9334 out of 10000 = 0.9334; cost=0.0416654571536\n",
      "Halving learning rate to: 0.046875, count=1\n",
      "Epoch 16 of 30 : 9329 out of 10000 = 0.9329; cost=0.0415525851561\n",
      "Epoch 17 of 30 : 9330 out of 10000 = 0.933; cost=0.0415915185918\n",
      "Halving learning rate to: 0.0234375, count=1\n",
      "Epoch 17 of 30 : 9328 out of 10000 = 0.9328; cost=0.0415774069755\n",
      "Halving learning rate to: 0.01171875, count=2\n",
      "Epoch 17 of 30 : 9330 out of 10000 = 0.933; cost=0.0415714380325\n",
      "Halving learning rate to: 0.005859375, count=3\n",
      "Epoch 17 of 30 : 9328 out of 10000 = 0.9328; cost=0.0415618977884\n",
      "Halving learning rate to: 0.0029296875, count=4\n",
      "Epoch 17 of 30 : 9328 out of 10000 = 0.9328; cost=0.0415583792954\n",
      "Halving learning rate to: 0.00146484375, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Momentum\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='momentum', mu=mu, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better accuracy than vanilla gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [CS231n](http://cs231n.github.io/neural-networks-3/),\n",
    "```\n",
    "v_prev = v # back this up\n",
    "v = mu * v - learning_rate * dx # velocity update stays the same\n",
    "x += -mu * v_prev + (1 + mu) * v # position update changes form\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE, MOMENTUM, NAG\n",
    "def backProp(weights, X, Y, learningRate, optimizer=None, mu=0.9):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Initialize velocity in the shape of weights for use with momentum and NAG\n",
    "    v = [np.zeros(w.shape) for w in weights]\n",
    "    prevV = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # CHANGE WEIGHTS of the current layer (W <- W + eta*deltaW)\n",
    "        if optimizer is None:\n",
    "            w += -learningRate * grad\n",
    "        \n",
    "        # Momentum\n",
    "        if optimizer is 'momentum':\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad\n",
    "            w += v[-l-1]\n",
    "        \n",
    "        # Nesterov Momentum\n",
    "        if optimizer is 'nag':\n",
    "            prevV[-l-1] = np.array(v[-l-1]) # back this up\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad # velocity update stays the same\n",
    "            w += -mu * prevV[-l-1] + (1 + mu) * v[-l-1] # position update changes form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network with the same initial weights, using step decay in learning rate, and Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n",
      "Epoch 1 of 30 : 8849 out of 10000 = 0.8849; cost=0.0717524192533\n",
      "Epoch 2 of 30 : 9091 out of 10000 = 0.9091; cost=0.055670681835\n",
      "Epoch 3 of 30 : 9153 out of 10000 = 0.9153; cost=0.0525418892571\n",
      "Epoch 4 of 30 : 9117 out of 10000 = 0.9117; cost=0.0535916565845\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 4 of 30 : 9233 out of 10000 = 0.9233; cost=0.0481753882615\n",
      "Epoch 5 of 30 : 9237 out of 10000 = 0.9237; cost=0.0457395937391\n",
      "Epoch 6 of 30 : 9285 out of 10000 = 0.9285; cost=0.0452398507075\n",
      "Epoch 7 of 30 : 9251 out of 10000 = 0.9251; cost=0.0457638468201\n",
      "Halving learning rate to: 0.75, count=1\n",
      "Epoch 7 of 30 : 9295 out of 10000 = 0.9295; cost=0.0440445403005\n",
      "Epoch 8 of 30 : 9282 out of 10000 = 0.9282; cost=0.0428945881307\n",
      "Epoch 9 of 30 : 9302 out of 10000 = 0.9302; cost=0.0433700328082\n",
      "Halving learning rate to: 0.375, count=1\n",
      "Epoch 9 of 30 : 9307 out of 10000 = 0.9307; cost=0.0428650912142\n",
      "Epoch 10 of 30 : 9304 out of 10000 = 0.9304; cost=0.042643556965\n",
      "Epoch 11 of 30 : 9305 out of 10000 = 0.9305; cost=0.0428952189068\n",
      "Halving learning rate to: 0.1875, count=1\n",
      "Epoch 11 of 30 : 9312 out of 10000 = 0.9312; cost=0.0425517801805\n",
      "Epoch 12 of 30 : 9306 out of 10000 = 0.9306; cost=0.0423009801797\n",
      "Epoch 13 of 30 : 9314 out of 10000 = 0.9314; cost=0.0424361769251\n",
      "Halving learning rate to: 0.09375, count=1\n",
      "Epoch 13 of 30 : 9313 out of 10000 = 0.9313; cost=0.0422647509358\n",
      "Epoch 14 of 30 : 9307 out of 10000 = 0.9307; cost=0.0423255143017\n",
      "Halving learning rate to: 0.046875, count=1\n",
      "Epoch 14 of 30 : 9305 out of 10000 = 0.9305; cost=0.0422772878245\n",
      "Halving learning rate to: 0.0234375, count=2\n",
      "Epoch 14 of 30 : 9307 out of 10000 = 0.9307; cost=0.0422874367474\n",
      "Halving learning rate to: 0.01171875, count=3\n",
      "Epoch 14 of 30 : 9315 out of 10000 = 0.9315; cost=0.0422782299156\n",
      "Halving learning rate to: 0.005859375, count=4\n",
      "Epoch 14 of 30 : 9313 out of 10000 = 0.9313; cost=0.0422704386248\n",
      "Halving learning rate to: 0.0029296875, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Nesterov Momentum (NAG)\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='nag', mu=mu, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [CS231n](http://cs231n.github.io/neural-networks-3/),\n",
    "\n",
    "```\n",
    "# Assume the gradient dx and parameter vector x\n",
    "cache += dx**2\n",
    "x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE, MOMENTUM, NAG, ADAGRAD\n",
    "def backProp(weights, X, Y, learningRate, optimizer=None, mu=0.9):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Initialize velocity in the shape of weights for use with momentum and NAG\n",
    "    v = [np.zeros(w.shape) for w in weights]\n",
    "    prevV = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Initialize cache for use with Adagrad\n",
    "    cache = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # CHANGE WEIGHTS of the current layer (W <- W + eta*deltaW)\n",
    "        if optimizer is None:\n",
    "            w += -learningRate * grad\n",
    "        \n",
    "        # Momentum\n",
    "        if optimizer is 'momentum':\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad\n",
    "            w += v[-l-1]\n",
    "        \n",
    "        # Nesterov Momentum\n",
    "        if optimizer is 'nag':\n",
    "            prevV[-l-1] = np.array(v[-l-1]) # back this up\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad # velocity update stays the same\n",
    "            w += -mu * prevV[-l-1] + (1 + mu) * v[-l-1] # position update changes form\n",
    "        \n",
    "        # Adagrad\n",
    "        if optimizer is 'adagrad':\n",
    "            cache[-l-1] += grad**2\n",
    "            w += - learningRate * grad / (np.sqrt(cache[-l-1]) + np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 0 out of 10000 = 0.0; cost=0.5\n",
      "Epoch 2 of 30 : 0 out of 10000 = 0.0; cost=0.5\n",
      "Halving learning rate to: 1.5, count=1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-21063ad43c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n\u001b[0;32m---> 13\u001b[0;31m                       decay='step', optimizer='adagrad', testX=x_test, testY=y_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-223-45d6a71401ac>\u001b[0m in \u001b[0;36mtrainUsingMinibatchGD\u001b[0;34m(weights, X, Y, minibatchSize, nEpochs, learningRate, decay, optimizer, mu, testX, testY)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Run backprop, with an optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mbackProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mySample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Check cost and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-227-9fe1fc26d15a>\u001b[0m in \u001b[0;36mbackProp\u001b[0;34m(weights, X, Y, learningRate, optimizer, mu)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Take care, l goes from 0 to 1, while the weights are in reverse order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# If 1st layer has been reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mxL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddBiasTerms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mxL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddBiasTerms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-651f95fd6008>\u001b[0m in \u001b[0;36maddBiasTerms\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Inserting bias terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   4901\u001b[0m             \u001b[0;31m# very different from a[:,[0],:] = ...! This changes values so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4902\u001b[0m             \u001b[0;31m# it works likes the second case. (here a[:,0:1,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4903\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4904\u001b[0m         \u001b[0mnumnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m         \u001b[0mnewshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnumnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mrollaxis\u001b[0;34m(a, axis, start)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN using Adagrad\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='adagrad', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the learningRate is too high at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1004 out of 10000 = 0.1004; cost=2.37056621443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 : 5806 out of 10000 = 0.5806; cost=0.260757491593\n",
      "Epoch 2 of 30 : 6782 out of 10000 = 0.6782; cost=0.204815630246\n",
      "Epoch 3 of 30 : 6842 out of 10000 = 0.6842; cost=0.192322763729\n",
      "Epoch 4 of 30 : 6874 out of 10000 = 0.6874; cost=0.187409859878\n",
      "Epoch 5 of 30 : 6444 out of 10000 = 0.6444; cost=0.206949182702\n",
      "Halving learning rate to: 0.5, count=1\n",
      "Epoch 5 of 30 : 6822 out of 10000 = 0.6822; cost=0.185802116025\n",
      "Epoch 6 of 30 : 6915 out of 10000 = 0.6915; cost=0.185329131066\n",
      "Epoch 7 of 30 : 7694 out of 10000 = 0.7694; cost=0.156780001919\n",
      "Epoch 8 of 30 : 7686 out of 10000 = 0.7686; cost=0.147885292916\n",
      "Epoch 9 of 30 : 7745 out of 10000 = 0.7745; cost=0.149509382291\n",
      "Halving learning rate to: 0.25, count=1\n",
      "Epoch 9 of 30 : 7776 out of 10000 = 0.7776; cost=0.142723029862\n",
      "Epoch 10 of 30 : 7794 out of 10000 = 0.7794; cost=0.140961464727\n",
      "Epoch 11 of 30 : 7650 out of 10000 = 0.765; cost=0.148943247373\n",
      "Halving learning rate to: 0.125, count=1\n",
      "Epoch 11 of 30 : 7715 out of 10000 = 0.7715; cost=0.146167760021\n",
      "Halving learning rate to: 0.0625, count=2\n",
      "Epoch 11 of 30 : 7904 out of 10000 = 0.7904; cost=0.140910028135\n",
      "Epoch 12 of 30 : 7899 out of 10000 = 0.7899; cost=0.139502353621\n",
      "Epoch 13 of 30 : 7924 out of 10000 = 0.7924; cost=0.139301605439\n",
      "Epoch 14 of 30 : 7928 out of 10000 = 0.7928; cost=0.139030056525\n",
      "Epoch 15 of 30 : 7843 out of 10000 = 0.7843; cost=0.139055102172\n",
      "Halving learning rate to: 0.03125, count=1\n",
      "Epoch 15 of 30 : 7899 out of 10000 = 0.7899; cost=0.138513794825\n",
      "Epoch 16 of 30 : 7888 out of 10000 = 0.7888; cost=0.140383794651\n",
      "Halving learning rate to: 0.015625, count=1\n",
      "Epoch 16 of 30 : 7915 out of 10000 = 0.7915; cost=0.139852236166\n",
      "Halving learning rate to: 0.0078125, count=2\n",
      "Epoch 16 of 30 : 7928 out of 10000 = 0.7928; cost=0.138930431629\n",
      "Halving learning rate to: 0.00390625, count=3\n",
      "Epoch 16 of 30 : 7920 out of 10000 = 0.792; cost=0.138591936215\n",
      "Halving learning rate to: 0.001953125, count=4\n",
      "Epoch 16 of 30 : 7924 out of 10000 = 0.7924; cost=0.138398817994\n",
      "Epoch 17 of 30 : 7922 out of 10000 = 0.7922; cost=0.138667644032\n",
      "Halving learning rate to: 0.0009765625, count=1\n",
      "Epoch 17 of 30 : 7922 out of 10000 = 0.7922; cost=0.138561992417\n",
      "Halving learning rate to: 0.00048828125, count=2\n",
      "Epoch 17 of 30 : 7922 out of 10000 = 0.7922; cost=0.138487825914\n",
      "Halving learning rate to: 0.000244140625, count=3\n",
      "Epoch 17 of 30 : 7924 out of 10000 = 0.7924; cost=0.138409643251\n",
      "Halving learning rate to: 0.0001220703125, count=4\n",
      "Epoch 17 of 30 : 7924 out of 10000 = 0.7924; cost=0.138398883401\n",
      "Halving learning rate to: 6.103515625e-05, count=5\n",
      "Training ended prematurely...\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Adagrad\n",
    "\n",
    "# Revert to old weights\n",
    "weights = [np.array(w) for w in initialWeights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 1.0\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='adagrad', testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
