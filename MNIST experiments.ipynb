{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-requisites\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To clear print buffer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "1\n",
      "(2, 3)\n",
      "[[ 0.50989498  0.90468367 -1.71849463]\n",
      " [ 0.50226972 -0.60305691  0.92248982]]\n",
      "2\n",
      "(1, 3)\n",
      "[[ 1.74069311  0.36830228 -1.59473413]]\n",
      "Cost: 0.204595059595\n",
      "Accuracy: 1 of 4 = 0.25\n",
      "[[ 0.72657099]\n",
      " [ 0.63178799]\n",
      " [ 0.7824104 ]\n",
      " [ 0.6876973 ]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing weight matrices from layer sizes\n",
    "def initializeWeights(layers):\n",
    "    weights = [np.random.randn(o, i+1) for i, o in zip(layers[:-1], layers[1:])]\n",
    "    return weights\n",
    "\n",
    "# Add a bias term to every data point in the input\n",
    "def addBiasTerms(X):\n",
    "        # Make the input an np.array()\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Forcing 1D vectors to be 2D matrices of 1xlength dimensions\n",
    "        if X.ndim==1:\n",
    "            X = np.reshape(X, (1, len(X)))\n",
    "        \n",
    "        # Inserting bias terms\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))\n",
    "\n",
    "# Forward Propagation of outputs\n",
    "def forwardProp(X, weights):\n",
    "    # Initializing an empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Assigning a name to reuse as inputs\n",
    "    inputs = X\n",
    "    \n",
    "    # For each layer\n",
    "    for w in weights:\n",
    "        # Add bias term to input\n",
    "        inputs = addBiasTerms(inputs)\n",
    "        \n",
    "        # Y = Sigmoid ( X .* W^T )\n",
    "        outputs.append(sigmoid(np.dot(inputs, w.T)))\n",
    "        \n",
    "        # Input of next layer is output of this layer\n",
    "        inputs = outputs[-1]\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "# Compute COST (J) of Neural Network\n",
    "def nnCost(weights, X, Y):\n",
    "    # Calculate yPred\n",
    "    yPred = forwardProp(X, weights)[-1]\n",
    "    \n",
    "    # Compute J\n",
    "    J = 0.5*np.sum((yPred-Y)**2)/len(Y)\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Evaluate the accuracy of weights for input X and desired outptut Y\n",
    "def evaluate(weights, X, Y):\n",
    "    yPreds = forwardProp(X, weights)[-1]\n",
    "    # Check if maximum probability is from that neuron corresponding to desired class,\n",
    "    # AND check if that maximum probability is greater than 0.5\n",
    "    yes = sum( int( ( np.argmax(yPreds[i]) == np.argmax(Y[i]) ) and \n",
    "                    ( (yPreds[i][np.argmax(yPreds[i])]>0.5) == (Y[i][np.argmax(Y[i])]>0.5) ) )\n",
    "              for i in range(len(Y)) )\n",
    "    return yes\n",
    "\n",
    "\n",
    "# TRAINING USING MINI-BATCH GRADIENT DESCENT\n",
    "def trainUsingMinibatchGD(weights, X, Y, minibatchSize, nEpochs, learningRate=1.0, \n",
    "                          decay=None, decayRate=0.1, optimizer=None, mu=0.9, testX=None, testY=None):\n",
    "    # If testing data is not provided, check accuracy on training data\n",
    "    if testX is None:\n",
    "        testX = X\n",
    "        testY = Y\n",
    "    \n",
    "    # Check cost and accuracy\n",
    "    # Initialize cost\n",
    "    prevCost = nnCost(weights, testX, testY)\n",
    "    yes = evaluate(weights, testX, testY)\n",
    "    print(\"Before training: \"+str(yes)+\" of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "          \"; cost=\"+str(prevCost))\n",
    "    \n",
    "    # Backup weights to revert back in case cost increases\n",
    "    oldWeights = [np.array(w) for w in weights]\n",
    "    \n",
    "    # To count the number of times learning rate had to be halved contiguously\n",
    "    countLRHalf = 0\n",
    "    \n",
    "    # Initialize index for iteration through epochs\n",
    "    epoch = 0\n",
    "    \n",
    "    # For nEpochs number of epochs:\n",
    "    while epoch < nEpochs:\n",
    "        # clear output\n",
    "        #clear_output()\n",
    "        \n",
    "        # Make a list of all the indices\n",
    "        fullIdx = list(range(len(Y)))\n",
    "        \n",
    "        # Shuffle the full index\n",
    "        np.random.shuffle(fullIdx)\n",
    "        \n",
    "        # Count number of mini-batches\n",
    "        nOfMinibatches = int(len(X)/minibatchSize)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for m in range(nOfMinibatches):\n",
    "            \n",
    "            # Compute the starting index of this mini-batch\n",
    "            startIdx = m*minibatchSize\n",
    "            \n",
    "            # Declare sampled inputs and outputs\n",
    "            xSample = X[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "            ySample = Y[fullIdx[startIdx:startIdx+minibatchSize]]\n",
    "\n",
    "            # Run backprop, with an optimizer\n",
    "            backProp(weights, xSample, ySample, learningRate, optimizer, mu)\n",
    "        \n",
    "        # Check cost and accuracy\n",
    "        cost = nnCost(weights, testX, testY)\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"Epoch \"+str(epoch+1)+\" of \"+str(nEpochs)+\" : \"+\n",
    "              str(yes)+\" of \"+str(len(testY))+\" = \"+str(round(float(yes/len(testY)),4))+\n",
    "              \"; cost=\"+str(cost))\n",
    "        \n",
    "        # If decay type is 'step', when cost increases, revert back weights and halve learning rate \n",
    "        if decay is 'step':\n",
    "            # If cost does not decrease\n",
    "            if cost >= prevCost:\n",
    "                # Revert weights back to those at the start of this epoch\n",
    "                weights = [np.array(w) for w in oldWeights]\n",
    "                \n",
    "                # Recalculate prevCost\n",
    "                cost = nnCost(weights, testX, testY)\n",
    "                \n",
    "                # Halve the learning rate\n",
    "                learningRate = learningRate/2.0\n",
    "                \n",
    "                # Revert iteration number\n",
    "                epoch -= 1\n",
    "                \n",
    "                # Increment the count of halving learning rate by 1\n",
    "                countLRHalf += 1\n",
    "                \n",
    "                print(\"Halving learning rate to: \"+str(learningRate)+\", count=\"+str(countLRHalf))\n",
    "            # If cost decreases, reset the count to 0\n",
    "            else:\n",
    "                countLRHalf = 0\n",
    "        \n",
    "        # If decay is 'time'\n",
    "        if decay is 'time':\n",
    "            learningRate *= np.exp(-decayRate)\n",
    "        \n",
    "        # If learningRate has been halved contiguously for too long, break\n",
    "        if countLRHalf is 10:\n",
    "            break\n",
    "        \n",
    "        # Set prevCost for next epoch\n",
    "        prevCost = cost\n",
    "        \n",
    "        # Set oldWeights for next epoch\n",
    "        oldWeights = [np.array(w) for w in weights]\n",
    "        \n",
    "        # Increase iteration number for epochs\n",
    "        epoch += 1\n",
    "    \n",
    "    # If training was stopped because accuracy was not increasing\n",
    "    if epoch < nEpochs:\n",
    "        print(\"Training ended prematurely...\")\n",
    "    # If training ended in correct number of epochs\n",
    "    else:\n",
    "        print(\"Training complete.\")\n",
    "    \n",
    "    # Printing training accuracy\n",
    "    yes = evaluate(weights, X, Y)\n",
    "    print(\"TRAINING ACCURACY, COST : \"+str(yes)+\" of \"+str(len(Y))+\n",
    "          \" = \"+str(round(float(yes/len(Y)),4)))\n",
    "    \n",
    "    # Printing test accuracy\n",
    "    if testY is not Y:\n",
    "        yes = evaluate(weights, testX, testY)\n",
    "        print(\"TEST ACCURACY, COST : \"+str(yes)+\" of \"+str(len(testY))+\n",
    "              \" = \"+str(round(float(yes/len(testY)),4)))\n",
    "    \n",
    "\n",
    "# IMPLEMENTING BACK-PROPAGATION WITH LEARNING RATE, MOMENTUM, NAG, ADAGRAD\n",
    "def backProp(weights, X, Y, learningRate, optimizer=None, mu=0.9):\n",
    "    # Forward propagate to find outputs\n",
    "    outputs = forwardProp(X, weights)\n",
    "    \n",
    "    # For the last layer, bpError = error = yPred - Y\n",
    "    bpError = outputs[-1] - Y\n",
    "    \n",
    "    # Initialize velocity in the shape of weights for use with momentum and NAG\n",
    "    v = [np.zeros(w.shape) for w in weights]\n",
    "    prevV = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Initialize cache for use with Adagrad\n",
    "    cache = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Back-propagating from the last layer to the first\n",
    "    for l, w in enumerate(reversed(weights)):\n",
    "        \n",
    "        # Find yPred for this layer\n",
    "        yPred = outputs[-l-1]\n",
    "        \n",
    "        # Calculate delta for this layer using bpError from next layer\n",
    "        delta = np.multiply(np.multiply(bpError, yPred), 1-yPred)\n",
    "        \n",
    "        # Find input to the layer, by adding bias to the output of the previous layer\n",
    "        # Take care, l goes from 0 to 1, while the weights are in reverse order\n",
    "        if l==len(weights)-1: # If 1st layer has been reached\n",
    "            xL = addBiasTerms(X)\n",
    "        else:\n",
    "            xL = addBiasTerms(outputs[-l-2])\n",
    "        \n",
    "        # Calculate the gradient for this layer\n",
    "        grad = np.dot(delta.T, xL)/len(Y)\n",
    "        \n",
    "        # Calculate bpError for previous layer to be back-propagated\n",
    "        bpError = np.dot(delta, w)\n",
    "        \n",
    "        # Ignore bias term in bpError\n",
    "        bpError = bpError[:,1:]\n",
    "        \n",
    "        # CHANGE WEIGHTS of the current layer (W <- W + eta*deltaW)\n",
    "        if optimizer is None:\n",
    "            w += -learningRate * grad\n",
    "        \n",
    "        # Momentum\n",
    "        if optimizer is 'momentum':\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad\n",
    "            w += v[-l-1]\n",
    "        \n",
    "        # Nesterov Momentum\n",
    "        if optimizer is 'nag':\n",
    "            prevV[-l-1] = np.array(v[-l-1]) # back this up\n",
    "            v[-l-1] = mu * v[-l-1] - learningRate * grad # velocity update stays the same\n",
    "            w += -mu * prevV[-l-1] + (1 + mu) * v[-l-1] # position update changes form\n",
    "        \n",
    "        # Adagrad\n",
    "        if optimizer is 'adagrad':\n",
    "            cache[-l-1] += grad**2\n",
    "            w += - learningRate * grad / (np.sqrt(cache[-l-1]) + np.finfo(float).eps)\n",
    "\n",
    "# Initialize network\n",
    "layers = [2, 2, 1]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])\n",
    "\n",
    "# Declare input and desired output for AND gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Check current accuracy and cost\n",
    "print(\"Cost: \"+str(nnCost(weights, X, Y)))\n",
    "yes = evaluate(weights, X, Y)\n",
    "print(\"Accuracy: \"+str(yes)+\" of \"+str(len(Y))+\" = \"+str(round(float(yes/len(Y)), 4)))\n",
    "print(forwardProp(X, weights)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data from npz, and format it for our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST DATA\n",
    "# Use numpy.load() to load the .npz file\n",
    "f = np.load('mnist.npz')\n",
    "# Saving the files\n",
    "x_train = f['x_train']\n",
    "y_train = f['y_train']\n",
    "x_test = f['x_test']\n",
    "y_test = f['y_test']\n",
    "f.close()\n",
    "\n",
    "# Reshaping x_train and x_test for our network with 784 inputs neurons\n",
    "x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "# Normalize x_train\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Make new y_train of nx10 elements\n",
    "new_y_train = np.zeros((len(y_train), 10))\n",
    "for i in range(len(y_train)):\n",
    "    new_y_train[i, y_train[i]] = 1\n",
    "\n",
    "del y_train\n",
    "y_train = new_y_train\n",
    "\n",
    "# Make new y_test of nx10 elements\n",
    "new_y_test = np.zeros((len(y_test), 10))\n",
    "for i in range(len(y_test)):\n",
    "    new_y_test[i, y_test[i]] = 1\n",
    "\n",
    "del y_test\n",
    "y_test = new_y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline accuracy\n",
    "\n",
    "Minimum accuracy that can be obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1470 of 10000 = 0.147; cost=1.75485656197\n",
      "Epoch 1 of 30 : 8836 out of 10000 = 0.8836; cost=0.0703368832207\n",
      "Epoch 2 of 30 : 9046 out of 10000 = 0.9046; cost=0.0582771715058\n",
      "Epoch 3 of 30 : 9144 out of 10000 = 0.9144; cost=0.0536424597654\n",
      "Epoch 4 of 30 : 9211 out of 10000 = 0.9211; cost=0.0493324090936\n",
      "Epoch 5 of 30 : 9211 out of 10000 = 0.9211; cost=0.0494807448319\n",
      "Halving learning rate to: 1.5, count=1\n",
      "Epoch 5 of 30 : 9236 out of 10000 = 0.9236; cost=0.0449246405479\n",
      "Epoch 6 of 30 : 9290 out of 10000 = 0.929; cost=0.0438768907748\n",
      "Epoch 7 of 30 : 9313 out of 10000 = 0.9313; cost=0.0439081304674\n",
      "Halving learning rate to: 0.75, count=1\n",
      "Epoch 7 of 30 : 9324 out of 10000 = 0.9324; cost=0.042031568918\n",
      "Epoch 8 of 30 : 9329 out of 10000 = 0.9329; cost=0.0413069393454\n",
      "Epoch 9 of 30 : 9320 out of 10000 = 0.932; cost=0.0416932310691\n",
      "Halving learning rate to: 0.375, count=1\n",
      "Epoch 9 of 30 : 9340 out of 10000 = 0.934; cost=0.0410020648576\n",
      "Epoch 10 of 30 : 9335 out of 10000 = 0.9335; cost=0.0409575082161\n",
      "Epoch 11 of 30 : 9321 out of 10000 = 0.9321; cost=0.0406082574368\n",
      "Epoch 12 of 30 : 9348 out of 10000 = 0.9348; cost=0.0401353422356\n",
      "Epoch 13 of 30 : 9336 out of 10000 = 0.9336; cost=0.0408684272943\n",
      "Halving learning rate to: 0.1875, count=1\n",
      "Epoch 13 of 30 : 9360 out of 10000 = 0.936; cost=0.0402145568815\n",
      "Halving learning rate to: 0.09375, count=2\n",
      "Epoch 13 of 30 : 9342 out of 10000 = 0.9342; cost=0.0402911586586\n",
      "Halving learning rate to: 0.046875, count=3\n",
      "Epoch 13 of 30 : 9350 out of 10000 = 0.935; cost=0.04022414165\n",
      "Halving learning rate to: 0.0234375, count=4\n",
      "Epoch 13 of 30 : 9351 out of 10000 = 0.9351; cost=0.0401632790394\n",
      "Halving learning rate to: 0.01171875, count=5\n",
      "Epoch 13 of 30 : 9350 out of 10000 = 0.935; cost=0.0401104088141\n",
      "Epoch 14 of 30 : 9350 out of 10000 = 0.935; cost=0.0401540226282\n",
      "Halving learning rate to: 0.005859375, count=1\n",
      "Epoch 14 of 30 : 9350 out of 10000 = 0.935; cost=0.0401352861011\n",
      "Halving learning rate to: 0.0029296875, count=2\n",
      "Epoch 14 of 30 : 9350 out of 10000 = 0.935; cost=0.0401221278491\n",
      "Halving learning rate to: 0.00146484375, count=3\n",
      "Epoch 14 of 30 : 9352 out of 10000 = 0.9352; cost=0.0401163886797\n",
      "Halving learning rate to: 0.000732421875, count=4\n",
      "Epoch 14 of 30 : 9352 out of 10000 = 0.9352; cost=0.0401134285044\n",
      "Halving learning rate to: 0.0003662109375, count=5\n",
      "Epoch 14 of 30 : 9352 out of 10000 = 0.9352; cost=0.0401119010916\n",
      "Halving learning rate to: 0.00018310546875, count=6\n",
      "Epoch 14 of 30 : 9352 out of 10000 = 0.9352; cost=0.0401111462336\n",
      "Halving learning rate to: 9.1552734375e-05, count=7\n",
      "Epoch 14 of 30 : 9351 out of 10000 = 0.9351; cost=0.0401107764569\n",
      "Halving learning rate to: 4.57763671875e-05, count=8\n",
      "Epoch 14 of 30 : 9351 out of 10000 = 0.9351; cost=0.0401105922048\n",
      "Halving learning rate to: 2.288818359375e-05, count=9\n",
      "Epoch 14 of 30 : 9351 out of 10000 = 0.9351; cost=0.040110500463\n",
      "Halving learning rate to: 1.1444091796875e-05, count=10\n",
      "Training ended prematurely...\n",
      "TRAINING ACCURACY, COST : 57195 out of 60000 = 0.9533\n",
      "TEST ACCURACY, COST : 9350 out of 10000 = 0.935\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Take backup of weights to be used later for comparison\n",
    "#initialWeights = [np.array(w) for w in weights]\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 30\n",
    "learningRate = 3.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='step', optimizer='nag', mu=mu, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the above weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"nagWeights_mini10_epochs30_lr3.0_mu0.9\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Train with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 882 of 10000 = 0.0882; cost=1.66119949405\n",
      "Epoch 1 of 60 : 8978 out of 10000 = 0.8978; cost=0.0660061413096\n",
      "Epoch 2 of 60 : 8886 out of 10000 = 0.8886; cost=0.0681654748053\n",
      "Epoch 3 of 60 : 9074 out of 10000 = 0.9074; cost=0.0586687958812\n",
      "Epoch 4 of 60 : 9086 out of 10000 = 0.9086; cost=0.0591863168985\n",
      "Epoch 5 of 60 : 9134 out of 10000 = 0.9134; cost=0.0545254522007\n",
      "Epoch 6 of 60 : 9158 out of 10000 = 0.9158; cost=0.0539935307214\n",
      "Epoch 7 of 60 : 9248 out of 10000 = 0.9248; cost=0.0481049381248\n",
      "Epoch 8 of 60 : 9253 out of 10000 = 0.9253; cost=0.0482258093188\n",
      "Epoch 9 of 60 : 9267 out of 10000 = 0.9267; cost=0.0477349709068\n",
      "Epoch 10 of 60 : 9307 out of 10000 = 0.9307; cost=0.0462745291633\n",
      "Epoch 11 of 60 : 9229 out of 10000 = 0.9229; cost=0.0483732879842\n",
      "Epoch 12 of 60 : 9352 out of 10000 = 0.9352; cost=0.0443519019619\n",
      "Epoch 13 of 60 : 9340 out of 10000 = 0.934; cost=0.0437223217595\n",
      "Epoch 14 of 60 : 9353 out of 10000 = 0.9353; cost=0.0431274428996\n",
      "Epoch 15 of 60 : 9308 out of 10000 = 0.9308; cost=0.0446194613717\n",
      "Epoch 16 of 60 : 9364 out of 10000 = 0.9364; cost=0.0428785273068\n",
      "Epoch 17 of 60 : 9306 out of 10000 = 0.9306; cost=0.0423653818405\n",
      "Epoch 18 of 60 : 9360 out of 10000 = 0.936; cost=0.0428022638162\n",
      "Epoch 19 of 60 : 9352 out of 10000 = 0.9352; cost=0.0431221617225\n",
      "Epoch 20 of 60 : 9316 out of 10000 = 0.9316; cost=0.0456217795307\n",
      "Epoch 21 of 60 : 9356 out of 10000 = 0.9356; cost=0.043960219719\n",
      "Epoch 22 of 60 : 9379 out of 10000 = 0.9379; cost=0.0403170283836\n",
      "Epoch 23 of 60 : 9359 out of 10000 = 0.9359; cost=0.0433493480137\n",
      "Epoch 24 of 60 : 9332 out of 10000 = 0.9332; cost=0.042756943845\n",
      "Epoch 25 of 60 : 9359 out of 10000 = 0.9359; cost=0.0426687939458\n",
      "Epoch 26 of 60 : 9385 out of 10000 = 0.9385; cost=0.0406184658434\n",
      "Epoch 27 of 60 : 9375 out of 10000 = 0.9375; cost=0.0400951827235\n",
      "Epoch 28 of 60 : 9423 out of 10000 = 0.9423; cost=0.0408033261127\n",
      "Epoch 29 of 60 : 9384 out of 10000 = 0.9384; cost=0.0406691692608\n",
      "Epoch 30 of 60 : 9377 out of 10000 = 0.9377; cost=0.0429807921029\n",
      "Epoch 31 of 60 : 9341 out of 10000 = 0.9341; cost=0.0427896283541\n",
      "Epoch 32 of 60 : 9429 out of 10000 = 0.9429; cost=0.039084458504\n",
      "Epoch 33 of 60 : 9378 out of 10000 = 0.9378; cost=0.0400863684934\n",
      "Epoch 34 of 60 : 9400 out of 10000 = 0.94; cost=0.0433373024053\n",
      "Epoch 35 of 60 : 9387 out of 10000 = 0.9387; cost=0.0422487453774\n",
      "Epoch 36 of 60 : 9425 out of 10000 = 0.9425; cost=0.0391043060751\n",
      "Epoch 37 of 60 : 9352 out of 10000 = 0.9352; cost=0.0409761773248\n",
      "Epoch 38 of 60 : 9420 out of 10000 = 0.942; cost=0.0397372599181\n",
      "Epoch 39 of 60 : 9364 out of 10000 = 0.9364; cost=0.0463147450516\n",
      "Epoch 40 of 60 : 9391 out of 10000 = 0.9391; cost=0.039659022852\n",
      "Epoch 41 of 60 : 9421 out of 10000 = 0.9421; cost=0.0391071422817\n",
      "Epoch 42 of 60 : 9420 out of 10000 = 0.942; cost=0.0376057848243\n",
      "Epoch 43 of 60 : 9428 out of 10000 = 0.9428; cost=0.040418513305\n",
      "Epoch 44 of 60 : 9376 out of 10000 = 0.9376; cost=0.0412143000544\n",
      "Epoch 45 of 60 : 9394 out of 10000 = 0.9394; cost=0.040312142584\n",
      "Epoch 46 of 60 : 9470 out of 10000 = 0.947; cost=0.0374693170293\n",
      "Epoch 47 of 60 : 9402 out of 10000 = 0.9402; cost=0.039588559127\n",
      "Epoch 48 of 60 : 9446 out of 10000 = 0.9446; cost=0.039178987512\n",
      "Epoch 49 of 60 : 9456 out of 10000 = 0.9456; cost=0.0386462939683\n",
      "Epoch 50 of 60 : 9482 out of 10000 = 0.9482; cost=0.0372708926751\n",
      "Epoch 51 of 60 : 9439 out of 10000 = 0.9439; cost=0.0390952061935\n",
      "Epoch 52 of 60 : 9468 out of 10000 = 0.9468; cost=0.0363724324352\n",
      "Epoch 53 of 60 : 9423 out of 10000 = 0.9423; cost=0.040413307754\n",
      "Epoch 54 of 60 : 9450 out of 10000 = 0.945; cost=0.0380986362207\n",
      "Epoch 55 of 60 : 9418 out of 10000 = 0.9418; cost=0.0391033490943\n",
      "Epoch 56 of 60 : 9415 out of 10000 = 0.9415; cost=0.0392794363085\n",
      "Epoch 57 of 60 : 9384 out of 10000 = 0.9384; cost=0.0392276800635\n",
      "Epoch 58 of 60 : 9420 out of 10000 = 0.942; cost=0.0386992621219\n",
      "Epoch 59 of 60 : 9419 out of 10000 = 0.9419; cost=0.0398726278266\n",
      "Epoch 60 of 60 : 9382 out of 10000 = 0.9382; cost=0.0423890086518\n",
      "Training complete.\n",
      "TRAINING ACCURACY, COST : 57396 out of 60000 = 0.9566\n",
      "TEST ACCURACY, COST : 9382 out of 10000 = 0.9382\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 60\n",
    "learningRate = 3.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='nag', mu=mu, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 974 of 10000 = 0.0974; cost=1.67281282913\n",
      "Epoch 1 of 60 : 8745 out of 10000 = 0.8745; cost=0.074443859338\n",
      "Epoch 2 of 60 : 8922 out of 10000 = 0.8922; cost=0.0611683263573\n",
      "Epoch 3 of 60 : 9139 out of 10000 = 0.9139; cost=0.0549056154571\n",
      "Epoch 4 of 60 : 9162 out of 10000 = 0.9162; cost=0.0502130992972\n",
      "Epoch 5 of 60 : 9190 out of 10000 = 0.919; cost=0.0508331219877\n",
      "Epoch 6 of 60 : 9263 out of 10000 = 0.9263; cost=0.0459753362356\n",
      "Epoch 7 of 60 : 9229 out of 10000 = 0.9229; cost=0.048099056032\n",
      "Epoch 8 of 60 : 9298 out of 10000 = 0.9298; cost=0.0443920915768\n",
      "Epoch 9 of 60 : 9246 out of 10000 = 0.9246; cost=0.0454406838599\n",
      "Epoch 10 of 60 : 9334 out of 10000 = 0.9334; cost=0.0437210506537\n",
      "Epoch 11 of 60 : 9310 out of 10000 = 0.931; cost=0.0427986209207\n",
      "Epoch 12 of 60 : 9263 out of 10000 = 0.9263; cost=0.0463192980383\n",
      "Epoch 13 of 60 : 9334 out of 10000 = 0.9334; cost=0.0411114303676\n",
      "Epoch 14 of 60 : 9355 out of 10000 = 0.9355; cost=0.0425227733942\n",
      "Epoch 15 of 60 : 9366 out of 10000 = 0.9366; cost=0.0406021325292\n",
      "Epoch 16 of 60 : 9384 out of 10000 = 0.9384; cost=0.0403747062987\n",
      "Epoch 17 of 60 : 9392 out of 10000 = 0.9392; cost=0.0404094037657\n",
      "Epoch 18 of 60 : 9402 out of 10000 = 0.9402; cost=0.0397143069788\n",
      "Epoch 19 of 60 : 9400 out of 10000 = 0.94; cost=0.0399423274764\n",
      "Epoch 20 of 60 : 9356 out of 10000 = 0.9356; cost=0.0412309465577\n",
      "Epoch 21 of 60 : 9353 out of 10000 = 0.9353; cost=0.0423859345578\n",
      "Epoch 22 of 60 : 9425 out of 10000 = 0.9425; cost=0.0395193987744\n",
      "Epoch 23 of 60 : 9414 out of 10000 = 0.9414; cost=0.0401339147637\n",
      "Epoch 24 of 60 : 9373 out of 10000 = 0.9373; cost=0.0401315323211\n",
      "Epoch 25 of 60 : 9402 out of 10000 = 0.9402; cost=0.0397733962728\n",
      "Epoch 26 of 60 : 9366 out of 10000 = 0.9366; cost=0.0432101941433\n",
      "Epoch 27 of 60 : 9403 out of 10000 = 0.9403; cost=0.04092711133\n",
      "Epoch 28 of 60 : 9406 out of 10000 = 0.9406; cost=0.0396041886079\n",
      "Epoch 29 of 60 : 9395 out of 10000 = 0.9395; cost=0.0408653609172\n",
      "Epoch 30 of 60 : 9406 out of 10000 = 0.9406; cost=0.040167607553\n",
      "Epoch 31 of 60 : 9412 out of 10000 = 0.9412; cost=0.0399217748562\n",
      "Epoch 32 of 60 : 9425 out of 10000 = 0.9425; cost=0.0387597512534\n",
      "Epoch 33 of 60 : 9390 out of 10000 = 0.939; cost=0.041696606151\n",
      "Epoch 34 of 60 : 9381 out of 10000 = 0.9381; cost=0.0420499879739\n",
      "Epoch 35 of 60 : 9406 out of 10000 = 0.9406; cost=0.0414085637227\n",
      "Epoch 36 of 60 : 9432 out of 10000 = 0.9432; cost=0.0395051959803\n",
      "Epoch 37 of 60 : 9447 out of 10000 = 0.9447; cost=0.0385121938588\n",
      "Epoch 38 of 60 : 9437 out of 10000 = 0.9437; cost=0.0394732875462\n",
      "Epoch 39 of 60 : 9442 out of 10000 = 0.9442; cost=0.0386619476498\n",
      "Epoch 40 of 60 : 9422 out of 10000 = 0.9422; cost=0.0384265104294\n",
      "Epoch 41 of 60 : 9463 out of 10000 = 0.9463; cost=0.0380242592321\n",
      "Epoch 42 of 60 : 9411 out of 10000 = 0.9411; cost=0.0428841428657\n",
      "Epoch 43 of 60 : 9410 out of 10000 = 0.941; cost=0.0397751143636\n",
      "Epoch 44 of 60 : 9434 out of 10000 = 0.9434; cost=0.0398328605932\n",
      "Epoch 45 of 60 : 9428 out of 10000 = 0.9428; cost=0.0398995393998\n",
      "Epoch 46 of 60 : 9437 out of 10000 = 0.9437; cost=0.0393294325527\n",
      "Epoch 47 of 60 : 9450 out of 10000 = 0.945; cost=0.0385457208289\n",
      "Epoch 48 of 60 : 9404 out of 10000 = 0.9404; cost=0.0406730062434\n",
      "Epoch 49 of 60 : 9429 out of 10000 = 0.9429; cost=0.0406248921423\n",
      "Epoch 50 of 60 : 9464 out of 10000 = 0.9464; cost=0.0390302366773\n",
      "Epoch 51 of 60 : 9460 out of 10000 = 0.946; cost=0.0389238626259\n",
      "Epoch 52 of 60 : 9451 out of 10000 = 0.9451; cost=0.0393043800467\n",
      "Epoch 53 of 60 : 9437 out of 10000 = 0.9437; cost=0.0403936954612\n",
      "Epoch 54 of 60 : 9423 out of 10000 = 0.9423; cost=0.0403273374564\n",
      "Epoch 55 of 60 : 9451 out of 10000 = 0.9451; cost=0.0396573133063\n",
      "Epoch 56 of 60 : 9445 out of 10000 = 0.9445; cost=0.0391337414649\n",
      "Epoch 57 of 60 : 9448 out of 10000 = 0.9448; cost=0.0387399214926\n",
      "Epoch 58 of 60 : 9450 out of 10000 = 0.945; cost=0.0392851463189\n",
      "Epoch 59 of 60 : 9440 out of 10000 = 0.944; cost=0.0395720385649\n",
      "Epoch 60 of 60 : 9455 out of 10000 = 0.9455; cost=0.0398381101804\n",
      "Training complete.\n",
      "TRAINING ACCURACY, COST : 58828 out of 60000 = 0.9805\n",
      "TEST ACCURACY, COST : 9455 out of 10000 = 0.9455\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 60\n",
    "learningRate = 1.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='nag', mu=mu, testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 931 of 10000 = 0.0931; cost=1.61107296067\n",
      "Epoch 1 of 100 : 8737 out of 10000 = 0.8737; cost=0.0783480782972\n",
      "Epoch 2 of 100 : 8873 out of 10000 = 0.8873; cost=0.0645569387094\n",
      "Epoch 3 of 100 : 9048 out of 10000 = 0.9048; cost=0.056951603865\n",
      "Epoch 4 of 100 : 9144 out of 10000 = 0.9144; cost=0.0511362917512\n",
      "Epoch 5 of 100 : 9200 out of 10000 = 0.92; cost=0.0497869089763\n",
      "Epoch 6 of 100 : 9228 out of 10000 = 0.9228; cost=0.0480547534428\n",
      "Epoch 7 of 100 : 9237 out of 10000 = 0.9237; cost=0.0477574263471\n",
      "Epoch 8 of 100 : 9247 out of 10000 = 0.9247; cost=0.0464066937476\n",
      "Epoch 9 of 100 : 9277 out of 10000 = 0.9277; cost=0.0453933840844\n",
      "Epoch 10 of 100 : 9290 out of 10000 = 0.929; cost=0.0447885192122\n",
      "Epoch 11 of 100 : 9309 out of 10000 = 0.9309; cost=0.0444222820193\n",
      "Epoch 12 of 100 : 9294 out of 10000 = 0.9294; cost=0.0439987882411\n",
      "Epoch 13 of 100 : 9311 out of 10000 = 0.9311; cost=0.0440451283848\n",
      "Epoch 14 of 100 : 9326 out of 10000 = 0.9326; cost=0.0436575875899\n",
      "Epoch 15 of 100 : 9333 out of 10000 = 0.9333; cost=0.0433620023597\n",
      "Epoch 16 of 100 : 9321 out of 10000 = 0.9321; cost=0.0428736342512\n",
      "Epoch 17 of 100 : 9335 out of 10000 = 0.9335; cost=0.0429041791768\n",
      "Epoch 18 of 100 : 9328 out of 10000 = 0.9328; cost=0.0427315748449\n",
      "Epoch 19 of 100 : 9353 out of 10000 = 0.9353; cost=0.0429218175211\n",
      "Epoch 20 of 100 : 9343 out of 10000 = 0.9343; cost=0.042663502263\n",
      "Epoch 21 of 100 : 9343 out of 10000 = 0.9343; cost=0.0425598022579\n",
      "Epoch 22 of 100 : 9326 out of 10000 = 0.9326; cost=0.0426900095941\n",
      "Epoch 23 of 100 : 9353 out of 10000 = 0.9353; cost=0.0422722106492\n",
      "Epoch 24 of 100 : 9351 out of 10000 = 0.9351; cost=0.0422726266545\n",
      "Epoch 25 of 100 : 9342 out of 10000 = 0.9342; cost=0.0421856660662\n",
      "Epoch 26 of 100 : 9353 out of 10000 = 0.9353; cost=0.0423502086893\n",
      "Epoch 27 of 100 : 9348 out of 10000 = 0.9348; cost=0.0423919362032\n",
      "Epoch 28 of 100 : 9359 out of 10000 = 0.9359; cost=0.0423463753471\n",
      "Epoch 29 of 100 : 9352 out of 10000 = 0.9352; cost=0.0421520623177\n",
      "Epoch 30 of 100 : 9356 out of 10000 = 0.9356; cost=0.0422469852363\n",
      "Epoch 31 of 100 : 9353 out of 10000 = 0.9353; cost=0.0421408568347\n",
      "Epoch 32 of 100 : 9352 out of 10000 = 0.9352; cost=0.0423149970556\n",
      "Epoch 33 of 100 : 9354 out of 10000 = 0.9354; cost=0.0422062751287\n",
      "Epoch 34 of 100 : 9355 out of 10000 = 0.9355; cost=0.0421190710047\n",
      "Epoch 35 of 100 : 9353 out of 10000 = 0.9353; cost=0.0421825398666\n",
      "Epoch 36 of 100 : 9353 out of 10000 = 0.9353; cost=0.0421874670698\n",
      "Epoch 37 of 100 : 9363 out of 10000 = 0.9363; cost=0.042182239901\n",
      "Epoch 38 of 100 : 9356 out of 10000 = 0.9356; cost=0.042193268711\n",
      "Epoch 39 of 100 : 9355 out of 10000 = 0.9355; cost=0.0421856502656\n",
      "Epoch 40 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421781033969\n",
      "Epoch 41 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421327199767\n",
      "Epoch 42 of 100 : 9353 out of 10000 = 0.9353; cost=0.0421646918675\n",
      "Epoch 43 of 100 : 9352 out of 10000 = 0.9352; cost=0.0421330708066\n",
      "Epoch 44 of 100 : 9355 out of 10000 = 0.9355; cost=0.042126763589\n",
      "Epoch 45 of 100 : 9359 out of 10000 = 0.9359; cost=0.0421154594479\n",
      "Epoch 46 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421549404716\n",
      "Epoch 47 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421377708718\n",
      "Epoch 48 of 100 : 9356 out of 10000 = 0.9356; cost=0.04215401887\n",
      "Epoch 49 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421525735233\n",
      "Epoch 50 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421284037338\n",
      "Epoch 51 of 100 : 9359 out of 10000 = 0.9359; cost=0.0421222781472\n",
      "Epoch 52 of 100 : 9356 out of 10000 = 0.9356; cost=0.0421340234546\n",
      "Epoch 53 of 100 : 9355 out of 10000 = 0.9355; cost=0.0421189874797\n",
      "Epoch 54 of 100 : 9358 out of 10000 = 0.9358; cost=0.042122142723\n",
      "Epoch 55 of 100 : 9356 out of 10000 = 0.9356; cost=0.0421208832244\n",
      "Epoch 56 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421210671374\n",
      "Epoch 57 of 100 : 9356 out of 10000 = 0.9356; cost=0.0421324124154\n",
      "Epoch 58 of 100 : 9356 out of 10000 = 0.9356; cost=0.0421356651977\n",
      "Epoch 59 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421329807912\n",
      "Epoch 60 of 100 : 9358 out of 10000 = 0.9358; cost=0.0421315274095\n",
      "Epoch 61 of 100 : 9358 out of 10000 = 0.9358; cost=0.042130551635\n",
      "Epoch 62 of 100 : 9359 out of 10000 = 0.9359; cost=0.0421332597352\n",
      "Epoch 63 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421308790477\n",
      "Epoch 64 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421324581613\n",
      "Epoch 65 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421328812047\n",
      "Epoch 66 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421307650245\n",
      "Epoch 67 of 100 : 9357 out of 10000 = 0.9357; cost=0.042130544548\n",
      "Epoch 68 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421306499192\n",
      "Epoch 69 of 100 : 9357 out of 10000 = 0.9357; cost=0.042131072621\n",
      "Epoch 70 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421309307882\n",
      "Epoch 71 of 100 : 9357 out of 10000 = 0.9357; cost=0.042130746472\n",
      "Epoch 72 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421308281857\n",
      "Epoch 73 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421305119203\n",
      "Epoch 74 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421306721394\n",
      "Epoch 75 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421306210485\n",
      "Epoch 76 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421308614471\n",
      "Epoch 77 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421307497207\n",
      "Epoch 78 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421306646564\n",
      "Epoch 79 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421305894521\n",
      "Epoch 80 of 100 : 9357 out of 10000 = 0.9357; cost=0.042130477241\n",
      "Epoch 81 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421304110942\n",
      "Epoch 82 of 100 : 9357 out of 10000 = 0.9357; cost=0.04213036541\n",
      "Epoch 83 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421302421913\n",
      "Epoch 84 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421302082366\n",
      "Epoch 85 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421301781185\n",
      "Epoch 86 of 100 : 9357 out of 10000 = 0.9357; cost=0.042130155274\n",
      "Epoch 87 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421301327737\n",
      "Epoch 88 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421301011173\n",
      "Epoch 89 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421300936029\n",
      "Epoch 90 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421300683246\n",
      "Epoch 91 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421300427978\n",
      "Epoch 92 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421300290896\n",
      "Epoch 93 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421300175461\n",
      "Epoch 94 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421300099793\n",
      "Epoch 95 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421300039819\n",
      "Epoch 96 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421299995342\n",
      "Epoch 97 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421299926697\n",
      "Epoch 98 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421299861211\n",
      "Epoch 99 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421299813293\n",
      "Epoch 100 of 100 : 9357 out of 10000 = 0.9357; cost=0.0421299768731\n",
      "Training complete.\n",
      "TRAINING ACCURACY, COST : 57931 out of 60000 = 0.9655\n",
      "TEST ACCURACY, COST : 9357 out of 10000 = 0.9357\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 100\n",
    "learningRate = 1.0\n",
    "decayRate = 0.1\n",
    "nagMu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay='time', decayRate=decayRate, optimizer='nag', mu=nagMu,\n",
    "                      testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1516 of 10000 = 0.1516; cost=1.91848557009\n",
      "Epoch 1 of 100 : 4815 of 10000 = 0.4815; cost=0.26115046542\n",
      "Epoch 2 of 100 : 6253 of 10000 = 0.6253; cost=0.196657200378\n",
      "Epoch 3 of 100 : 6445 of 10000 = 0.6445; cost=0.185546611304\n",
      "Epoch 4 of 100 : 6450 of 10000 = 0.645; cost=0.183981785899\n",
      "Epoch 5 of 100 : 6510 of 10000 = 0.651; cost=0.182547138359\n",
      "Epoch 6 of 100 : 6594 of 10000 = 0.6594; cost=0.178527580912\n",
      "Epoch 7 of 100 : 6584 of 10000 = 0.6584; cost=0.175072600092\n",
      "Epoch 8 of 100 : 6606 of 10000 = 0.6606; cost=0.175843939101\n",
      "Epoch 9 of 100 : 6659 of 10000 = 0.6659; cost=0.179198957328\n",
      "Epoch 10 of 100 : 6605 of 10000 = 0.6605; cost=0.175178182316\n",
      "Epoch 11 of 100 : 6707 of 10000 = 0.6707; cost=0.174422266315\n",
      "Epoch 12 of 100 : 6641 of 10000 = 0.6641; cost=0.174000157486\n",
      "Epoch 13 of 100 : 6648 of 10000 = 0.6648; cost=0.174920532667\n",
      "Epoch 14 of 100 : 6668 of 10000 = 0.6668; cost=0.171294499136\n",
      "Epoch 15 of 100 : 6597 of 10000 = 0.6597; cost=0.173744196854\n",
      "Epoch 16 of 100 : 6672 of 10000 = 0.6672; cost=0.171827666661\n",
      "Epoch 17 of 100 : 6662 of 10000 = 0.6662; cost=0.171494689013\n",
      "Epoch 18 of 100 : 6672 of 10000 = 0.6672; cost=0.171479941669\n",
      "Epoch 19 of 100 : 6717 of 10000 = 0.6717; cost=0.170087763814\n",
      "Epoch 20 of 100 : 7503 of 10000 = 0.7503; cost=0.131764280036\n",
      "Epoch 21 of 100 : 7664 of 10000 = 0.7664; cost=0.124125190622\n",
      "Epoch 22 of 100 : 7651 of 10000 = 0.7651; cost=0.123916129836\n",
      "Epoch 23 of 100 : 7703 of 10000 = 0.7703; cost=0.121921285125\n",
      "Epoch 24 of 100 : 7660 of 10000 = 0.766; cost=0.124593312826\n",
      "Epoch 25 of 100 : 7706 of 10000 = 0.7706; cost=0.126340816568\n",
      "Epoch 26 of 100 : 7692 of 10000 = 0.7692; cost=0.122937561851\n",
      "Epoch 27 of 100 : 7688 of 10000 = 0.7688; cost=0.124208819756\n",
      "Epoch 28 of 100 : 7732 of 10000 = 0.7732; cost=0.121829166231\n",
      "Epoch 29 of 100 : 7700 of 10000 = 0.77; cost=0.121555925046\n",
      "Epoch 30 of 100 : 7725 of 10000 = 0.7725; cost=0.121363609868\n",
      "Epoch 31 of 100 : 7717 of 10000 = 0.7717; cost=0.120844988823\n",
      "Epoch 32 of 100 : 7725 of 10000 = 0.7725; cost=0.119214091392\n",
      "Epoch 33 of 100 : 7732 of 10000 = 0.7732; cost=0.119789524191\n",
      "Epoch 34 of 100 : 7728 of 10000 = 0.7728; cost=0.120876016914\n",
      "Epoch 35 of 100 : 8596 of 10000 = 0.8596; cost=0.0820934522675\n",
      "Epoch 36 of 100 : 8556 of 10000 = 0.8556; cost=0.0822892520837\n",
      "Epoch 37 of 100 : 8673 of 10000 = 0.8673; cost=0.0765440787959\n",
      "Epoch 38 of 100 : 8678 of 10000 = 0.8678; cost=0.083032664935\n",
      "Epoch 39 of 100 : 8661 of 10000 = 0.8661; cost=0.0748433404703\n",
      "Epoch 40 of 100 : 8616 of 10000 = 0.8616; cost=0.0762875890986\n",
      "Epoch 41 of 100 : 8564 of 10000 = 0.8564; cost=0.0775066485311\n",
      "Epoch 42 of 100 : 8653 of 10000 = 0.8653; cost=0.0736777289837\n",
      "Epoch 43 of 100 : 8625 of 10000 = 0.8625; cost=0.0759810831911\n",
      "Epoch 44 of 100 : 8635 of 10000 = 0.8635; cost=0.0750785691386\n",
      "Epoch 45 of 100 : 8608 of 10000 = 0.8608; cost=0.077685611524\n",
      "Epoch 46 of 100 : 8656 of 10000 = 0.8656; cost=0.0740838896066\n",
      "Epoch 47 of 100 : 8638 of 10000 = 0.8638; cost=0.0748882612048\n",
      "Epoch 48 of 100 : 8697 of 10000 = 0.8697; cost=0.076241417923\n",
      "Epoch 49 of 100 : 8635 of 10000 = 0.8635; cost=0.07598935889\n",
      "Epoch 50 of 100 : 8648 of 10000 = 0.8648; cost=0.0760086324143\n",
      "Epoch 51 of 100 : 8685 of 10000 = 0.8685; cost=0.074122026801\n",
      "Epoch 52 of 100 : 8683 of 10000 = 0.8683; cost=0.0741374398649\n",
      "Epoch 53 of 100 : 8600 of 10000 = 0.86; cost=0.0781057985434\n",
      "Epoch 54 of 100 : 8662 of 10000 = 0.8662; cost=0.075264733204\n",
      "Epoch 55 of 100 : 8670 of 10000 = 0.867; cost=0.0754951854861\n",
      "Epoch 56 of 100 : 8647 of 10000 = 0.8647; cost=0.0740121967927\n",
      "Epoch 57 of 100 : 8658 of 10000 = 0.8658; cost=0.0737399920585\n",
      "Epoch 58 of 100 : 8685 of 10000 = 0.8685; cost=0.0742666712634\n",
      "Epoch 59 of 100 : 8668 of 10000 = 0.8668; cost=0.0746163343987\n",
      "Epoch 60 of 100 : 8662 of 10000 = 0.8662; cost=0.0743403514222\n",
      "Epoch 61 of 100 : 8695 of 10000 = 0.8695; cost=0.0735420111388\n",
      "Epoch 62 of 100 : 8607 of 10000 = 0.8607; cost=0.0746545901482\n",
      "Epoch 63 of 100 : 8680 of 10000 = 0.868; cost=0.073210277387\n",
      "Epoch 64 of 100 : 8691 of 10000 = 0.8691; cost=0.0735593555976\n",
      "Epoch 65 of 100 : 8681 of 10000 = 0.8681; cost=0.0723792777023\n",
      "Epoch 66 of 100 : 8714 of 10000 = 0.8714; cost=0.0731382169447\n",
      "Epoch 67 of 100 : 8662 of 10000 = 0.8662; cost=0.0736452237712\n",
      "Epoch 68 of 100 : 8697 of 10000 = 0.8697; cost=0.072856489574\n",
      "Epoch 69 of 100 : 8667 of 10000 = 0.8667; cost=0.072906960601\n",
      "Epoch 70 of 100 : 8659 of 10000 = 0.8659; cost=0.0749080858939\n",
      "Epoch 71 of 100 : 8641 of 10000 = 0.8641; cost=0.0737524473037\n",
      "Epoch 72 of 100 : 8663 of 10000 = 0.8663; cost=0.0722786598337\n",
      "Epoch 73 of 100 : 8665 of 10000 = 0.8665; cost=0.074658016398\n",
      "Epoch 74 of 100 : 8697 of 10000 = 0.8697; cost=0.0736775477462\n",
      "Epoch 75 of 100 : 8683 of 10000 = 0.8683; cost=0.0732223643035\n",
      "Epoch 76 of 100 : 8665 of 10000 = 0.8665; cost=0.0718677032724\n",
      "Epoch 77 of 100 : 8659 of 10000 = 0.8659; cost=0.0727341363375\n",
      "Epoch 78 of 100 : 8630 of 10000 = 0.863; cost=0.0742687797091\n",
      "Epoch 79 of 100 : 8669 of 10000 = 0.8669; cost=0.072637760541\n",
      "Epoch 80 of 100 : 8673 of 10000 = 0.8673; cost=0.0728876226076\n",
      "Epoch 81 of 100 : 8718 of 10000 = 0.8718; cost=0.0720743282656\n",
      "Epoch 82 of 100 : 8670 of 10000 = 0.867; cost=0.0735296636794\n",
      "Epoch 83 of 100 : 8645 of 10000 = 0.8645; cost=0.0759921183217\n",
      "Epoch 84 of 100 : 8694 of 10000 = 0.8694; cost=0.072280156689\n",
      "Epoch 85 of 100 : 8682 of 10000 = 0.8682; cost=0.0728833932977\n",
      "Epoch 86 of 100 : 8684 of 10000 = 0.8684; cost=0.0734870358009\n",
      "Epoch 87 of 100 : 8672 of 10000 = 0.8672; cost=0.0738096846706\n",
      "Epoch 88 of 100 : 8683 of 10000 = 0.8683; cost=0.0729034220953\n",
      "Epoch 89 of 100 : 8689 of 10000 = 0.8689; cost=0.0719867085714\n",
      "Epoch 90 of 100 : 8705 of 10000 = 0.8705; cost=0.0721965737723\n",
      "Epoch 91 of 100 : 8678 of 10000 = 0.8678; cost=0.0736865725253\n",
      "Epoch 92 of 100 : 8683 of 10000 = 0.8683; cost=0.0720960616443\n",
      "Epoch 93 of 100 : 8670 of 10000 = 0.867; cost=0.0728185570523\n",
      "Epoch 94 of 100 : 8700 of 10000 = 0.87; cost=0.0722579217687\n",
      "Epoch 95 of 100 : 8700 of 10000 = 0.87; cost=0.0721663030808\n",
      "Epoch 96 of 100 : 8696 of 10000 = 0.8696; cost=0.0718677880592\n",
      "Epoch 97 of 100 : 8705 of 10000 = 0.8705; cost=0.0716831974962\n",
      "Epoch 98 of 100 : 8696 of 10000 = 0.8696; cost=0.0719005715644\n",
      "Epoch 99 of 100 : 8699 of 10000 = 0.8699; cost=0.0718137558954\n",
      "Epoch 100 of 100 : 8699 of 10000 = 0.8699; cost=0.071832300831\n",
      "Training complete.\n",
      "TRAINING ACCURACY, COST : 53670 of 60000 = 0.8945\n",
      "TEST ACCURACY, COST : 8699 of 10000 = 0.8699\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 128, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 100\n",
    "learningRate = 3.0\n",
    "nagMu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='nag', mu=nagMu,\n",
    "                      testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 1205 of 10000 = 0.1205; cost=2.46936963982\n",
      "Epoch 1 of 100 : 4170 of 10000 = 0.417; cost=0.293425313212\n",
      "Epoch 2 of 100 : 4586 of 10000 = 0.4586; cost=0.284537827722\n",
      "Epoch 3 of 100 : 4472 of 10000 = 0.4472; cost=0.28094716287\n",
      "Epoch 4 of 100 : 4552 of 10000 = 0.4552; cost=0.276532039097\n",
      "Epoch 5 of 100 : 4618 of 10000 = 0.4618; cost=0.27519764717\n",
      "Epoch 6 of 100 : 5506 of 10000 = 0.5506; cost=0.233797128552\n",
      "Epoch 7 of 100 : 5453 of 10000 = 0.5453; cost=0.231355488045\n",
      "Epoch 8 of 100 : 5549 of 10000 = 0.5549; cost=0.230258491935\n",
      "Epoch 9 of 100 : 5528 of 10000 = 0.5528; cost=0.230304760574\n",
      "Epoch 10 of 100 : 5542 of 10000 = 0.5542; cost=0.226973430262\n",
      "Epoch 11 of 100 : 5553 of 10000 = 0.5553; cost=0.226042940372\n",
      "Epoch 12 of 100 : 5618 of 10000 = 0.5618; cost=0.226177398449\n",
      "Epoch 13 of 100 : 5606 of 10000 = 0.5606; cost=0.225591245157\n",
      "Epoch 14 of 100 : 5601 of 10000 = 0.5601; cost=0.225035983064\n",
      "Epoch 15 of 100 : 5620 of 10000 = 0.562; cost=0.225802162087\n",
      "Epoch 16 of 100 : 5589 of 10000 = 0.5589; cost=0.224676646017\n",
      "Epoch 17 of 100 : 5616 of 10000 = 0.5616; cost=0.225816301065\n",
      "Epoch 18 of 100 : 5624 of 10000 = 0.5624; cost=0.224505418631\n",
      "Epoch 19 of 100 : 5625 of 10000 = 0.5625; cost=0.224395054163\n",
      "Epoch 20 of 100 : 5602 of 10000 = 0.5602; cost=0.224543396266\n",
      "Epoch 21 of 100 : 5628 of 10000 = 0.5628; cost=0.223965601845\n",
      "Epoch 22 of 100 : 5612 of 10000 = 0.5612; cost=0.224176569872\n",
      "Epoch 23 of 100 : 5621 of 10000 = 0.5621; cost=0.223640832058\n",
      "Epoch 24 of 100 : 5652 of 10000 = 0.5652; cost=0.223384613466\n",
      "Epoch 25 of 100 : 5644 of 10000 = 0.5644; cost=0.223819806864\n",
      "Epoch 26 of 100 : 5645 of 10000 = 0.5645; cost=0.223262858117\n",
      "Epoch 27 of 100 : 5657 of 10000 = 0.5657; cost=0.223322875695\n",
      "Epoch 28 of 100 : 5657 of 10000 = 0.5657; cost=0.223135330445\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 128, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 100\n",
    "learningRate = 1.0\n",
    "nagMu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, x_train, y_train, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='nag', mu=nagMu,\n",
    "                      testX=x_test, testY=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KAGGLE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download \"train.csv\" and \"test.csv\" from https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "kaggleTrain = np.loadtxt(open(\"train.csv\"), delimiter=',', skiprows=1)\n",
    "kaggleTest = np.loadtxt(open(\"test.csv\"), delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed(\"kaggleData.npz\", x=kaggleTest, y=kaggleTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training and test data into the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training inputs and outputs\n",
    "kaggleTrainX = kaggleTrain[:, 1:]\n",
    "kaggleTrainY = kaggleTrain[:, 0]\n",
    "\n",
    "# Normalize x_train and x_test\n",
    "kaggleTrainX = kaggleTrainX / 255.0\n",
    "kaggleTestX = kaggleTest / 255.0\n",
    "\n",
    "# Make new y_train of nx10 elements\n",
    "new_y_train = np.zeros((len(kaggleTrainY), 10))\n",
    "for i in range(len(kaggleTrainY)):\n",
    "    new_y_train[i, int(kaggleTrainY[i])] = 1\n",
    "del kaggleTrainY\n",
    "kaggleTrainY = new_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using Kaggle training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 4232 of 42000 = 0.1008; cost=1.55663129091\n",
      "Epoch 1 of 60 : 35443 of 42000 = 0.8439; cost=0.0900705110879\n",
      "Epoch 2 of 60 : 37303 of 42000 = 0.8882; cost=0.0655559645305\n",
      "Epoch 3 of 60 : 38012 of 42000 = 0.905; cost=0.0584415033671\n",
      "Epoch 4 of 60 : 38498 of 42000 = 0.9166; cost=0.0491312638999\n",
      "Epoch 5 of 60 : 38995 of 42000 = 0.9285; cost=0.0459145322529\n",
      "Epoch 6 of 60 : 39082 of 42000 = 0.9305; cost=0.0443336530561\n",
      "Epoch 7 of 60 : 39468 of 42000 = 0.9397; cost=0.0383742213748\n",
      "Epoch 8 of 60 : 39285 of 42000 = 0.9354; cost=0.0402843166376\n",
      "Epoch 9 of 60 : 39702 of 42000 = 0.9453; cost=0.0347878377413\n",
      "Epoch 10 of 60 : 39712 of 42000 = 0.9455; cost=0.0341864112373\n",
      "Epoch 11 of 60 : 39776 of 42000 = 0.947; cost=0.032362313093\n",
      "Epoch 12 of 60 : 39960 of 42000 = 0.9514; cost=0.0307108280391\n",
      "Epoch 13 of 60 : 39993 of 42000 = 0.9522; cost=0.0297291448352\n",
      "Epoch 14 of 60 : 40258 of 42000 = 0.9585; cost=0.0271756510094\n",
      "Epoch 15 of 60 : 40271 of 42000 = 0.9588; cost=0.0262856915832\n",
      "Epoch 16 of 60 : 40279 of 42000 = 0.959; cost=0.0253799174771\n",
      "Epoch 17 of 60 : 40321 of 42000 = 0.96; cost=0.025204294785\n",
      "Epoch 18 of 60 : 40335 of 42000 = 0.9604; cost=0.0268057956419\n",
      "Epoch 19 of 60 : 40370 of 42000 = 0.9612; cost=0.0240247381356\n",
      "Epoch 20 of 60 : 40521 of 42000 = 0.9648; cost=0.0230584682167\n",
      "Epoch 21 of 60 : 40519 of 42000 = 0.9647; cost=0.0230610192146\n",
      "Epoch 22 of 60 : 40620 of 42000 = 0.9671; cost=0.0218436493602\n",
      "Epoch 23 of 60 : 40542 of 42000 = 0.9653; cost=0.0222790711844\n",
      "Epoch 24 of 60 : 40590 of 42000 = 0.9664; cost=0.0222323020917\n",
      "Epoch 25 of 60 : 40639 of 42000 = 0.9676; cost=0.0202307122887\n",
      "Epoch 26 of 60 : 40540 of 42000 = 0.9652; cost=0.022268026213\n",
      "Epoch 27 of 60 : 40688 of 42000 = 0.9688; cost=0.0200685424182\n",
      "Epoch 28 of 60 : 40850 of 42000 = 0.9726; cost=0.0187246608122\n",
      "Epoch 29 of 60 : 40818 of 42000 = 0.9719; cost=0.0189888837\n",
      "Epoch 30 of 60 : 40740 of 42000 = 0.97; cost=0.0192163054522\n",
      "Epoch 31 of 60 : 40817 of 42000 = 0.9718; cost=0.0190792724741\n",
      "Epoch 32 of 60 : 40878 of 42000 = 0.9733; cost=0.0175842484203\n",
      "Epoch 33 of 60 : 40846 of 42000 = 0.9725; cost=0.0171995686943\n",
      "Epoch 34 of 60 : 40606 of 42000 = 0.9668; cost=0.0206296853218\n",
      "Epoch 35 of 60 : 40864 of 42000 = 0.973; cost=0.0184915483579\n",
      "Epoch 36 of 60 : 40880 of 42000 = 0.9733; cost=0.017535996517\n",
      "Epoch 37 of 60 : 40707 of 42000 = 0.9692; cost=0.0202700705116\n",
      "Epoch 38 of 60 : 40786 of 42000 = 0.9711; cost=0.0175474012974\n",
      "Epoch 39 of 60 : 40815 of 42000 = 0.9718; cost=0.0172057036245\n",
      "Epoch 40 of 60 : 40939 of 42000 = 0.9747; cost=0.0164034595017\n",
      "Epoch 41 of 60 : 41019 of 42000 = 0.9766; cost=0.0151426452902\n",
      "Epoch 42 of 60 : 41038 of 42000 = 0.9771; cost=0.015312695658\n",
      "Epoch 43 of 60 : 40982 of 42000 = 0.9758; cost=0.0157154585889\n",
      "Epoch 44 of 60 : 40974 of 42000 = 0.9756; cost=0.015194574976\n",
      "Epoch 45 of 60 : 41066 of 42000 = 0.9778; cost=0.0148867911396\n",
      "Epoch 46 of 60 : 41118 of 42000 = 0.979; cost=0.0136066989714\n",
      "Epoch 47 of 60 : 41114 of 42000 = 0.9789; cost=0.0138844824004\n",
      "Epoch 48 of 60 : 41045 of 42000 = 0.9773; cost=0.0147834447753\n",
      "Epoch 49 of 60 : 41087 of 42000 = 0.9783; cost=0.0146686482079\n",
      "Epoch 50 of 60 : 41112 of 42000 = 0.9789; cost=0.0135588683457\n",
      "Epoch 51 of 60 : 40881 of 42000 = 0.9734; cost=0.0162814341682\n",
      "Epoch 52 of 60 : 40984 of 42000 = 0.9758; cost=0.0154294818429\n",
      "Epoch 53 of 60 : 41156 of 42000 = 0.9799; cost=0.0128466419811\n",
      "Epoch 54 of 60 : 41100 of 42000 = 0.9786; cost=0.0142763558824\n",
      "Epoch 55 of 60 : 40885 of 42000 = 0.9735; cost=0.0174530449171\n",
      "Epoch 56 of 60 : 41212 of 42000 = 0.9812; cost=0.0126047077385\n",
      "Epoch 57 of 60 : 41196 of 42000 = 0.9809; cost=0.0126365406505\n",
      "Epoch 58 of 60 : 41160 of 42000 = 0.98; cost=0.0126897968852\n",
      "Epoch 59 of 60 : 41306 of 42000 = 0.9835; cost=0.0110626889035\n",
      "Epoch 60 of 60 : 41189 of 42000 = 0.9807; cost=0.0123812053144\n",
      "Training complete.\n",
      "TRAINING ACCURACY, COST : 41189 of 42000 = 0.9807\n"
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 60\n",
    "learningRate = 1.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, kaggleTrainX, kaggleTrainY, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='nag', mu=mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "np.save(\"kaggleWeights_mini10_60epochs_lr1.0_decayNone_nagMu0.9_accuracy0.9807\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 5091 of 42000 = 0.1212; cost=2.0068225263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-4cc84b9cdef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m trainUsingMinibatchGD(weights, kaggleTrainX, kaggleTrainY, minibatchSize, nEpochs, learningRate,\n\u001b[0;32m---> 15\u001b[0;31m                       decay=None, optimizer='nag', mu=mu)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-3c55f0a7fc7d>\u001b[0m in \u001b[0;36mtrainUsingMinibatchGD\u001b[0;34m(weights, X, Y, minibatchSize, nEpochs, learningRate, decay, decayRate, optimizer, mu, testX, testY)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# Run backprop, with an optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mbackProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mySample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Check cost and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-3c55f0a7fc7d>\u001b[0m in \u001b[0;36mbackProp\u001b[0;34m(weights, X, Y, learningRate, optimizer, mu)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Calculate bpError for previous layer to be back-propagated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbpError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Ignore bias term in bpError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN using Mini-batch Gradient Descent\n",
    "\n",
    "# Initialize network\n",
    "layers = [784, 30, 30, 10]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "# Set options of mini-batch gradient descent\n",
    "minibatchSize = 10\n",
    "nEpochs = 100\n",
    "learningRate = 1.0\n",
    "mu = 0.9\n",
    "\n",
    "# Train\n",
    "trainUsingMinibatchGD(weights, kaggleTrainX, kaggleTrainY, minibatchSize, nEpochs, learningRate,\n",
    "                      decay=None, optimizer='nag', mu=mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on Kaggle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "preds = forwardProp(kaggleTestX, weights)[-1]\n",
    "\n",
    "# Record the digit numbers\n",
    "testY = -np.ones((len(preds), 2))\n",
    "for i in range(len(preds)):\n",
    "    testY[i][0] = i+1\n",
    "    testY[i][1] = np.argmax(preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.27486584e-03   2.06595069e-04   9.69224712e-01   5.11547819e-03\n",
      "   7.43323595e-04   4.39173581e-04   6.71424976e-03   3.33608838e-04\n",
      "   4.63776709e-03   8.61203901e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preds[0])\n",
    "testY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions in the format dictated by Kaggle\n",
    "np.savetxt(\"preds.csv\", testY, fmt='%i', delimiter=',', header=\"ImageId,Label\", comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myPreds = np.loadtxt(open(\"preds.csv\"), delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 2)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPreds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
